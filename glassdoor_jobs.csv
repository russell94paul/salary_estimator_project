Job Title,Salary Estimate,Job Description,Rating,Company Name,Location,Headquarters,Size,Founded,Type of ownership,Industry,Sector,Revenue,Competitors
Data Engineer,$63k-$118k (Glassdoor Est.),"Organization Description
Tides is a philanthropic partner and nonprofit accelerator dedicated to building a world of shared prosperity and social justice. Tides works at the nexus of funders, changemakers, and policy, with extensive impact solutions including philanthropic giving and grant making, impact investing, fiscal sponsorship for social ventures, collaborative workspaces, and policy initiatives. Our extensive tools and know-how give our partners the freedom to hit the ground running and drive change faster than they can on their own. For more information, please visit www.tides.org.

Position Summary
We are seeking a service-oriented and self-motivated professional to join Tides’ IT team. In this role, you will be responsible for managing our data architecture, optimizing our data flows between existing systems, and working with corss functional teams around continuous improvements. You will be tasked with maintaining our ERDs, helping map and support our integrations, data security, and data governance requirements.

Essential Functions
• Map existing data models on SaaS and internal databases and systems as required.
• Manage change management across systems integrations, workflows, and automations to assure proper data management and protections are stabilized.
• Partner with all Tides departments on refining data needs and contribution across the systems, process, and people continuous improvement model.
• Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Drive data classification, handling, and security efforts across the organization and with our partnerships.
• Remain effective and process-oriented with great documentation skills.

Knowledge, Skills, and Abilities
•Strong organizational, problem-solving, and analytical skills; ability to manage situations, reprioritize, and meet demanding deadlines
•Strong analytical skills around datasets and correlation
•Project management skills around scoping, timelines, and expectation management
•Demonstrated experience around data mapping, ERD development, and integrations management
•Excellent verbal and written communication skills
•Ability to work collaboratively with a diversity of individuals at all organizational levels
•SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
•Excellent interpersonal skills with proven ability to develop effective working relationships with individuals possessing a variety of communication styles in a multicultural environment
•Ability to make timely and sound decisions and maintain confidentiality
•Experience working for and/or with nonprofits is a plus
•Proven ability to provide exceptional customer service to a variety of clients with varying degrees of technical expertise
•Ability to diagnose and resolve wide-ranging technical issues
•Proﬁciency in English
•Experience with schema design and dimensional data modeling
•Ability in managing and communicating data warehouse plans to internal clients
•Experience designing, building, and maintaining data processing systems

Education and Experience

• 3+ years of experience in a data management role
• Bachelor’s Degree in Computer Science, Statistics, Informatics, Information Systems, or a similar field
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Experience building with big data tools and analytics systems

Application Instructions
Please submit a resume and a thoughtful cover letter online. Your cover letter should express your interest in working for Tides and your qualifications for the role. You may also share your detailed LinkedIn profile with us. Tides is an Equal Opportunity employer. We value diversity and inclusion and we look forward to reviewing applications from all who are qualified to apply.

Equal Employment Opportunity
Tides is an equal opportunity employer. We strongly encourage applications from women, people of color, and bilingual and bicultural individuals, as well as members of the lesbian, gay, bisexual, and transgender communities. Applicants shall not be discriminated against because of race, religion, sex, national origin, ethnicity, age, disability, political affiliation, sexual orientation, gender identity, color, marital status, or medical condition including acquired immune deficiency syndrome (AIDS) and AIDS-related conditions. Also pursuant to the San Francisco Fair Chance Ordinance, we encourage and will consider for employment qualified applicants with arrest and conviction records.

Applicants with Disabilities
Reasonable accommodation will be made so that qualified disabled applicants may participate in the application process. Please advise in writing of special needs at the time of application.",3.6,"Tides
3.6","San Francisco, CA",-1,51 to 200 Employees,1976,Nonprofit Organization,Grantmaking Foundations,Non-Profit,$100 to $500 million (USD),-1
Data Engineer,$91k-$159k (Glassdoor Est.),"Do you believe that creators should have the ability to get paid for the value they give to their fans?


We do, which is why we're building Patreon, a platform that powers membership services for creators with established followings. Patreon strives to provide creators with insight, education, and tools that make it possible to retain creative control while running their creative business, so creators can focus on creating and energizing their fanbases.

Our user base has doubled in the last year alone, and we have paid over $500 million directly to creators on our platform. In order to support this level of growth, Patreon is looking for a Data Engineer.

What you will do:
Maintain and improve our data warehouse
Organize and optimize raw data
Develop new data pipelines and maintain existing pipelines
Collaborate with data scientists on data analysis and query creation/optimization
Skills and experience you possess:


3+ years experience in a Data Engineer role
2+ years experience writing Python or some other object oriented programming language
Experience building and maintaining data warehouses
Experience building, maintaining and optimizing data pipelines
Experience with data pipeline and workflow management tools such as Airflow, Luigi
Advanced SQL knowledge
Basic knowledge of Infrastructure as Code and configuration management systems (Chef, Puppet, or Ansible)
Experience with AWS big data tools such as Redshift (Redshift Spectrum, Athena, Kinesis, EMR) a plus
Experience with big data technologies such as Kafka, Spark a plus
Knowledge of MySQL a plus
Projects you may work on:


A new Machine Learning Pipeline for serving Data Science and the Product
Migrating our data warehouse from Redshift to Snowflake
Creating a new Sparking Streaming Pipeline for Creator Insights
Any other data needs from the product or our data scientists.
What you will have the chance to learn:
Data Warehouse and Pipelining best practices
On a team of 2-3 determining the long-term data architecture of Patreon.
AWS Infrastructure, Data science or Machine Learning if you so choose.
Who you'll work with:


At Patreon, you'll join a high-performing and highly-empathetic team of people who proudly work on fulfilling our mission of funding the creative class. Our culture of creator-first, thoughtful teammates keeps work creative, stretching, and rewarding.

Our Core Behaviors:
Put Creators First. Patreon is nothing without our creators.
Achieve Ambitious Outcomes. Set, measure, and accomplish goals that deliver massive value to our creators and patrons.
Cultivate Inclusion. We want an environment that retains and engages the diverse teams we build.
Bias Towards Action. When in doubt, we take the next best step, then course correct when needed. We go out of our way to fix problems when we see them. We take ownership seriously.
Be Candid and Kind. Be extremely caring and extremely direct in all you do at Patreon, especially when it comes to giving positive and constructive feedback.
Be Curious. You don't know it all, and that's the fun part. Everything gets better when you're curious. Things get more interesting, more clear, and more approachable. When you bring curiosity into the workplace, you're growing yourself, your teammates, and Patreon as a whole.
Want to learn more about Patreon?
Check out our reviews on Glassdoor
Check to see if you know a Patreon teammate on LinkedIn
#LI-KM2",3.2,"Patreon
3.2","San Francisco, CA",-1,201 to 500 Employees,2013,Company - Private,Publishing,Media,Unknown / Non-Applicable,-1
Data Engineer,-1,"At Phantom AI, experience the fast paced environment and help the development of the next generation of autonomous vehicles. We are seeking a Data Engineer to help our visionary team through building a robust pipeline and handling terabytes of data.
You will be designing and implementing full-stack solutions for computing, storing, and presenting evaluation metrics using data gathered from vehicles. This will be a unique opportunity to both improve and implement a scalable pipeline that will be the backbone of the company.

Responsibilities
· Design, improve, and manage datasets
· Support technical design and implementation of demand planning automation projects that use these datasets
· Monitor and enforce data quality standards
· Hands-on coding, able to implement what was designed
· Familiar with architecting and designing tools, and self-motivated for documentation
· Optimize pipeline performance to reduce data lag and refresh time

Requirements
· 3+ years of experience working in data architecture or a related field
· 3+ years of Python Experience
· Web framework experience
· Experience building and optimizing data sets
· Deep understanding of data architecture best practices such as normalizing data structures, minimizing logic complexity, defining intuitive and consistent semantic layers, and eliminating redundancy
· Self-motivated and strong work ethic for continuous development with a strong sense of ownership

Preferred Skills
· Master’s degree in Computer Science or Data Science or work equivalent experience
· Experience with ROS
· Web development experience
· Hands-on experience in dev-ops best practices and implementations
· Demonstrated industry efficiency in the fields of database, data warehousing or data sciences",5.0,"Phantom AI
5.0","Burlingame, CA",-1,1 to 50 Employees,2016,Company - Private,Computer Hardware & Software,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,-1,"Airtable's unique approach to enabling end-user software creation has struck a chord with users across many industries and use cases. Our accelerating present and future growth, coupled with our ambitious product surface area, brings many challenges. Data engineering can play a critical role in understanding how people use the open-ended toolkit that Airtable offers, thereby enabling our team to improve users' experience and accelerate their rate of success.

As one of the first data engineers at Airtable, you'll make an enormous contribution to our burgeoning data engineering efforts. You'll design and own mission-critical data pipelines to enable decision-making, partner with company leaders to create scalable data solutions and launch innovative alerting and visualization solutions.

What you'll do
Work between our engineering organization and stakeholders from our data science, growth, sales, marketing, and product teams, to understand the data needs of the business and produce pipelines, data marts and other data solutions that enable better product and growth decision-making.
Design and update our foundational business tables in order to simplify analysis across the entire company.
Continue to improve the performance and reliability of our data-warehouse.
Build and enforce a pattern language across our data stack, ensuring that our event taxonomy and tables are consistent, accurate, and well-understood.
Who you are
You're passionate and thoughtful about building systems that enhance human understanding.
You have professional experience designing, creating and maintaining scalable data pipelines.
You've wrangled enough data to understand how often the complex systems that produce data can go wrong.
You can write clear, correct code in at least one programming language, and are willing to become effective in others as needed to get your job done.
You are highly effective with SQL and understand how to write and tune complex queries.
You communicate with clarity and precision in written form; experience communicating with graphs and plots, or at least understanding how to enable other people to do this, is a big bonus.
What we offer
Health care: we have you 100% covered (and your dependents 50% covered) with competitive medical, dental, and vision insurance. You'll also be eligible for a complimentary membership to One Medical Group
Learning & Development: we offer a $2,000 per year stipend for your personal career development
Gym Membership: we're proud to provide employees in our San Francisco and New York offices with complimentary gym memberships to Equinox, or up to $100/month reimbursement towards any other gym
Catered lunches: we have high-quality catered lunches every day and well-stocked kitchens. We'll also reimburse you for any reasonable food expenses incurred while working
Generous PTO, sick leave, and parental leave
About Airtable


Airtable is working on the next computer revolution: one where anyone even without technical training can create customized applications that fit their needs, build more interconnected teams, and take part in a growing community of people who share what they create. Founded in 2013 and headquartered in San Francisco, Airtable powers teams at more than 200,000 organizations around the world. Our recent Series D funding round, which included Thrive Capital, Coatue, and Benchmark, doubled our total investment to more than $350 million. And we're just getting started.",4.5,"Airtable
4.5","San Francisco, CA",-1,51 to 200 Employees,2013,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,$108k-$192k (Glassdoor Est.),"Data Engineer

The data team designs, implements and scales data pipelines that transform raw data into actionable models and metrics that enable insight. This team owns the pipelines that transport and process database data from all of Asana's product surfaces. We build and operate the infrastructure and services that ensure data accuracy and data availability for data scientists, analysts, and business and product teams.

We are looking for a data engineer to found our product data engineering team. This team will have ownership of the core data pipelines powering Asana's metrics and partner with our existing data infrastructure team, data science team, and cross-functional partners to help evolve our analytic data model as data volume grows and new needs arise to support the growth of the business.

What you'll achieve
Architect, build, and launch scalable data pipelines to support Asana's growing data processing and analytics needs. Some upcoming projects you may work on include designing and implementing pipelines that deliver data with high quality, consistency, and timeliness. Evangelize high-quality software engineering practices for building data infrastructure and pipelines at scale.
Produce foundational data tables and metrics with clear definitions, lineage, and test coverage to ensure that data is reliable, intelligible, and maintainable
Understand and influence logging frameworks and practices to support our data flow, architecting logging best practices where needed
Implement systems to track data quality and consistency
Partner with business domain experts, data scientists and analysts, and engineering teams to build a roadmap for foundational data sets that are aligned with business goals and that enable self-service. Some upcoming projects you may work on include evolving data models and schemas based on business, product, and engineering needs to facilitate data-driven decisions and features across Asana. Drive the collection of new data and the refinement of existing data sources; develop relationships with product engineering teams to manage our analytic data model as the Asana product evolves.
About you
Bachelor's degree in Computer Science, Math, Statistics, Engineering, or a related quantitative field, or equivalent experience
5+ years of industry experience in software engineering or data engineering
Experience using coding languages Python, Java, Scala
Experience using workflow management engines (e.g. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
Experience with Hadoop or similar ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Effective in working across team boundaries to establish overarching data architecture, and provide guidance to individual teams
Expertise in relational databases and query authoring (SQL)
About us

At Asana, we're building a better way to work, fueled by transparency, inclusion, and technology that is a force for positive change. Asana is a work management platform that helps teams orchestrate their work, from daily tasks to strategic initiatives, so they can move faster and accomplish more with less. For the past 5 years, we've been named a top workplace, including top 10 Great Place to Work Best Small & Medium Workplaces, #1 Fortune Best Workplaces in the Bay Area, #8 Fortune Best Workplaces for Women, and one of Ireland's Best Workplaces. With offices all over the world, we are always looking for curious, collaborative, and mission-driven people to help us enable the world's teams to work together effortlessly.

We believe in supporting people to do their best work and thrive, and building a diverse, equitable, and inclusive company is core to our mission. Our goal is to ensure that Asana upholds an inclusive environment where all people feel that they are equally respected and valued, whether they are applying for an open position or working at the company. We welcome applicants of any educational background, gender identity and expression, sexual orientation, religion, ethnicity, age, citizenship, socioeconomic status, disability, and veteran status.

More information:
Rethinking the org chart: Areas of Responsibility (AoRs)
Distributed responsibility: An engineering manager's perspective
The Pyramid of Clarity",4.9,"Asana
4.9","San Francisco, CA",-1,501 to 1000 Employees,2009,Company - Public,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Autonomous Driving Data Engineer,$99k-$178k (Glassdoor Est.),"Leading the future of luxury mobility

Lucid’s mission is to inspire the adoption of sustainable energy by creating the most captivating luxury electric vehicles, centered around the human experience. Working at Lucid Motors means having a shared vision to power the future in revolutionary ways. Be part of a once-in-a-lifetime opportunity to transform the automotive industry.

We are currently seeking an experienced Big Data Engineer to help building up our large-scale data management system for the ADAS / Autonomous Driving team. This field allows contributions on various levels around the data pipeline and data processing, data annotation, machine learning, data ingestion, data pre-processing, integration of Deep Neural Network frameworks, efficient data search and high performance data manipulation.
You Will:
Work with a state-of-the-art data pipeline on the development and test process of the ADAS/AD features
Enable efficient data (pre-)processing in our cloud infrastructure
Contribute to the architecture and implementation of an efficient data ingestion process to transfer and store large-scale vehicle sensor data from our Lucid Air test vehicles efficiently into our data storage
Help to optimize the data storage, search and data processing based on a diverse mix of storage solutions, caching techniques and big data frameworks.
Contribute to the implementation of a secure distributed data access solution for data on-premise and via cloud providers to connect Lucid data with service providers and partners
Work with machine learning experts, testing engineers and software developers to solve challenging development problems
You Bring:
Knowledge of database systems, big data concepts and cluster computing frameworks (e.g. Spark, Hadoop, or other tools)
Excellence in C++, Python, Java or other related programming languages
Previous experience with big data applications or back-end software development
Experience with machine learning, data engineering, deep learning frameworks and related open-loop testing techniques
Strong communication skills
Great to Have:
Previous experience building efficient large-scale data collection, storage and processing pipelines
Understanding of cyber-security issues and best practices
Advanced degrees preferred
Education:
BS minimum in the areas of Computer Science/Engineering, Data Engineering or other related fields
4+ years of work experience or a PhD in a related field
LMHP

Be part of something amazing

Come work alongside some of the most accomplished minds in the industry. Beyond providing competitive salaries, we’re providing a community for innovators who want to make an immediate and significant impact. If you are driven to create a better, more sustainable future, then this is the right place for you.

At Lucid, we don’t just welcome diversity - we celebrate it! Lucid Motors is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, national or ethnic origin, age, religion, disability, sexual orientation, gender, gender identity and expression, marital status, and any other characteristic protected under applicable State or Federal laws and regulations.

To all recruitment agencies: Lucid Motors does not accept agency resumes. Please do not forward resumes to our careers alias or other Lucid Motors employees. Lucid Motors is not responsible for any fees related to unsolicited resumes.",3.9,"Lucid Motors
3.9","Newark, CA",-1,1001 to 5000 Employees,2007,Company - Private,Transportation Equipment Manufacturing,Manufacturing,Unknown / Non-Applicable,-1
Data Engineer,$99k-$178k (Glassdoor Est.),"Tribe and Karma of the Team:


At Segment, we believe that good data is the foundation of good decision-making. Too often, we see product managers and marketers with siloed or conflicting pieces of the picture, and engineers who struggle to keep up with evolving data needs. Segment standardizes and streamlines data infrastructure using a single platform that collects, unifies, and sends data to hundreds of business tools with the flip of a switch. We ensure that all of our customers are equipped with a holistic and detailed view of their customers, unlocking digital transformations, personalized marketing, and a data-driven future.

Data Engineering enables Segment to derive insights about our customers and product usage effectively and efficiently, and it is the backbone of all data-driven decisions we make to move the business forward.

This is a rare opportunity to be one of the founding members of Segment's internal Data Engineering team. As a founding data engineer, you will be part of the core team responsible for building out our data lake, processing many terabytes of data, and developing efficient ETL pipelines to deliver data and insights to partners across the company. Additionally, you have the opportunity to provide feedback to Product and Engineering teams that will help shape the future of our products.

Drive and Focus of the Role:

Design, build and launch efficient and reliable data pipelines for ingesting and transforming data from internal and cloud applications
Assist partners in Analytics, Product, and Go-to-Market teams to with their data transformation and infrastructure needs
Optimize Segment's internal data storage and compute resources to improve performance, reliability, and availability of the data
Execute on our data lake and self-service strategies
Own and maintain Segment's internal data infrastructure assets primarily hosted on AWS
Build back-end data services for internal applications
Build data engineering tools and frameworks to enable common data processing patterns at scale
What we're looking for:

2+ years of hands-on experience with Python, Java, or Scala for data processing
2+ years of data or software engineering experience
Knowledge of data pipelining and workflow management tools such as Airflow
Working experience with data warehouses such as Snowflake
Advanced working SQL experience
Experience building big data (terabyte scale) processing pipelines using a distributed data processing engine/framework
Hands-on experience with versioning, continuous integration, and build & deployment tools and platforms such as Github and Buildkite
Familiar with dimensional data modeling and data normalization
Familiar with data-lake architecture and data serialization formats such as JSON and Parquet
Exposure working in a dynamic and fast-paced shop
BS or MS degree in Computer Science or a related technical field

Segment is an equal opportunity employer. We believe that everyone should receive equal consideration and treatment in all terms and conditions of employment regardless of sex, gender (including pregnancy, childbirth, breastfeeding or related medical conditions), sexual orientation, gender identity, gender expression, race, color, religion, creed, national origin, ancestry, age (over 40), physical disability, mental disability, medical condition, genetic information, marital status, domestic partner status, military or veteran status, height, weight, AIDS/HIV status, and any other protected category under federal, state or local law. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.",4.2,"Segment
4.2","San Francisco, CA",-1,201 to 500 Employees,2011,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Big Data Engineer,$92k-$168k (Glassdoor Est.),"Job Title:
Big Data Engineer

Location:
US, California, San Jose
Role Overview:


You will work with a team of Software Engineers to create the next generation of McAfee security products to enhance the auto detection and investigation of security breaches within large organizations. You will work with Architects and other Cloud Infrastructure Engineers to understand the designs, validate the features, and to deliver and integrate solutions in an AWS environment. If you are passionate about building security solutions that keep millions of enterprise customers safe with an outstanding user experience, then this position might be a perfect opportunity for you. You will report to the Sr. Manager, Software Engineering and will be based in San Jose, CA.



Company Overview


From device to cloud, McAfee provides market-leading cybersecurity solutions for both business and consumers. We help businesses orchestrate cyber environments that are truly integrated, where protection, detection, and correction of security threats happen simultaneously. For consumers, McAfee secures your devices against viruses, malware, and other threats, both at home and away. We want to continue to shape the future of cybersecurity by working together to build best in class products and solutions.

About the Role:
Help develop our next-generation micro-services to enhance auto detection of different breaches and other security concerns.
Use your knowledge of Java, Go or Python to create new features in the cloud.
Work with other software developers and be responsible for the whole solution including uptime and design of the product.
Work with other developers to fix bugs, debug issues and achieve resolution.
Understand and influence logging to support our Data Flow.
Pioneer a new way of thinking about Data Pipelines, Orchestration and Configuration at McAfee.
About You:
Your experience includes 5+ years of hands-on experience with Big Data technology(Kafka, Spark or Airflow).
Successful track record in developing and automating large-scale, high-performance data processing systems (batch and streaming).
Design data models for optimal storage and retrieval to meet critical product requirements.
Experience with both scripting and system programming languages (Python, Go and Scala).
Experience with microservices including defining and testing APIs.
Company Benefits and Perks:


We work hard to embrace diversity and inclusion and encourage everyone at McAfee to bring their authentic selves to work every day. We offer a variety of social programs, flexible work hours and family-friendly benefits to all of our employees.
Pension and Retirement Plans
Medical, Dental and Vision Coverage
Paid Time Off
Paid Parental Leave
Support for Community Involvement
We're serious about our commitment to diversity which is why McAfee prohibits discrimination based on race, color, religion, gender, national origin, age, disability, veteran status, marital status, pregnancy, gender expression or identity, sexual orientation or any other legally protected status.

Job Type:


Experienced Hire

Primary Location:
US, California, San Jose

Additional Locations:",3.5,"McAfee
3.5","San Jose, CA",-1,5001 to 10000 Employees,1987,Company - Private,Computer Hardware & Software,Information Technology,$10+ billion (USD),-1
Data Engineer,$108k-$199k (Glassdoor Est.),"Grow your career at AppLovin.

AppLovin is a global leader in mobile entertainment. Its studios create popular, immersive mobile games and its technology brings games to more players around the world. Since 2012, the company's platform has been instrumental in driving the explosive growth of mobile games, resulting in a richer ecosystem and more games played by millions of people every day. AppLovin is headquartered in Palo Alto, California with several offices globally. Learn more at applovin.com.

AppLovin is one of Inc.'s Best Workplaces and a recipient of the 2019 Glassdoor Top CEO employee's choice award. The San Francisco Business Times awarded AppLovin one of the Bay Area's Best Places to Work in 2019 and 2020, and the Workplace Wellness Award in 2019 which recognizes businesses that are leaders in improving worker well-being.

About You:
Have 1-3 years of experience and a minimum of a BS and/or MS in Computer Science
Have excellent knowledge of computer science fundamentals including data structures, algorithms, and coding
Experience independently creating and maintaining projects
Product focused mindset
Have experience working with big data systems (Spark, Hadoop, Pig, Impala, Kafka)
Experience designing, building, and maintaining data processing systems
Experience with a backend language such as Java or Scala
About the Role:
Collaborate with various engineering teams to meet a wide range of technological challenges
Work closely with product and business teams to improve data models that feed business intelligence tools
Define company data models using Spark
Perks:
Free medical, dental, and vision insurance
Daily lunches and fully stocked kitchen
Free public transit
Free laundry service (wash/dry clean)
Free gym membership
401k matching
Fun company parties and events
Autonomy to make decisions in a rapidly growing company
Flexible Time Off - work hard and take time when you need it
Interested? Send us your resume and let's talk!

AppLovin is proud to be an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.

#LI-JZ1",4.8,"AppLovin
4.8","Palo Alto, CA",-1,501 to 1000 Employees,2012,Company - Private,Video Games,Media,Unknown / Non-Applicable,-1
Senior Data Engineer,$123k-$217k (Glassdoor Est.),"Faire is an online wholesale marketplace built on the belief that the future is local there are over 1M independent retailers in the U.S. and Canada doing more than twice the revenue of Walmart and Amazon combined. At Faire, we're using the power of tech, data, and machine learning to connect a thriving community of over 100,000 brands and local retailers around the world. Picture your favorite boutique in town we help them discover the best products to sell in their stores. With the right tools and insights, we believe that we can level the playing field so that small businesses can compete with these big box and ecommerce giants. We're looking for smart, resourceful and passionate people to join us as we power the shop local movement.

Job Description

The Data Engineering team is the backbone of all data-related processes and enables the Data Science team to develop and deploy a wide variety of algorithms and models that power the marketplace. Our infrastructure is used by the whole company for analytics, reporting, forecasting and research. We care about having a reliable infrastructure with quality data and building machine learning models that help our customers thrive.

As a Senior Data Engineer you'll be responsible for developing and automating large scale, high-performance data storage and processing systems.

Our team already includes experienced Data Scientists and Engineers from Airbnb, Facebook, Quora, Square, Uber, TripAdvisor, and Overstock. Faire will soon be known as a top destination for data science and machine learning, and you will help take us there!

What you will be doing:
Develop our machine learning infrastructure to help us scale for where we're going over the next several years
Manage our data infrastructure and ETL platform
What it takes:
4+ years experience in a Data Engineering role
Strong skills in Python, Git, Docker, SQL, Airflow, ETL pipelines
Familiarity with at least one of: Hive, Presto, Snowflake, AWS Redshift, BigQuery,
A passion for programming and solving problems with code
A bachelor's degree in Computer Science/Software Engineering or equivalent industry experience
A love for technology, and an insatiable curiosity for new tools to tackle real problems
Why you'll love working at Faire:
We are entrepreneurs: We believe entrepreneurship is a calling and our mission is to empower entrepreneurs to chase their dreams. Every member of our team is an owner of the business and taking part in the founding process.
We are using tech and machine learning to level the playing field: We are using the power of technology and data to connect brands and boutiques from all over the world, building a thriving community of over 100,000 small business owners.
We build products our customers love: Everything we do is ultimately in the service of helping our customers grow their business because our goal is to grow the pie - not steal a piece from it. Running a small business is hard work, but using Faire makes it easy.
We are curious and resourceful: We always find a way to get the job done and come up with creative solutions to whatever problems are standing in our way. People at Faire are insatiably curious. We lead with curiosity and data in our decision making and reason from a first principles mindset.
Faire was founded in 2017 by a team of early product and engineering leads from Square. We're backed by some of the top investors in retail and tech including: Y Combinator, Lightspeed Venture Partners, Forerunner Ventures, Khosla Ventures, Sequoia Capital, Founders Fund, and DST Global. We have offices in San Francisco, Kitchener-Waterloo, and Salt Lake City. We're looking for smart, resourceful and passionate people to join us as we power the shop local movement. To learn more about Faire and our customers, you can read more on our blog.

Faire is being built for entrepreneurs, by entrepreneurs.

Additional Information

Faire provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, sexual orientation, gender identity or gender expression.",4.3,"Faire
4.3","San Francisco, CA",-1,201 to 500 Employees,2016,Company - Private,Wholesale,Business Services,Unknown / Non-Applicable,-1
"Data Engineer, Analytics",$115k-$187k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Do you like working with big data? Do you want to use data to influence product decisions for products being used by hundreds of millions of people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. You will be working with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.

This is a full time position based in our office in New York.
Inform, influence, support, and execute our product decisions and product launches
Manage data warehouse plans for a product or a group of products.
Interface with engineers, product managers and product analysts to understand data needs.
Partner with Product and Engineering teams to solve problems and identify trends and opportunities.
Build data expertise and own data quality for allocated areas of ownership.
Design, build and launch new data extraction, transformation and loading processes in production.
Support existing processes running in production.
Define and manage SLA for all data sets in allocated areas of ownership.
Work with data infrastructure to triage infra issues and drive to resolution.
BS/BA in Technical Field, Computer Science or Mathematics.
4+ years experience in the data warehouse space.
4+ years experience in custom ETL design, implementation and maintenance.
4+ years experience working with either a Map Reduce or an MPP system.
4+ years experience with schema design and dimensional data modeling.
4+ years experience in writing SQL statements.
Ability to analyze data to identify deliverables, gaps and inconsistencies.
Communication skills including the ability to identify and communicate data driven insights.
Ability in managing and communicating data warehouse plans to internal clients.
4+ years experience using Python or Java
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","San Francisco, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Senior Data Engineer,$146k-$260k (Glassdoor Est.),"Roofstock is the leading marketplace for investing in single-family rental homes that cash flow day one. With over $2B in transactions, our mission is to make real estate investing radically accessible, cost effective and simple. We want to use technology to transform the way real estate is bought and sold and make real estate investing as simple as investing in stocks. Simply put, we are passionate about helping our customers build wealth through real estate.

Roofstock has been recognized as a great workplace by Glassdoor and Great Place to Work® and was recently named to the Forbes Fintech 50 and the Red Herring 100 lists of most innovative companies. Roofstock has raised over $83M to date, is based in Oakland, CA and Dallas,TX with approximately 200 people and is growing rapidly. Check out our reviews and see why our employees love working here!

We are looking for a talented Senior Data Engineer to join our established, small but quickly growing company, working on a wide range of data projects in close collaboration with other data engineers, data scientists, and data analysts in an integrated Data Team. We use a modern all-cloud data stack including Airflow, Docker, DBT, Python, Snowflake, Tableau, Sigma, and our old friend SQL.

If you thrive in a team environment, are willing to pitch in wherever needed to help the team succeed, are passionate about data and excited about empowering users with data to drive decision making, Roofstock is the place for you.
What you will do:
Improve and maintain the data infrastructure
Design, implement and deploy, scalable, fault-tolerant pipelines that ingest, and refine large diverse (structured, semi-structured and unstructured datasets) into simplified accessible data models in production
Built departmental data-marts for supporting analytics across the company
Collaborate with cross-functional teams to understand data flows and processes to enable design and creation of the best possible solutions to each engineering challenge
Provide quality data solutions in a timely manner and be responsible for data governance and integrity while meeting objectives and maintaining SLAs
Build tools and fundamental data sets that encourage self-service
What you bring with you:
BS or MS in a technical field: computer science, engineering or similar
8+ years professional experience working with both relational and analytical databases (preferably SQL Server, PostgresSQL, Snowflake)
5+ years professional experience in building robust data pipelines (beyond simple API pulls) and writing ETL/ELT code (SSIS, Informatica, python code, shell scripts, complex SQL)
3+ years of experience working with Airflow, AWS/Azure
Expert understanding of the SDLC, data warehousing concepts and dimensional data modeling
Advanced skills related to database management, administration and security
Experience building and deploying data-related infrastructure (messaging, storage, compute, transform, execution via docker, and/or CI/CD pipelines across dev/stage/prod)
Experience in DBT is a plus
Strong communication and interpersonal skills
Roofstock is an equal opportunity employer. In keeping with the values of Roofstock, we make all employment decisions including hiring, evaluation, termination, promotional and training opportunities, without regard to race, religion, color, sex, age, national origin, ancestry, sexual orientation, physical handicap, mental disability, medical condition, disability, gender or identity or expression, pregnancy or pregnancy-related condition, marital status, height and/or weight.",4.3,"Roofstock
4.3","Oakland, CA",-1,51 to 200 Employees,2015,Company - Private,Real Estate,Real Estate,$10 to $25 million (USD),-1
Senior Data Engineer,$91k-$166k (Glassdoor Est.),"Our Data Warehouse team is looking to add a Senior Data Engineer. Qualified applicants will have 5-7 years’ experience in a Data Engineer position with a background supporting data transformation, data structures, metadata, dependency, and workload management. Ideal candidates will be experts in Talend, Python, Snowflake, CID and SQL.

Here's what you can expect from the job and what you need to be successful:

Job duties
Create and maintain optimal data pipeline architecture, including building a data pipeline infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL , cloud based relational or non-relational databases employing Talend, and/or scripting languages like Perl/Python
Build complex data platforms, and large scale CI/CD data pipelines utilizing a variety of technologies (REST APIs, Teamcity, Jennkis) and cloud databases (Snowflake)
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with data and analytics experts to strive for greater functionality in the organization’s data integration platform
Collaborate with technical staff to identify, learn, and understand software problems
Follow established configuration/change control processes
Identify options for potential solutions and assessing them for both technical and business suitability
Work closely with peers, stakeholders, and end users to ensure technical compatibility and user satisfaction
Essential Skills
A minimum of 5 years of developing ETL processes using Talendnformatica including the processing of NoSQL and JSON formats. Talend Certification preferred.
Strong skills with Ant, Maven, GIT, Jenkins, TeamCity.
Experience in writing Jenkins, Teamcity pipeline groovy, PowerShell scripts
Strong skills in source code repository tools such as Clear Case, SVN, CVS, and Git.
Advanced working knowledge in scripting languages, including Python, Perl and Shell.
Deep understanding of ETL, ELT, Talend, Informatica, Hadoop
Demonstrate independence, creativity, and initiative; as well as the ability to master the company's product architecture and its business goals
Experience in working in an Agile project management environment
Excellent problem solving and critical thinking skills
Superb verbal and written communication skills
Location: San Jose, CA 95134

First Tech is not currently offering Visa sponsorship for this position",3.2,"First Tech Federal Credit Union
3.2","San Jose, CA",-1,1001 to 5000 Employees,1952,Company - Private,Banks & Credit Unions,Finance,$100 to $500 million (USD),-1
Data Engineer SQL,$81k-$147k (Glassdoor Est.),"The Role We are looking for a Data Engineer to be part of our Applications Engineering team. This person will design, develop, maintain and support our Enterprise Data Warehouse & BI platform within Tesla using various data & BI tools, this position offers unique opportunity to make significant impact to the entire organization in developing data tools and driving data driven culture. Responsibilities: Work in a time constrained environment to analyze, design, develop and deliver Enterprise Data Warehouse solutions for Tesla’s Finance and Accounting teams Work on ETL tools like SSIS and Informatica, Business Intelligence & Reporting tools like SSRS, SSAS and Tableau Work with systems that handle sensitive data with strict SOX controls and change management processes Develop collaborative relationships with key business sponsors and IT resources for the efficient resolution of work requests. Provide timely and accurate estimates for newly proposed functionality enhancements critical situation Communicate technical and business topics, as appropriate, in a 360 degree fashion, when required; communicate using written, verbal and/or presentation materials as necessary. Develop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughs. Utilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practices. Qualifications: Minimum Qualifications: 5+ years of experience in SSIS, Informatica 9.X/10.X on in large/medium scale implementations Must have strong experience in Data Warehouse ETL design and development, methodologies, tools, processes and best practices Experience in Finance functional areas like planning and budgeting, accounting, business intelligence, procure-to-pay, order-to-cash Understanding of SOX controls and audits procedures Strong experience in stellar dashboards and reports creation for C-level executives Preferred Qualifications: 3+ years of development experience in Open Source technologies like Python, Java Experience in Big Data processing using Apache Hadoop/Spark ecosystem applications like Hadoop, Hive, Spark, Kafka and HDFS preferable Excellent query writing skill and communication skills Familiarity with common API’s: REST, SOAP Employee Benefits: As a full time Tesla employee you will receive full benefits from day 1 for you and your dependents. Kaiser and UnitedHealthcare PPO and HSA plans (including infertility coverage) 3 medical plan choices with $0 paycheck contribution Vision & dental plans (including orthodontic coverage) Company paid Life, AD&D, short-term and long-term disability 401(k), Employee Stock Purchase Plans, and other financial benefits Employee Assistance Program, Paid Time Off, and Paid Holidays Back-up childcare and employee discounts Tesla participates in the E-Verify Program",3.5,"Tesla
3.5","Fremont, CA",-1,10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$2 to $5 billion (USD),-1
Sr. Data Engineer,$169k-$291k (Glassdoor Est.),"Company Description

Wish is a mobile e-commerce platform that flips traditional shopping on its head. We connect hundreds of millions of people with the widest selection of delightful, surprising, and—most importantly—affordable products delivered directly to their doors. Each day on Wish, millions of customers in more than 160 countries around the world discover new products. For our over 1 million merchant partners, anyone with a good idea and a mobile phone can instantly tap into a global market.

We're fueled by creating unique products and experiences that give people access to a new type of commerce, where all are welcome. If you’ve been searching for a supportive environment to chase your curiosity and use data to investigate the questions that matter most to you, this is the place.

Job Description

At Wish, our Data Science & Engineering team is composed of Data Scientists, Data Analysts & Data Engineers who focus on centralizing corporate data in order to gain insights, knowledge and scalability, that empower a proactive and rigorous analysis of key business indicators. Our mission is to derive wisdom from data via the application of Data Science.

We have opportunities for talented Data Engineers to build our framework and platform to power all data applications at Wish.

#LI-WISHXDS #HiringNow

What you'll be doing:
Design and implement an efficient and scalable data engineering framework that powers the company's key data applications such as experimentation and metrics reporting.
Design and drive data architecture for future data storage, processing, and applications.
Develop and maintain critical data pipelines.
Qualifications
Minimum 2 years of work experience in data engineering related fields.
Experience and knowledge of modern data warehouse, pipeline and reporting/analytic techniques and tools such as Airflow, presto/hive, Spark.
Familiar with Python and SQL.
Bachelor's degree in Computer Science or related field.
Preferred Qualifications:
4 years of work experience in data engineering related fields.
Experience in building framework to automate and scale workflows.
Experience with data related AWS services such as EMR and S3.
Experience in building web interface and data visualization is a plus.
Additional Information

Wish values diversity and is committed to creating an inclusive work environment. We provide equal employment opportunity for all applicants and employees. We do not discriminate based on any legally-protected class or characteristic. Employment decisions are made based on qualifications, merit, and business needs. If you need assistance or accommodation due to a disability, please let your recruiter know. For job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records.

Individuals applying for positions at Wish, including California residents, can see our privacy policy here.",3.5,"Wish
3.5","San Francisco, CA",-1,501 to 1000 Employees,2011,Company - Private,Other Retail Stores,Retail,$1 to $2 billion (USD),-1
"Data Engineer, Growth",$102k-$180k (Glassdoor Est.),"About Pinterest: Millions of people across the world come to Pinterest to find new ideas every day. It’s where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. As a Pinterest employee, you’ll be challenged to take on work that upholds this mission and pushes Pinterest forward. You’ll grow as a person and leader in your field, all the while helping users make their lives better in the positive corner of the internet. The Pinner Performance Marketing team works to bring high quality users to Pinterest via relevant, targeted advertisements on a variety of social media and other advertising platforms. Performance Marketing is an incredibly complex and interesting area of work, bringing together data engineering with complex Machine Learning and other forms of data modeling. These models leverage billions of Pinner signals to target millions of users across the internet. Beyond the purely engineering aspects, it also extends to optimizing creative text and images and deeply understanding the needs of our users. This is a role that will challenge not only your technical skills but also your creative thinking and communication skills due to the wide array of disciplines required to be successful in this space. What you’ll do: Develop scalable and reliable workflows that efficiently process big data Build marketing automation that manages media spend across dozens of advertising strategies Partner with data scientists on machine learning and statistical models to predict user revenue and retention What we’re looking for: 3+ years of data engineering experience, preferably in Growth teams Strong experience in writing reliable, low-maintenance, and powerful code that may be used by many other engineers Desire to understand the marketing/advertising space and learn metrics-driven approach to software development Interest in working closely with marketers, designers, product managers, data scientists, and other partners to bring business vision & growth alive #LI-PB1219",3.9,"Pinterest
3.9","San Francisco, CA",-1,1001 to 5000 Employees,2010,Company - Public,Internet,Information Technology,$100 to $500 million (USD),-1
Data Engineer,$118k-$192k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

At Facebook, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Analytics team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Facebook's Data Center organization. You will be responsible for creating the technology that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise and provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team and located in Fremont, CA.
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Collaboration with the Data Center SMEs, Data Scientists, and Program Managers
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data
BS/MS in Computer Science or a related technical field
7+ years of SQL (Oracle, Vertica, Hive, etc.) experience and relational databases experience (Oracle, MySQL)
7+ years of experience in custom or structured (i.e. Informatica/Talend/Pentaho) ETL design, implementation and maintenance
7+ years experience in data engineering, experience in applying DWH/ETL best practices
7+ years of Java and/or Python development experience
2+ years experience in LAMP and the Big Data stack environments (Hadoop, MapReduce, Hive)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Fremont, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Data Engineer,$96k-$173k (Glassdoor Est.),"Data Engineer San Francisco, CA or Remote Our San Francisco-based team is committed to enhancing physician and patient quality of life through Elation: our cloud-based, clinical platform. Since inception, we've been focused on building a world-class Electronic Health Records (EHR) solution that creates an experience of delight and ease for physicians, and that our users love. We have enjoyed considerable growth, and we are now looking for an experienced Data Engineer to join our team. As a Data Engineer at Elation, you will be responsible for the lion's share of technical data activities. This includes customer data on-boarding, tools and process around each stage of data ingest, and creating a scalable data import pipeline for use by adjacent teams. We're counting on you to have both the skills to execute on our current process, but also to identify how to make it better, and to build that future! Responsibilities: Collaborate with our Implementation, Onboarding, Engineering, and Operations teams to perform data imports that migrate large datasets of patient data into Elation Arm yourself with an expert understanding of the current Elation ecosystem of tools and process so that you can develop, build, and utilize systems and tools of your own design to improve the experience of data import activities at Elation Undertake technical troubleshooting of existing tools and process to successfully debug issues with data import activities Collaborate with other engineering teams to build out pipelines for data imports (and more!) Identify gaps in Elation's current data pipelines and develop strategies and tools that apply autonomy and speed to those workflows Work as a champion for data security and patient privacy by marrying Elation's security principles to the tools and process that you build Requirements: Proven track record of solving data focused problems and building tools to enhance data workflows Prior experience working with (& building) data transformation tool sets & generalist ETL Proficient skill programming in Python Passion for improvement and growth in all that you do Strong analytical, troubleshooting, and communication skills Bonus Points: Prior experience creating advanced data pipelines Prior experience working with Airflow Prior experience working with Django Prior experience working in a healthcare focused space Prior experience working in Amazon Web Services If this is you and you're as excited as we are to expand the community of clinicians and patients introduced to the delight of Elation, email your resume to jobs@elationhealth.com to get the conversation started!",4.2,"Elation Health
4.2","San Francisco, CA",-1,51 to 200 Employees,2010,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Data Engineer,-1,"Acara Solutions is looking for an Data Engineer for our Client located in Sunnyvale, CA
Designs, builds and oversees the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources.
Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms
Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis.
Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements.
Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs.

Required Skills / Qualifications:
Bachelors Degree
Minimum of 8 years experience as Data Engineer
Minimum of 4 years experience working on data using SQL on multiple platforms (SQL server, my SQL, Mongo)
Minimum of 5 years experience with Python and C# programming
Minimum of 4 years experience with Data analytics (dashboards) and derive insights
Minimum of 3 years experience with Application Development Skills - web based or service based
Minimum of 3 years experience with ETL tools and automation
Preferred Skills / Qualifications:
Minimum of 1 year experience AWS technology stack understanding
Additional Information:
Upon offer of employment, the individual will be subject to a background check and a drug screen.
Aleron companies (Acara Solutions, Aleron Shared Resources, Broadleaf Results, Lume Strategies, TalentRise, Viaduct, and Aleron?s strategic partner, SDI) are Equal Employment Opportunity and Affirmative Action Employers. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity, sexual orientation, national origin, genetic information, sex, age, disability, veteran status, or any other legally protected basis. The Aleron companies welcome and encourage applications from diverse candidates, including people with disabilities. Accommodations are available upon request for applicants taking part in all aspects of the selection process.

Applicants for this position must be legally authorized to work in the United States. This position does not meet the employment requirements for individuals with F-1 OPT STEM work authorization status.",3.0,"Acara Solutions
3.0","Sunnyvale, CA",-1,10000+ Employees,1957,Company - Private,Staffing & Outsourcing,Business Services,Unknown / Non-Applicable,-1
Data Engineer,$102k-$187k (Glassdoor Est.),"Dubbed an ""open-source unicorn"" by Forbes, Confluent is the fastest-growing enterprise subscription company our investors have ever seen. And how are we growing so fast? By pioneering a new technology category with an event streaming platform, which enables companies to leverage their data as a continually updating stream of events, not as static snapshots. This innovation has led Coatue Management, Altimeter Capital and Franklin Templeton to join earlier investors Sequoia Capital, Benchmark, and Index Ventures in the recent Series E financing of a combined $250 million at a $4.5B valuation. Our product has been adopted by Fortune 100 customers across all industries, and we’re being led by the best in the space—our founders were the original creators of Apache Kafka®. We’re looking for talented and amazing team players who want to accelerate our growth, while doing some of the best work of their careers. Join us as we build the next transformative technology platform!

About the Team:

The mission of the Data Science team at Confluent is to serve as the central nervous system of all things data for the company: we build analytics infrastructure, insights, models and tools, to empower data-driven thinking, and optimize every part of the business. This position offers limitless opportunities for an ambitious data engineer to make an immediate and meaningful impact within a hyper growth start-up, and contribute to a highly engaged open source community.

About the Role:
As a Data Engineer, you will take on big data challenges in an agile way. You will build data pipelines that enable data scientists, GTM operation teams, and executives to make data accessible to the entire company. You will also build data models to deliver insightful analytics while ensuring the highest standard in data integrity. You are encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.

This is a partnership-heavy role. As a member of the Field Ops Data Science team, you will enable all the Go-to-Market functions of the company, i.e. Sales, Customer Success, Business Development, Marketing, to be data-driven. This role will be located in Mountain View, CA.
Responsibilities:
Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse and real-time systems
Developing strong subject matter expertise and manage the SLAs for those data pipelines; improve existing or develop new tools to detect data anomalies real time and through offline metrics
Data Modeling - Partner with analytic consumers to improve existing datasets and build new ones
Set up and improve BI tooling and platforms to help the team create dynamic tools and reporting
Partnering with Data Scientists and business partners to develop internal data products to improve operational efficiencies organizationally

What We're Looking For:

3+ years of experience in a Data Engineering role, with a focus on data warehouse technologies, data pipelines and BI tooling
Bachelor or advanced degree in Computer Science, Mathematics, Statistics, Engineering, or related technical disciplineExpert knowledge of SQL and of relational database systems and concepts
Strong knowledge of data architectures and data modeling and data infrastructure ecosystem
Experience with enterprise business systems such as Salesforce, Marketo, Zendesk, etc.
Experience with ETL pipeline tools like Airflow, and with code version control systems like Git
The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
The ability to thrive in a dynamic environment. That means being flexible and willing to jump in and do whatever it takes to be successful.

Additional Highly Desirable Experience:

Experience with Apache Kafka
Knowledge of batch and streaming data architectures
Product mindset to understand business needs, and come up with scalable engineering solutions
#LI-JN1

Come As You Are

At Confluent, equality is a core tenet of our culture. We are committed to building an inclusive global team that represents a variety of backgrounds, perspectives, beliefs, and experiences. The more diverse we are, the richer our community and the broader our impact.

Click here to review our California Candidate Privacy Notice, which describes how and when Confluent, Inc., and its group companies, collects, uses, and shares certain personal information of California job applicants and prospective employees.",4.4,"Confluent
4.4","Mountain View, CA",-1,501 to 1000 Employees,2014,Company - Private,Computer Hardware & Software,Information Technology,$100 to $500 million (USD),-1
Data Engineer,-1,"Data EngineerSunnyvale, CA

Amick Brown is seeking an experienced Data Engineer for our direct client.

Location: Sunnyvale, CA 94086.
Duration: 4 Months with possible extension.

Description:
Designs, builds and oversees the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources.
Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis.
Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements.
Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs.
Required Skills:
4 years of hands-on experience working on data using SQL on multiple platforms (SQL server, my SQL, Mongo) (required)
5 years of hands-on experience with Python and C# programming (required)
4 years of hands-on experience with Data analytics (dashboards) and derive insights
3-4 years of hands-on experience with Application Development Skills - web based or service based (required)
3 years of hands-on experience with ETL tools and automation (required)
1 year of experience AWS technology stack understanding (desirable)
BS degree: 8-9 years of hands-on experience
MS degree- 4-5 years of hands-on experience
Amick Brown is an Information Technology consulting company specializing in ERP, Data Analytics, Information Security, Application Development, Networking, and Cloud Computing. The company was founded in 2010 and is headquartered in San Ramon, California.

Regular full-time employees are eligible for the following Amick Brown provided benefits:
Health
Vision
Dental
401k with company match
Paid time off
Sick Leave
Short-Term Disability
Life Insurance
Wellness & Discount Programs
Contract length: 4 months

Job Types: Full-time, Contract

Schedule:
8 hour shift
Work Remotely:
No",4.9,"Amick Brown, LLC
4.9","Sunnyvale, CA",-1,1 to 50 Employees,-1,Company - Public,-1,-1,$5 to $10 million (USD),-1
Data Engineer,-1,"About Scoop Scoop brings co-workers and neighbors together to enjoy a smooth carpooling experience—unlocking new opportunities to create friendships, improve their well-being, and make the most of their valuable time. Learn more in Crunchbase: https://news.crunchbase.com/news/scoop-raises-60m-for-corporate-carpooling-as-gridlock-ruins-america/ Engineering @ Scoop Few companies get to face such diverse technical challenges as Scoop, and we’ve built a team of people excited to face these challenges together while investing in each others’ growth. Scoop’s engineering team may move bits and pixels, but we also put real, live human beings in cars together. We’re touching problems academics have written about for years, and have data that no other company has ever collected. But Scoop knows engineering is not a lone discipline. We’re a small team with varied backgrounds: big companies, VC-backed startups, bootcamps, academia. We like to build together, and we like to learn together. Our entire team and process are built around helping you grow and be successful. And we’d love to tell you more about the impact you could have at Scoop. In this role, you will: Architect, develop, and deploy infrastructure for large scale ETL pipelines with data processing frameworks Productionizing machine learning - from research into fault-tolerant, production-scale deployments Manage data workflows Work closely with the Data Science, Analytics, Product and Platform teams to understand business needs and create a data platform that serves business needs Tech Lead projects and mentor team members You should: 4+ years or equivalent experience Be proficient in Python and/or Scala (Scala preferred) Experience designing and operating distributed systems Emphasis in high code quality, high code clarity, automated testing, and engineering best practices Be able to clearly communicate complex technical concepts Preferred Qualifications: Mobility/Transportation domain knowledge Worked with open sourced projects like Spark, Kafka or Airflow Life @ Scoop Founded in 2015 and based in downtown SF, our team mixes technology and elbow grease every day, with one statistic in our crosshairs: 80% of Americans drive alone to work. At Scoop, we envision a world where commuters feel empowered - starting with a choice to make their commute a meaningful part of their day. We embody that same spirit within our own culture, empowering every team member to make this the most meaningful experience of their career. The atmosphere overall is dynamic and unique. It’s influenced by our backgrounds at successful startups, big tech companies, and premier consulting firms - blended and crafted into what feels natural and right for this company. It plays out in our balance of scrappy and strategic, frameworks, and fast thinking. This spirit also plays out in our investments in Diversity, Equity, and Inclusion (DEI). In 2019, Scoop established a DEI Committee with a mission reflective of our belief that energy is contagious and being considerate of others is core to how we operate at Scoop. We’re dedicated to building a diverse, equitable, and inclusive culture where all employees feel safe, respected, and a sense of belonging. Our initiatives over the past year have included: offering education on mental health opportunities in the workplace, formalizing Employee Resource Groups, and improving our candidate pool and hiring processes. These efforts have not gone unnoticed; Scoop has been externally recognized for both our leadership and commitment to diversity. At Scoop, we’re all united by our desire to change the way people get to work - and committed to enjoying the journey together along the way.",4.1,"Scoop Technologies
4.1","San Francisco, CA",-1,51 to 200 Employees,2015,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Workato Data Engineer / API Integration resource,$78k-$146k (Glassdoor Est.),"*Title: Workato Data Engineer / API Integration resource*

*Location: San Francisco, California*

*Mode of employment: CWR/FTE*

*Details: *

* Experience with Workato integration tool and can seamlessly integrate with Salesforce, SAP, Workday and several other applications.

SS089565TV

Job Types: Full-time, Contract

Pay: $55.00 - $65.00 per hour

Schedule:
* Monday to Friday

Experience:
* Workato: 1 year (Required)",3.5,"Compunnel Inc.
3.5","San Francisco, CA",-1,1001 to 5000 Employees,1994,Company - Private,IT Services,Information Technology,$100 to $500 million (USD),-1
Staff/Lead Data Engineer,$80k-$144k (Glassdoor Est.),"Description

WHO WE ARE:

Freedom Financial Network is a family of companies that takes a people-first approach to financial services, using technology to empower consumers to overcome debt and create a brighter financial future. The company was founded in 2002 by Brad Stroh and Andrew Housser on the belief that by staying committed to helping people, you can ensure better financial outcomes for both the customer and the business. This Heart + $ philosophy still guides the vision of our growing company, which has helped millions of people find solutions for their financial needs.

What began with 2 people in a spare bedroom has now rapidly expanded to a vibrant business that employs over 2300 employees (known internally as The Freedom Family) in two locations: San Mateo, CA and Tempe, AZ. When you visit either of our offices, you’ll understand why our employees have voted us the Best Place to Work for the last several years. It’s a place where the Heart + $ philosophy continues to thrive, where we believe that success is only achieved by doing what’s right for our customers, our employees, and our communities.

In order to create brighter futures for our clients, employees, and businesses, Freedom Financial Network holds itself to four core values that have grown out of our Heart + $ philosophy: to care for everyone around us, act with integrity every time, collaborate with everybody we work with, and get better at what we do every day.

THE OPPORTUNITY:

As a Staff Engineer with Freedom Financial Network in its Tempe, AZ or San Mateo, CA office, you will contribute directly to the success of the business and have a meaningful impact on our customers’ lives.

We are looking for seasoned engineers to bring new and innovative ideas and tackle challenges across the full-stack. We believe in building teams that are passionate about solving complex problems and ready to grow and evolve alongside our expanding business and technology footprint.

THE ROLE:

Lead product requirements and advanced analytics requirements gathering efforts, interfacing with Data Engineering, Data Scientists, Analytics, and stakeholder teams •
Work with analytics, data science, and wider engineering teams to help with automating data analysis and visualization needs, advise on transformation processes to populate data models, and explore ways to design and develop data infrastructure
Use technology to solve business problems and drive positive outcomes
Build a scalable technology platform to support a growing business
Deliver high-quality code to production, consistently
Integration with various third-party systems
Play a key role in building out a large scale distributed and event based Data Platform that serves as an underpinning for all of Freedom’s businesses and products

REQUIREMENTS/CHARACTERISTICS:

10 years’ experience building large scale, real-time distributed systems with solid coding, problem-solving, and design skills
Experience working in highly collaborative teams
Ability to demonstrate a team-first attitude towards software development
Hands on Experience with Python, Java, SQL
Experience working with various data stores: SQL, NoSQL, and/or key-value store
Mature engineering practices (CI/CD, testing, secure coding, etc)
Experience with, or willingness to learn:
Event-driven architecture and messaging frameworks (Pub/Sub, Kafka, and/or RabbitMQ)
Cloud infrastructure (Google Cloud Platform, AWS, and/or Azure)

EDUCATION:

Bachelor’s in Computer Science or related field, or relevant experience in software development

CULTURAL FIT (Our Core Values):

Care (for everyone): We show compassion and contribute to the well-being and growth of those around us. We only pursue products that improve the financial lives of our clients.
Act with Integrity (every time): We take the right action even when it is hard and even when no one is watching. We treat our employees, clients, and communities the way they wish to be treated.
Get Better (every day): We innovate, iterate, and improve each day. We are creative, take thoughtful risks, and ultimately learn and recover from failures.
COLLABORATE (with everybody): We strive to work together toward a common purpose by proactively sharing information and inviting participation. We recognize the perspective of various groups and embrace a healthy, constructive debate.

WHY JOIN THE FREEDOM FAMILY?

Fast, continued growth – there’s a lot of opportunity for advancement
Voted the Best Place to Work multiple times by our employees, most recently #1 in Phoenix for the 2nd year in a row!
Benefits start within 30 days
401k with employer match
3 weeks’ paid vacation (increased with tenure)
9 paid holidays & 5 sick days
Paid time off for volunteer work and on your birthday",3.4,"Freedom Financial Network
3.4","San Mateo, CA",-1,1001 to 5000 Employees,2002,Company - Private,Investment Banking & Asset Management,Finance,$500 million to $1 billion (USD),-1
Data Engineer,$74k-$139k (Glassdoor Est.),"Tesla is seeking a talented and motivated Data Engineer to join our Data and Analytics team at Tesla. We are building a state of the art analytics platform for business and operation intelligence. At Tesla, we have enormous amounts of data and we want to give meaning to it and help business users to make data driven decisions. Our platform will allow users to answer ""what"", ""when"" and ""how"" questions as well as allow them to ask ""what if"". This person will design, develop, maintain and support our Enterprise Data Warehouse & BI platform within Tesla using various data & BI tools and this position offers unique opportunity to make significant impact to the entire organization in developing data tools and driving data driven culture. Responsibilities: Work in a time constrained environment to analyze, design, develop and deliver Enterprise Data Warehouse solutions Create ETL/ELT pipelines using Python, Airflow Design, develop, maintain and support our Enterprise Data Warehouse & BI platform within Tesla using various data & BI tools Create real time data streaming and processing using Open source technologies like Kafka , Spark etc Build ad-hoc applications as needed to support more curious data users and to provide automation as possible Work with systems that handle sensitive data with strict SOX controls and change management processes Develop collaborative relationships and work with key business sponsors, IT resources to gather requirements and for the efficient resolution of requests. Provide timely and accurate estimates for newly proposed functionality enhancements Communicate technical and business topics, as appropriate, in a 360 degree fashion, when required; communicate using written, verbal and/or presentation materials as necessary. Develop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughs. Utilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practices Take ownership of deployment and release process Keep up to date on relevant technologies and frameworks Requirements: 3+ years of experience in creating data pipelines using Python / Airflow is required Work experience with REST API’s and processing large number of files in Python / Java is required Experience with designing DataMart, Data Warehouse, database objects within relational databases MySQL, SQL Server, Vertica is required Strong proficiency in SQL and query writing is required Familiarity with common API’s: REST, SOAP Strong Problem-Solving, Verbal and Written communication skills Excellent analytical, organizational skills and ability to work under pressure /deliver on tight deadlines is a must Strong experience in stellar dashboards and reports creation for C-level executives Nice to have: Experience with data science tools such as Pandas, Numpy, R 3+ years of development experience in Open Source technologies like Python, Java is preferred Understanding of distributed computing, i.e. how HDFS, Spark and Presto work Proficient in Scala, Splunk Work experience with Python, SSIS, Informatica Experience in Big Data processing using Apache Hadoop/Spark ecosystem applications like Hadoop, Hive, Spark, Kafka and HDFS preferable Work experience with React JS, Node JS, Semantic UI / Bulma / Bootstrap CSS Work experience with Tableau Working with a system at scale and with Docker/Kubernetes/Jenkins CI/CD pipeline is preferred Experience implementing dynamic UIs and working with D3 / NVD3/React is preferred Experience with Kafka or RabbitMQ messaging queues is preferred Experience with React, rest API / web development with Node JS framework is preferred Tesla participates in the E-Verify Program",3.5,"Tesla
3.5","Fremont, CA",-1,10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$2 to $5 billion (USD),-1
Data Engineer,-1,"We're looking for people who are good at engineering systems to manipulate, process, and make sense of data. If you're expert in some areas of data engineering, but not others, we'll train you up. If you have the knowledge and skills to architect and design systems, we'll let you do that and follow your lead. Your team will own Bespoke data pipeline logic and processes. Performance and scaling for internal and external real-time data services. Conventional(-ish) ETL reporting and data engineering. Incorporating appropriate use of ML techniques in the pipeline You should have Experience working directly on one or more of the following: database schema design, query optimization, development of ETL / analytics / reporting systems, big-data systems, large-scale text-parsing systems, stream processing, or ML pipeline engineering. Experience with large scale distributed systems desirable Experience with Databricks, Apache Spark, PrestoDB, or Kafka desirable A positive attitude, willingness to learn, and desire for self-improvement We really like people who Own problems end-to-end - we have a very ownership driven culture Are product focused - solving the underlying product problem is more important than writing lots of code Have a good sense of humor Want to make a positive difference in people's lives At Human API, we are opening up the world of health data for the next generation of healthcare applications. We’ve built the first real-time network for health data, making health data accessible and actionable for companies building valuable applications and experiences with it. By enabling health data liquidity, we are powering a more effective, personal and responsive healthcare system. Human API is powering over 10,000 applications in 40 countries, including Johnson & Johnson, Kaiser Permanente, and AXA. The company is backed by Andreessen Horowitz and BlueRun Ventures. We're looking for independent thinkers who care deeply about the problems we're trying to solve. At Human API, we believe that a diverse variety of people makes us better, and so we welcome people of all backgrounds. Want to build the future of health data with us? Get in touch.",3.5,"Human API
3.5","San Francisco, CA",-1,1 to 50 Employees,2013,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Data Engineer,$108k-$127k (Glassdoor Est.),"When you come across a sea of data, is your first instinct to put on your software diving suit and go deep? We’re looking for highly-skilled Data Engineers to design and automate large scale data solutions that power our state-of-the-art artificial intelligence platform. If “changing the world” is on your to-do list, Entefy is your chance to make a career of it.

We’re redefining digital interaction, and our next Data Engineer will play a key role in the growing agile team that’s making it all happen.
Requirements
6+ years relevant experience developing and integrating frameworks and database technologies that support highly scalable data processing
Advanced knowledge of system architecture and database design
Proficiency in Big Data tools: Spark, Hadoop, Kafka, etc.
Advanced experience with SQL and NoSQL database architecture and implementation (hands-on experience with PostgreSQL, Elasticsearch, and Cassandra a plus)
Demonstrable experience designing, developing, and implementing ETL processes
Experience working with private cloud infrastructure
Demonstrable experience building and optimizing Big Data pipelines and architecture
Proficiency in Python, Java, C++
Demonstrable experience with Stream Processing and workload management for data transformation, augmentation, analysis, etc.
Ability to collaborate well with others
Strong communication skills
Visit www.entefy.com and www.blog.entefy.com",4.3,"Entefy
4.3","Palo Alto, CA",-1,1 to 50 Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Senior Data Engineer,$95k-$167k (Glassdoor Est.),"Luminar Technologies is seeking a Senior Data Engineer to contribute to the development efforts of our end-to-end data pipeline and corresponding web applications. Our vision is to make autonomous transportation safe and ubiquitous. Far too many lives are lost in vehicle accidents each year. Because when real people’s lives are at stake, driving safely 99% of the time isn't good enough. We just launched Hydra, and it’s capabilities are unmatched: road tracking out to 80m, lanes to 150m, and objects to 250m. Visit us @ https://www.luminartech.com/ to find out more.

*We are interviewing and on-boarding candidates completely virtually until the County of Santa Clara eases restrictions due to the ongoing COVID-19 pandemic. Our team is currently entirely working from home, except for those individuals carrying out ‘essential’ business activities.

Responsibilities
Contribute to the development of automotive-grade software for perception and self-driving applications based on Luminar’s industry leading LiDAR platform
Implementation of BigData infrastructure and services
Quality control of data infrastructure and data service development
Automation of data infrastructure
Implementation of data transformation and streaming services
Manage site reliability of data services
Minimum Qualifications
B.S. in Computer Science or a related field
4+ years of relevant industry experience
Already shipped and operated Big Data systems in production environments
Experience in scalability of data systems
Experience Cloud services, Cloud deployment and developer operations
Hands-on Big Data software and infrastructure development skills
Expert knowledge in technologies like Kafka, Kibana/Elastic, Spark, Airflow or Hadoop
Hands-on experience of data storage and schemes like Avro, Parquet
Very strong coding skills in Java or C++ and Python
Preferred Qualifications
Expert knowledge in rational, no-sql and distributed data stores
M.S. in Computer Science or a related field
Operational experience with on-prem and cloud data systems
Experience with large-scale ingestion architectures
Hands-on experience in data streaming technologies
Experience in cloud and on-premises data systems
Experience with Machine Learning and/or Computer Vision algorithms
Benefits & Perks
Location: HQ near Stanford University in the beautiful city of Palo Alto, California
Timing: A start-up backed by industry leaders, at a critical stage of growth
Compensation: Competitive salaries and meaningful equity
Benefits: Comprehensive package (medical, dental, vision, and more)
PTO: Take it when you need it, we are a results-oriented team (not just 9-5 job)
Luminar is an equal opportunity employer. All applicants will be considered for employment without regard to race, color, ancestry, national origin, sex, gender, sexual orientation, marital status, religion, age, disability, gender identity, results of genetic testing, service in the military, or any other characteristic protected by applicable federal, state or local laws. We will make a reasonable accommodation for any qualified applicant with a disability, provided that the individual is otherwise qualified to safely perform the essential functions of the job with or without accommodation and that the accommodation would not impose an undue hardship on the operation of our business. Please let us know if you believe you require reasonable accommodation, or if you would like assistance to complete an application or to participate in an interview at the company.",4.1,"Luminar
4.1","Palo Alto, CA",-1,201 to 500 Employees,2012,Company - Private,Computer Hardware & Software,Information Technology,Unknown / Non-Applicable,-1
Staff Data Engineer,$138k-$246k (Glassdoor Est.),"Why Data @ Airbnb? Quality data is fundamental to Airbnb's success. As a rapidly growing pre-IPO company, we are preparing for a future of tremendous growth and transformation. We are rebuilding our Data Engineering practice to enable the company's success by building a solid data foundation. We are seeking stunning Staff Data Engineers to help us define and realize our vision for trustworthy data across the company. This is a unique opportunity to shape Data Engineering for a strong, but high potential, company early in its lifecycle. Like all teams at Airbnb, we value and promote the diversity of our workforce, our guests, our hosts, our marketplace platform, and the world. Simply put, you belong at Airbnb. What is Data Engineering at Airbnb? We need to ensure every area of the business has trustworthy data to fuel insight and innovation. Understanding the business need, securing the right data sources, designing usable data models, and building robust & dependable data pipelines are essential skills to meet this goal. At the same time, the technology used to create great data is continually evolving. We are moving to a reality where both batch & stream processing are leveraged to meet the latency requirements for the business. The Data Engineering paved path is still taking shape, and we want to collaboratively develop this to support the entire company. We need senior engineers who are passionate not only about the data, but also about improving the technology we leverage for Data Engineering. We are looking for talented senior Data Engineers who are excited about redefining what it means to do Data Engineering. Data Engineering is part of our Engineering org as we believe great Data Engineering depends on solid Software Engineering fundamentals. However, we also recognize that each Data Engineer has a unique blend of skills. Whether your strength is in data modeling or in stream processing, we want to talk to you. Responsibilities: Develop and automate large scale, high-performance data processing systems (batch and/or streaming) to drive Airbnb business growth and improve the product experience. Evangelize high quality software engineering practices towards building data infrastructure and pipelines at scale. Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable Design our data models for optimal storage and retrieval and to meet critical product and business requirements. Understand and influence logging to support our data flow, architecting logging best practices where needed Contribute to shared Data Engineering tooling & standards to improve the productivity and quality of output for Data Engineers across the company Minimum Requirements: 8+ years of relevant industry experience. Bachelor's and/or Master's degree in CS/EE, or equivalent experience. Working with data at the petabyte scale. Experience designing, building and operating robust distributed systems. Experience designing and deploying high performance systems with reliable monitoring and logging practices. Effectively work across team boundaries to establish overarching data architecture, and provide guidance to individual teams. Working knowledge of relational databases and query authoring (SQL). Experience with Java / Scala / Spark is preferred Excellent communication skills, both written and verbal. Benefits: Stock Competitive salaries Quarterly employee travel coupon Paid time off Medical, dental, & vision insurance Life insurance and disability benefits Fitness Discounts 401K Flexible Spending Accounts Apple equipment Commuter Subsidies Community Involvement (4 hours per month to give back to the community) Company sponsored tech talks and happy hours Much more… Offices: Airbnb has Engineering offices along the West Coast in the U.S. and our teams are growing quickly! Contact us to find out which office works best for you. Candidate Privacy Notice If you are a California resident, learn about what personal information we collect about you and how we use it here.",4.1,"Airbnb
4.1","San Francisco, CA",-1,5001 to 10000 Employees,2008,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Staff Data Engineer,$142k-$246k (Glassdoor Est.),"Medium’s mission is to help people deepen their understanding of the world and discover ideas that matter. We are building a place where ideas are judged on the value they provide to readers, not the fleeting attention they can attract for advertisers. We are creating the best place for reading and writing on the internet—a place where today’s smartest writers, thinkers, experts, and storytellers can share big, interesting ideas. We are looking for a Staff Data Engineer that will help build, maintain, and scale our business critical Data Platform. In this role, you will help define a long-term vision for the Data Platform architecture and implement new technologies to help us scale our platform over time. You'll also lead development of both transactional and data warehouse designs, mentoring our team of cross functional engineers and Data Scientists. At Medium, we are proud of our product, our team, and our culture. Medium’s website and mobile apps are accessed by millions of users every day. Our mission is to move thinking forward by providing a place where individuals, along with publishers, can share stories and their perspectives. Behind this beautifully-crafted platform is our engineering team who works seamlessly together. From frontend to API, from data collection to product science, Medium engineers work multi-functionally with open communication and feedback. What Will You Do Work on high impact projects that improve data availability and quality, and provide reliable access to data for the rest of the business. Drive the evolution of Medium's data platform to support near real-time data processing and new event sources, and to scale with our fast-growing business. Help define the team strategy and technical direction, advocate for best practices, investigate new technologies, and mentor other engineers. Design, architect, and support new and existing ETL pipelines, and recommend improvements and modifications. Be responsible for ingesting data into our data warehouse and providing frameworks and services for operating on that data including the use of Spark. Analyze, debug and maintain critical data pipelines. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Spark and AWS technologies. About You You have 7+ years of software engineering experience. You have 3+ years of experience writing and optimizing complex SQL and ETL processes, preferably in connection with Hadoop or Spark. You have outstanding coding and design skills, particularly in Java/Scala and Python. You have helped define the architecture, tooling, and strategy for a large-scale data processing system. You have hands-on experience with AWS and services like EC2, SQS, SNS, RDS, Cache etc or equivalent technologies. You have a BS in Computer Science / Software Engineering or equivalent experience. You have knowledge of Apache Spark, Spark streaming, Kafka, Scala, Python, and similar technology stacks. You have a strong understanding & usage of algorithms and data structures. Nice To Have Snowflake knowledge and experience Looker knowledge and experience Dimensional modeling skills At Medium, we foster an inclusive, supportive, fun yet challenging team environment. We value having a team that is made up of a diverse set of backgrounds and respect the healthy expression of diverse opinions. We embrace experimentation and the examination of all kinds of ideas through reasoning and testing. Come join us as we continue to change the world of digital media. Medium is an equal opportunity employer. Interested? We'd love to hear from you.",3.6,"Medium
3.6","San Francisco, CA",-1,201 to 500 Employees,2011,Company - Private,Publishing,Media,Unknown / Non-Applicable,-1
Big Data Engineer (DW/ETL),$87k-$157k (Glassdoor Est.),"Overview

Come join the Data Engineering Team as a Software Engineer II.We are leveraging big data technologies to gain new insights into our customer experiences; building data frameworks, ingestion pipelines and tools. Some of the technologies we are leveraging include: Hadoop, Vertica, Hive and AWS Big Data Systems.

Participate in the entire product lifecycle for software products and services that are broad in scope and complexity, applying a full understanding of software engineering methodologies and industry best practices. Work with Industry Experts; Senior, Staff and Principle Engineers, utilize specialized knowledge to develop, and maintain Intuit’s software. Although our team supports hosted services, we also publish cross platform libraries for Linux/Windows/Mac – so you are a good fit if you have competent working knowledge in both java and C++.

What you'll bring
BS or MS in Computer Science or related field or equivalent work experience
3+ years of core development experience
Skilled in developing Software for Java (Spring & Springboot), Scala for spark streaming & spark applications, or other JVM based languages.
Working Knowledge of XML, JavaScript, JSON, YML and Linux
Advanced experience with scripting language – Python or Shell is a must have
Strong knowledge of software development methodologies and practices
Experience working in Agile development teams; working knowledge of Agile (Scrum) development methodologies
Experience with Amazon web services: EC2, S3, and EMR (Elastic Map Reduce) or equivalent cloud computing approaches
Strong expertise in Data Warehousing and analytic architecture
Experience working with large data volumes
Experience in HTML5, CSS, and other Web technologies a plus.
Experience with low-latency NoSQL datastores (such as DynamoDB, HBase, Cassandra, MongoDB) is a plus
Experience with building stream-processing applications using Spark Streaming, Kinesis, etc. is a plus

Additional

Exposure to unit testing frameworks
Hands on experience with Hadoop stack of technologies – Hive, Pig, pig-udf
Ability to research and integrate 3rd party solutions
Evolving a mature code base into new technologies
Experience creating and consuming SOAP based or JSON/REST web services and communicating with systems.
Experience developing web services
How you will lead
70-85% hands-on development in all phases of the software life cycle.
Rapidly fix bugs and solve problems
Conduct design and code reviews
Defect remediation
Technical design specification
Automated unit tests
Estimates and sequence of individual activities as inputs to project plans
Analyzes and synthesizes a variety of inputs to create software and services.
Identify dependencies as inputs to project plans
Collaborates effectively with peer engineers and architects to solve complex problems spanning their respective areas to deliver end-to-end quality in our technology and customer experience.
Influences and communicates effectively with non-technical audiences including senior product and business management.
Designing/developing ETL jobs across multiple big data platforms and tools including S3, EMR, Hive, Vertica",4.3,"Intuit
4.3","Mountain View, CA",-1,5001 to 10000 Employees,1983,Company - Public,Computer Hardware & Software,Information Technology,$5 to $10 billion (USD),-1
"Data Engineer, Product Analytics",$110k-$179k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Do you like working with big data? Do you want to use data to influence product decisions for products being used by over half a billion people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. In this role, you will work with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.This is a full-time position based in our office in Menlo Park.
Manage data warehouse plans for a product or a group of products.
Interface with engineers, product managers and product analysts to understand data needs.
Build data expertise and own data quality for allocated areas of ownership.
Design, build and launch new data models in production.
Design, build and launch new data extraction, transformation and loading processes in production.
Support existing processes running in production.
Define and manage SLA for all data sets in allocated areas of ownership.
Work with data infrastructure to triage infra issues and drive to resolution.
2+ years experience in the data warehouse space.
2+ years experience in custom ETL design, implementation and maintenance.
2+ years experience working with either a MapReduce or an MPP system.
2+ years experience with object-oriented programming languages.
2+ years experience with schema design and dimensional data modeling.
2+ years experience in writing SQL statements.
Experience analyzing data to identify deliverables, gaps and inconsistencies.
Experience managing and communicating data warehouse plans to internal clients.
BS/BA in Technical Field, Computer Science or Mathematics.
Knowledge in Python or Java.
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Sunnyvale, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Big Data Engineer (DW/ETL),$87k-$157k (Glassdoor Est.),"Overview


Come join the Data Engineering Team as a Software Engineer II.We are leveraging big data technologies to gain new insights into our customer experiences; building data frameworks, ingestion pipelines and tools. Some of the technologies we are leveraging include: Hadoop, Vertica, Hive and AWS Big Data Systems.

Participate in the entire product lifecycle for software products and services that are broad in scope and complexity, applying a full understanding of software engineering methodologies and industry best practices. Work with Industry Experts; Senior, Staff and Principle Engineers, utilize specialized knowledge to develop, and maintain Intuits software. Although our team supports hosted services, we also publish cross platform libraries for Linux/Windows/Mac so you are a good fit if you have competent working knowledge in both java and C++.
What you'll bring
BS or MS in Computer Science or related field or equivalent work experience
3+ years of core development experience
Skilled in developing Software for Java (Spring & Springboot), Scala for spark streaming & spark applications, or other JVM based languages.
Working Knowledge of XML, JavaScript, JSON, YML and Linux
Advanced experience with scripting language Python or Shell is a must have
Strong knowledge of software development methodologies and practices
Experience working in Agile development teams; working knowledge of Agile (Scrum) development methodologies
Experience with Amazon web services: EC2, S3, and EMR (Elastic Map Reduce) or equivalent cloud computing approaches
Strong expertise in Data Warehousing and analytic architecture
Experience working with large data volumes
Experience in HTML5, CSS, and other Web technologies a plus.
Experience with low-latency NoSQL datastores (such as DynamoDB, HBase, Cassandra, MongoDB) is a plus
Experience with building stream-processing applications using Spark Streaming, Kinesis, etc. is a plus
Additional
Exposure to unit testing frameworks
Hands on experience with Hadoop stack of technologies Hive, Pig, pig-udf
Ability to research and integrate 3rd party solutions
Evolving a mature code base into new technologies
Experience creating and consuming SOAP based or JSON/REST web services and communicating with systems.
Experience developing web services

How you will lead
70-85% hands-on development in all phases of the software life cycle.
Rapidly fix bugs and solve problems
Conduct design and code reviews
Defect remediation
Technical design specification
Automated unit tests
Estimates and sequence of individual activities as inputs to project plans
Analyzes and synthesizes a variety of inputs to create software and services.
Identify dependencies as inputs to project plans
Collaborates effectively with peer engineers and architects to solve complex problems spanning their respective areas to deliver end-to-end quality in our technology and customer experience.
Influences and communicates effectively with non-technical audiences including senior product and business management.
Designing/developing ETL jobs across multiple big data platforms and tools including S3, EMR, Hive, Vertica",4.3,"Intuit - Data
4.3","Mountain View, CA",-1,5001 to 10000 Employees,1983,Company - Public,Computer Hardware & Software,Information Technology,$5 to $10 billion (USD),-1
Associate Data Engineer,$67k-$128k (Glassdoor Est.),"The Data Engineering team at Glu builds core data infrastructure and applications in support of all areas of our business, including our studio teams, user acquisition, monetization and finance. Glu is passionate about maximizing the value that data and analytics can provide to the business and is aggressively investing in new capabilities. Our team covers a lot of ground from data ingestion through to machine learning applications. We leverage a cutting-edge tech stack to build both batch systems (YARN+Spark/Hive) and stream processing applications (Kinesis/Flink/Spark Streaming/Druid) that operate efficiently at high scale. The ideal candidate has a strong engineering background and has built robust data platforms and pipelines and takes complete ownership of their area of expertise. This is a fantastic opportunity to use your engineering skills to make a material impact on a highly valued analytics platform You'll most often be: Taking ownership of and developing critical new features for our next-generation analytics platform, supporting Glu's worldwide studios and central functions such as marketing and finance. Building scalable, accurate and extensible stream processing applications using cutting-edge technology such as Spark Streaming and Apache Flink. Implementing complex and highly scalable end-to-end data pipelines, using Elastic Beanstalk, Kinesis, EMR, Spark, Hive, Druid, Cassandra. Building data support for our personalization and experimentation efforts, solving problems from statistical test automation to building real-time M/L applications. And your skills and experience include: Bachelor's degree in computer science/mathematics/engineering, or other fields with proven engineering experience. Software engineering experience, especially working on back-end data infrastructure. Proficiency with at least one of the following languages: Java, Python, Scala. Experience with distributed stream processing technologies such as Flink, Spark Streaming and/or Kafka Streams. Experience with AWS Ecosystem, especially Kinesis, EMR, Lambda, and Glue. Knowledge of NoSQL application data stores i.e. Druid, HBase, Cassandra, DynamoDB, Redis. . Bonus Points Experience with high-scale machine learning, I.e. Spark M/L, SageMaker, etc. Experience with SQL and SQL-like languages, especially Hive. Experience with CI/CD process, testing framework, and containerization technology Experience building data-rich web applications, especially with technologies like Angular, Node.js, and Elastic Beanstalk",3.5,"Glu Mobile
3.5","San Francisco, CA",-1,501 to 1000 Employees,2001,Company - Public,Computer Hardware & Software,Information Technology,$100 to $500 million (USD),-1
Sr. Data Engineer,$136k-$232k (Glassdoor Est.),"As a Senior Data Engineer, you will be working cross-functionally with business domain specialists, analytics, and engineering teams to design and implement our Data Warehouse model. You will be working in our Business Technology Data Engineering and Architecture function and have an opportunity to build a sound data foundation and processes that will scale with the company’s growth. You will architect, design, implement data pipelines enabling insights from our Product and Corporate Systems for key partners, data scientists and decision makers in organizations such as Sales, Marketing and Finance at Slack. You would also be responsible for improving and maintaining our data warehouse. Data engineers also need to guarantee compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines. This would enable faster data access, integrated data reuse and vastly improved time-to-solution for Slack’s data and analytics initiatives. The newly hired data engineer will be the key interface in operationalizing data and analytics on behalf of the organizational outcomes of business teams! This role will require both creative and collaborative working with Business technology, Product and the wider business teams. This is a fun role for problem solvers, who can intuitively anticipate problems and can also look beyond immediate issues. In short, we look for people who take pride in the craft and want to be part of creating and defining the teams operating model and contribution to the company. They will be a self-starter, detail and quality oriented, and passionate about having a huge impact at Slack. If this role has your name written all over it, please contact us with a resume so that we explore further. What you will be doing: Design, implement and build pipelines that deliver data with measurable quality under the SLA Partner with Data Engineers, Data architects, domain specialists, data analysts and other teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service Be a champion of the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements Own and document data pipelines and data lineage Identify, document and promote best practices Support and Maintain analytics tech ecosystem (data warehouse, ETL and BI tools) What you should have: BS or MS degree in Computer Science or Engineering field. At least 8 years or more of work experience in data management disciplines including data integration, modeling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks. At least 4 years of experience working in multi-functional teams and collaborating with business partners in Sales or Marketing in support of a departmental and/or multi-departmental data management and analytics initiative that involve working with data generated by product and business systems. Very strong experience in working with product usage data. Solid experience in dimensional modeling, supporting data warehouse, scaling and optimizing, performance tuning and ETL pipelines Deep understanding of relational as well as big data setup Problem solver with good interpersonal skills with ability to make sound sophisticated decisions in a fast-paced, technical environment. Ability to work on multiple areas like Data pipeline ETL, Data modeling & design, writing complex SQL queries etc. . Preferred: Prior experience with ETL tools (eg Informatica, Matillion, Snaplogic), Hive, Presto, Dimensional Modeling. Hands-on experience with Data Warehouse technologies (Snowflake, Redshift) and Big Data technologies (e.g Hadoop, Hive, Spark) Proficiency with programming languages is a big plus (e.g. Python) Capable of planning and executing on both short-term and long-term goals individually and with the team. Passionate about various data technologies including but not limited to SQL/No SQL/MPP databases etc. Excellent written and verbal communication and interpersonal skills, able to effectively collaborate with technical and business partners Excellent understanding of trade-offs Demonstrated ability to navigate between big-picture and implementation details Slack is registered as an employer in many, but not all, states. If you are not located in or able to work from a state where Slack is registered, you will not be eligible for employment.Visa sponsorship may not be available in certain remote locations. Visa sponsorship is not available for candidates living outside the country of this position. Slack is an Equal Opportunity Employer and participant in the U.S. Federal E-Verify program. Women, minorities, individuals with disabilities and protected veterans are encouraged to apply. Slack will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance. Slack has transformed business communication. It’s the leading channel-based messaging platform, used by millions to align their teams, unify their systems, and drive their businesses forward. Only Slack offers a secure, enterprise-grade environment that can scale with the largest companies in the world. It is a new layer of the business technology stack where people can work together more effectively, connect all their other software tools and services, and find the information they need to do their best work. Slack is where work happens. Ensuring a diverse and inclusive workplace where we learn from each other is core to Slack’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer and a pleasant and supportive place to work. Come do the best work of your life here at Slack.",4.7,"Slack
4.7","San Francisco, CA",-1,1001 to 5000 Employees,2014,Company - Public,Internet,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,-1,"About Backblaze Backblaze originated in a founder’s one-bedroom apartment where five people committed to helping people save their data. Backblaze provides backup and cloud storage that’s astonishingly easy to use and inexpensive. Our customers use our services so they can pursue dreams like curing cancer (genome mapping is data intensive), archiving the work of some of the greatest artists on the planet (learn more about how Austin City Limits uses Backblaze B2 Cloud Storage), or simply sleeping well at night (anyone that’s spilled a cup of coffee on a laptop knows the relief that comes with complete, secure backups). We are entrusted with almost an exabyte of data from customers in more than 150 countries. We exited 2019 growing quickly and cash flow positive, and we’ve done all this with just $3M of funding. We’ve managed to nurture a team-oriented culture with amazingly low turnover. Our approach is guided by honesty, transparency, and a commitment to doing the right thing for our customers and coworkers. Our customers are happy, and so are our coworkers: In the most recent “Great Place to Work” survey, 99% of our team rated Backblaze as “a great place to work.” Check out what our employees are saying on Glassdoor! But while there is a lot to celebrate in our past, there is almost as much opportunity ahead of us. We are seeking a Data Engineer! This position is located in San Mateo, California but will also consider remote work as long as you're no more than three time zones away and can come to San Mateo now and then. What You’ll Do: Build scalable, efficient and high-performance pipelines/ workflows that are capable of processing large amounts of batch and real-time data Build out our data service architecture to support internal and customer facing application use cases Multidisciplinary work supporting real-time streams, ETL pipelines, data warehouses and reporting services Bring new and innovative solutions to the table to resolve challenging performance and load issues Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability Build out the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Java, Python, and SQL Respond quickly to bug fixes and enhancement requests and be able to take directions and complete tasks on-time with minimal supervision. Collaborate with business analytics team to optimize complex queries needed by regular Tableau reports The Right Fit: 5+ years of Java or Python 3+ years in designing relational database, data modeling and ETL Strong experience with SQL databases Experience in operational data stores and real time data integration Experience with RESTful APIs and server-side APIs integration Proficient with development on Linux and Macintosh platforms Bonus points for: Maria DB NoSQL DB Cassandra experience Salesforce Looking for an attitude of: Passionate about building friendly, easy to use Interfaces and APIs. Likes to work closely with other engineers, support, and sales to help customers. Believes the whole world needs backup, not just English speakers in the USA. Customer Focused (!!) — always focus on the customer’s point of view and how to solve their problem! Required for all Backblaze Employees: Good attitude and willingness to do whatever it takes to get the job done Strong desire to work for a small, fast-paced company Desire to learn and adapt to rapidly changing technologies and work environment Rigorous adherence to best practices Relentless attention to detail Excellent interpersonal skills and good oral/written communication Excellent troubleshooting and problem-solving skills Backblaze Perks: 100% healthcare for family Competitive compensation and 401k Full-time employees receive option grants Flexible vacation policy MacBook Pro to use for work plus a generous stipend to personalize your workstation Fully stocked micro kitchens and strong coffee Catered breakfast and lunches twice a week Childcare bonus (human children only) Pet-friendly office Generous skills training policy to continue your professional development Culture that supports healthy work-life balance Backblaze is an Equal Opportunity Employer.",4.9,"Backblaze
4.9","San Mateo, CA",-1,51 to 200 Employees,2007,Company - Private,Computer Hardware & Software,Information Technology,$25 to $50 million (USD),-1
Data Engineer,-1,"We're looking for a brilliant and versatile entry-level Data Engineer to help us build and scale HealthCrowd. Though we prefer candidates with internship or graduate research experience, those who without are welcome to apply as well. YOU, our Data Engineer is a key member of our Engineering and Customer Success teams. Just like your Software Engineer counterpart at HealthCrowd, you have software engineering and technical skills; however, you are also good at: getting our customers off the ground and running, measuring their successes/shortfalls, turning data into insights and helping them meet their ROI objectives. If you'd like a technical position with some customer-facing responsibilities and a skew on data analytics, this could be just the right opportunity. YOU get: To truly participate in building a company and its products The opportunity to learn from and be mentored by senior team members An environment where your success is our success — we'll invest in your growth Salary + stock options + 401(k) Are you the right person to help us capture the opportunity ahead? YOU: Have a graduate degree in STEM from a top school, with demonstrable academic brilliance Have 1-2 yrs of professional/academic experience in software engineering or data analysis Have familiarity with (some of) the following: non-relational database, data analysis and visualization, statistical model, ML/NLP framework, OOP Are willing, able and excited to tackle problems outside your comfort zone Are located in the San Francisco Bay Area (or will find a way to get here!) We don't want to waste your time or ours. If you think you could be a fit, please: Respond with your resume to jobs@healthcrowd.com with “Data Engineer” in the subject line, send any supporting material that you think will help us get to know you better, and answer the questions below (there are no correct answers) What is the single most important quality/attribute you possess that will ensure your success at this position and why? What single project have you done (whether school or professional project) that demonstrates your critical thinking and problem solving skills?",2.8,"HealthCrowd
2.8","San Mateo, CA",-1,1 to 50 Employees,-1,Company - Private,Health Care Services & Hospitals,Health Care,Less than $1 million (USD),-1
Data Engineer,$106k-$194k (Glassdoor Est.),"ABOUT THE TEAM The data engineering team is a small, nimble group of data engineers that drive the company toward clean and informative data. As a member of the data engineering team that focuses on scalable data engineering, you'll contribute toward better optimization and scalability that will help power data science and the business. You'll build tools to make us efficient, as well as lead us in optimizing our self-service data infrastructure to facilitate scalable decision-making. As a team, we are driven by the thrill of helping our colleagues use data with less friction, which ultimately increases the velocity at which the business can progress! ABOUT THE ROLE Senior IC position on the data engineering team, within our Algorithms organization, focusing on our data infrastructure, optimization and scalability You will build and own large additions to our data engineering framework, charged with finding ways to improve scalability, reliability and performance of our most central data pipelines You will lead us in identifying opportunities for more optimized data storage in S3 and better usage of our spark resources across the data engineering organization You will build scalable data engineering solutions & frameworks to solve business and data problems You will be involved in the day-to-day operations of the team, including maintaining and improving our current tools & scripts and supporting full-stack data scientists You will have autonomy to help shape the future of data engineering at Stitch Fix by bringing your ideas on improving and automating what we do and how we do it YOU'RE EXCITED ABOUT THIS OPPORTUNITY BECAUSE YOU WILL... Work with different teams of data scientists and engineers on how to solve data and business problems in a scalable way Be part of a team which has high visibility across the organization Contribute ideas and direct the team's investment to impactful directions Contribute to a culture of technical collaboration and scalable development WE GET EXCITED ABOUT CANDIDATES WHO HAVE… Experience in building out scalable data engineering capabilities 5+ years of fully independent project experience with significant contributions. Exceptional coding and design skills in Python and SQL Excellent experience in Spark optimization and an understanding of data storage with Amazon S3 Experience in working autonomously and taking ownership of projects. Ability to think globally, devising and building solutions to meet many needs rather than completing individual projects or tasks Strong prioritization skills with business impact in mind Strong cross functional communication skills that help simplify and move complex problems forward YOU'LL LOVE WORKING AT STITCH FIX BECAUSE… We are a group of bright, kind and goal oriented people. You can be your authentic self here, and are empowered to encourage others to do the same! We are a successful, fast-growing company at the forefront of tech and fashion, redefining retail for the next generation We are a technologically and data-driven business We are committed to our clients and connected through our vision of ""Transforming the way people find what they love"" We love solving problems, thinking creatively and trying new things We believe in autonomy & taking initiative We are challenged, developed and have meaningful impact We take what we do seriously. We don't take ourselves seriously We have a smart, experienced leadership team that wants to do it right & is open to new ideas We offer competitive compensation packages and comprehensive health benefits You will be proud to say that you work for Stitch Fix and will know that the work you do brings joy to our clients every day ABOUT STITCH FIX At Stitch Fix, we're about personal styling for everybody and we believe in both a service and a workplace where you can be your best, most authentic self. We're the first fashion retailer to combine technology and data science with the human instinct of a Stylist to deliver a deeply personalized shopping experience. This novel juxtaposition attracts a highly diverse group of talented people who are both thinkers and doers. All of this results in a simple, powerful offering to our customers and a successful, growing business serving millions of men, women, and kids. We believe we are only scratching the surface on our opportunity, and we're looking for incredible people like you to help us carry on that trend. Please review Stitch Fix's Recruiting Privacy Policy here: https://www.stitchfix.com/privacy/usrecruitingprivacy",3.5,"Stitch Fix
3.5","San Francisco, CA",-1,5001 to 10000 Employees,2011,Company - Public,Other Retail Stores,Retail,$500 million to $1 billion (USD),-1
Data Engineer,-1,"We are looking for a Data Engineer who is excited about building products that wrangle AV data to supercharge our customers. You will drive the design and development of data infrastructure across our products and internal tools. At Applied, we encourage all engineers to take ownership over technical and product decisions, closely interact with users to collect feedback, and contribute to a thoughtful, dynamic team culture. At Applied, you will: Design powerful data pipelines that process fast sensor streams, leverage appropriate data stores, and offer easy-to-use APIs Develop and deploy high-quality software using modern tooling and frameworks Work with products and teams across Applied Intuition Work with customers across the AV ecosystem to understand their needs and the innards of their data systems We’re looking for someone who: Has 1.5+ years experience building scalable big data pipelines Has experience with open source data processing frameworks (Spark, Kafka, etc.) Has experience with different data storages (e.g., relational and NoSQL) Has experience with containerization and other modern software development workflows Takes initiative and ownership in a fast-paced environment Nice to have: Expertise with multiple modern programming languages (Python, C++, Go, etc.) Prior work in enterprise software, including on-prem and/or cloud deployments Prior work in either autonomy or simulation products Autonomy is one of the leading technological advances of this century that will come to impact our lives. The work you’ll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting edge technology while working with major players across the industry and the globe.",4.7,"Applied Intuition
4.7","Mountain View, CA",-1,1 to 50 Employees,-1,Company - Public,-1,-1,Less than $1 million (USD),-1
Data Engineer,-1,"5+ years of experience in Data Engineering
Collaborate and implement the overall enterprise data strategy.
Deliver data services and projects from India, ensuring that these solutions align with the organization’s strategic direction and technology standards.
Hands on in design, develop and maintain conceptual, logical and physical data models for enterprise data warehouse
Technical expertise on Big Data technologies like MQTT, Kafka, Spark, Sqoop, Spark and Kafka.
Deploy solution using AWS, S3, Redshift and Docker/Kubernetes
Deploy solution using AWS, S3, Redshift and Docker/Kubernetes
Own SLA’s for data availability in enterprise data warehouse by ensuring all data jobs run successfully • Forecast and plan resource requirements. Allocate work across team to ensure timely delivery of projects / enhancements
Serve as a subject matter expertise in the areas of data warehou
Job Types: Full-time, Contract

Pay: $115,603.00 - $191,059.00 per year

Schedule:
8 hour shift
Monday to Friday
Experience:
Kafka: 3 years (Preferred)
automotive domain: 2 years (Preferred)",3.5,"SPANIDEA SYSTEMS
3.5","San Francisco, CA",-1,501 to 1000 Employees,2011,Company - Private,Computer Hardware & Software,Information Technology,$10 to $25 million (USD),-1
"Data Engineer, Data Platform",-1,"Responsibilities
TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo.

As a data engineer in the data platform team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users.

Responsibilities - What You'll Do
• Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis);
• Design and implement reliable, scalable, robust and extensible big data systems that support core products and business;
• Establish solid design and best engineering practice for engineers as well as non-technical people.
Qualifications
• BS or MS degree in Computer Science or related technical field or equivalent practical experience;
• Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.);
• Experience with performing data analysis, data ingestion and data integration;
• Experience with ETL(Extraction, Transformation & Loading) and architecting data systems;
• Experience with schema design, data modeling and SQL queries;
• Passionate and self-motivated about technologies in the Big Data area.

TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too.

TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or an accommodation, please reach out to us at usrc@tiktok.com.",4.1,"TikTok
4.1","Mountain View, CA",-1,1001 to 5000 Employees,2016,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"Role Senior Data Engineer Who We Are Our mission: making memorable outdoor experiences accessible to everyone. Outdoorsy is the most trusted RV rental and outdoor experiences marketplace on the planet. We have grown from a lofty white-board idea in 2015, to over $525M in GMV and offices worldwide in the US, Canada, Australia, Europe and the UK. We’ve mobilized the 56+ million idle RVs and camper vans around the world to ensure everyone has the access, choice, and opportunity to safely spend more time outside. Over 856,000 vacation nights were booked on Outdoorsy in 2019. 93% of reviewed Outdoorsy bookings receive 5-star ratings. With over 600,000 families and adventure seekers—and 50,000 RVs, travel trailers, and campervans to choose from—that’s a lot of sunset selfies taken! Our platform handles availability scheduling, payment processing, communication, insurance, GPS tracking and roadside assistance all built in. Our rapidly growing user base has taken us from the United States to Canada, Australia and New Zealand. Our engineering team is a small mighty group of self-starting developers who are motivated by making an impact on the lives of our users, both financially for our owners or experientially for our renters. Who You Are A hands-on self directed data engineer that has built and refactored code bases to follow best practices. You’re a stickler for proper architecture and you’re not afraid to voice your opinion if something isn’t designed properly - as Data Engineering is the eyes through which they see the product. Understanding the business need, securing the right data sources, designing usable data models, and building robust & dependable data pipelines are essential skills. You are constantly learning about new cloud computing frameworks and technologies. Scalability, stability and performance are always top of mind when building new functionality or reviewing requests from the team. You will belong to a centralized Data Science/Data Engineering team, and contribute to a variety of projects and technologies. Projects include product relevance, ML modeling, services, and more. What You’ll Be Doing Design, build and automate large scale, high-performance data processing systems (batch and/or streaming) to enable data science solutions, drive business growth and improve the product experience Build extremely efficient and reliable data pipelines to move and transform data (both large and small amounts) Design, and develop our data models for optimal storage and retrieval to meet critical product and business requirements Build data domain expertise and own data quality for your areas Partner with data scientists, engineers and product managers to define problem statements, collect data, and make recommendations Use your data experience to educate your partners, identify and address gaps and inconsistencies in the existing logging and processes, and advance effective product solutions Required Experience Experience working in startup fast paced environments 4+ years of professional experience in custom ETL/data pipeline design, implementation and maintenance Experience with AWS or GCP cloud technologies and services (experience with analytics services and data lake concepts are a plus) Experience with large data sets, big data technologies (e.g. Spark) Experience with streaming data processing methods and frameworks (e.g. Kafka or Kinesis) 4+ years of professional experience working with SQL and noSQL databases Experience in at least one programming language (e.g. Python, Spark, Scala or Java) Experience designing and deploying high performance systems with reliable monitoring and logging practices Experience with notebook-based Data Science workflow Experience with Git hygiene and comfortable with Github, including feature branches, pull requests, and code review Creative problem solver who enjoys collaboration Excellent communication skills, both written and verbal Minimum BSc or equivalent in Computer Science, Math or other technical fields",4.1,"Outdoorsy, Inc.
4.1","San Francisco, CA",-1,51 to 200 Employees,2015,Company - Private,Internet,Information Technology,$100 to $500 million (USD),-1
Data Engineer,-1,"Our Customer is a corporation that develops, manufactures, and markets robotic products designed to improve clinical outcomes of patients through minimally invasive surgery. Founded in 1995, their goal was to create innovative, robotic-assisted systems that help empower doctors and hospitals to make surgery less invasive than an open approach. Working with the top medical professionals, they continue to develop new, minimally invasive surgical platforms and future diagnostic tools to help solve complex healthcare challenges around the world.

Overview:

We are seeking a Data Engineer who can help support our client’s day to day business needs.

What you’ll do:

• Designs, builds and oversees the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources.

• Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required.

• Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis

• Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements.

• Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs

Must Haves:

• 4 years of hands-on experience working on data using SQL on multiple platforms (SQL server, my SQL, Mongo) (required)

• 5 years of hands-on experience with Python and C# programming (required)

• 4 years of hands-on experience with Data analytics (dashboards) and derive insights

• 3-4 years of hands-on experience with Application Development Skills - web based or service based (required)

• 3 years of hands-on experience with ETL tools and automation (required)

• 1 year of experience AWS technology stack understanding (preferred)

Education:

• BS degree: 8-9 years of hands-on experience

• MS degree- 4-5 years of hands-on experience

Hours & Location:

M-F, 40 hours/week. This role will be remote while COVID restrictions are in place. The expectation is to be onsite at our Customer’s Sunnyvale, CA location once it is deemed safe to do so.

Now for the Perks!

Health Benefits: Medical, Dental, Vision, Life (including spouse & child), 401k, STD/LTD, AD&D, and Commuter Benefits program.",4.7,"The Mom Project
4.7","Sunnyvale, CA",-1,51 to 200 Employees,2016,Company - Private,Staffing & Outsourcing,Business Services,Unknown / Non-Applicable,-1
Cloud Data Engineer,$123k-$211k (Glassdoor Est.),"Company Description As data engineers in Revenue Science, our mission is to build real-time and offline solutions to make data accessible and reliable while leveraging the largest-scale data processing technologies in the world - and then apply them to the Revenue’s most critical and fundamental data problems. Learn more about some of the challenges we tackle on this team: Building a Petabyte-scale Data Warehouse (Google Cloud Next '18) https://youtu.be/APBF9Z3uBCc How Twitter Migrated its On-Prem Analytics to Google Cloud (Google Cloud Next '18) https://youtu.be/sitnQxyejUg Job Description You are passionate about data and driven to take the data organization challenges at the scope of entire Twitter’s Revenue. As a member of the Data Engineering team, you will build and own mission-critical data pipelines that are ‘source of truth’ for Twitter’s fundamental revenue data, as well as modern data warehouse solutions, while collaborating closely with Ads Data Science team. You will be a part of an early stage team and have a significant stake in defining its future with a considerable potential to impact all of Twitter’s revenue and hundreds of millions of users. You will be among the earliest adopters of bleeding-edge data technologies, working directly with Revenue Science and Revenue Platforms teams to integrate your services at scale. Your efforts will reveal invaluable business and user insights, leveraging vast amounts of Twitter revenue data to fuel numerous Revenue teams including Ads Analytics, Ads Experience, Ads Data Science, Marketplace, Targeting, Prediction, and many others. Qualifications Strong programming and algorithmic skills Experience with data processing (such as Hadoop, Spark, Pig, Hive, MapReduce etc). Proficiency with SQL (Relational, Redshift, Hive, Presto, Vertica) Nice to have: Experience writing Big Data pipelines, as well as custom or structured ETL, implementation and maintenance Experience with large-scale data warehousing architecture and data modeling Proficiency with Java, Scala, or Python Experience with GCP (BigQuery, BigTable, DataFlow) Experience with Druid or Apache Flink Experience with real-time streaming (Apache Kafka, Apache Beam, Heron, Spark Streaming) Ability in managing and communicating data warehouse project plans to internal clients Additional Information All your information will be kept confidential according to EEO guidelines.",4.1,"Twitter
4.1","San Francisco, CA",-1,1001 to 5000 Employees,2006,Company - Public,Internet,Information Technology,$2 to $5 billion (USD),-1
Data Engineer,-1,"Job Description: Develop mathematical analysis models to use in development of Beshton’s website applications for identifying product marketing influence and other software products to assist clients in improving their business operations; Apply knowledge of statistics and mathematics to software data analysis and provide analytical support on big data projects; Assemble data such as product news and process the data to use in analysis, utilize analytical methods including regression analysis and hypothesis testing; Design and analyze statistical algorithms for use in software, collaborate with application developers to ensure the mathematical accuracy of the platform; Perform complex analysis computations to validate software models and algorithms, identify discrepancies, and develop solutions; Disseminate analysis results through presenting reports with data visualizations to explain analysis process and results to peers and management. Requirement: Minimum a Master’s Degree in Applied Mathematics, or a closely related field. Apply send email to career@beshton.com",4.0,"Beshton Software
4.0","San Francisco, CA",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Platform Data Engineer,$98k-$172k (Glassdoor Est.),"Company Description Square builds common business tools in unconventional ways so more people can start, run, and grow their businesses. When Square started, it was difficult and expensive (or just plain impossible) for some businesses to take credit cards. Square made credit card payments possible for all by turning a mobile phone into a credit card reader. Since then Square has been building an entire business toolkit of both hardware and software products including Square Capital, Square Terminal, Square Payroll, and more. We’re working to find new and better ways to help businesses succeed on their own terms—and we’re looking for people like you to help shape tomorrow at Square. Job Description As a Platform Data Engineer, you will develop and maintain data infrastructure necessary to the continued growth of Square. You will collaborate with many teams across Square to understand data needs and turn those needs into data infrastructure and services, monitor and maintain the health of the data infrastructure, and have an impact on the data ecosystem within Square. You will: Create and maintain our real-time data pipeline Build ELTs to consume data from multiple sources, including relational databases, event streams, and third-party data, all populating our data warehouse and other data stores Work with various infrastructure and operations teams to maintain our data infrastructure Help build and maintain services and tooling to ensure resiliency, fix data discrepancies, and enhance the customer experience Monitor daily execution, diagnose and log issues, and fix pipelines to ensure SLAs are met with internal stakeholders Qualifications 3+ years experience in Data Engineering, Software Engineering, or similar role 3+ years experience writing complex SQL and ETL development, processing large datasets with cloud-based warehouses like Snowflake, Google BigQuery, and Amazon Redshift 3+ years experience working with relational databases Strong experience at least one of Python or Java Experience with message queues (such as Kafka, Kinesis) Experience working in and deploying to cloud environments, like AWS, GCP Experience working with task scheduling frameworks like Airflow and Luigi Experience with container deployment platforms and tools, such as Kubernetes, Docker, Helm, and Terraform Bonus: Experience with asyncio Python Bonus: Experience with Javascript / Vue Technologies we use and teach: SQL and Python Kafka and Confluent platform Snowflake and MySQL Airflow Kubernetes, Docker, Helm, and Terraform AWS, GCP Additional Information At Square, we value diversity and always treat all employees and job applicants based on merit, qualifications, competence, and talent. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance. Applicants in need of special assistance or accommodation during the interview process or in accessing our website may contact us by sending an email to assistance(at)squareup.com. We will treat your request as confidentially as possible. In your email, please include your name and preferred method of contact, and we will respond as soon as possible. Perks At Square, we want you to be well and thrive. Our global benefits package includes: Healthcare coverage Retirement Plans Employee Stock Purchase Program Wellness perks Paid parental leave Paid time off Learning and Development resources",4.0,"Square
4.0","San Francisco, CA",-1,1001 to 5000 Employees,2009,Company - Public,Computer Hardware & Software,Information Technology,$1 to $2 billion (USD),-1
Data Engineer,$140k-$238k (Glassdoor Est.),"Company Description Wish is a mobile e-commerce platform that flips traditional shopping on its head. We connect hundreds of millions of people with the widest selection of delightful, surprising, and—most importantly—affordable products delivered directly to their doors. Each day on Wish, millions of customers in more than 160 countries around the world discover new products. For our over 1 million merchant partners, anyone with a good idea and a mobile phone can instantly tap into a global market. We're fueled by creating unique products and experiences that give people access to a new type of commerce, where all are welcome. If you’ve been searching for a supportive environment to chase your curiosity and use data to investigate the questions that matter most to you, this is the place. Job Description Our engineers move extremely fast, while solving unique and challenging problems. Our team is small and nimble. We release every day to ensure that engineers are able to iterate quickly, and make an impact immediately. We’re looking for engineers to work on our massive semi-structured datasets. You'll develop software to process, transform and analyze the data to identify signals from billions of events we collect every day. You'll provide insights that improve the experience of hundreds of millions of users worldwide. You should be results-driven, highly motivated, and have a track record of using data analytics to drive the understanding, growth, and success of a product. What you'll be doing: Design and Develop data collecting and processing systems to handle large data sets. You’ll have the opportunity to design innovative data solutions and solve challenging problems. Design, Develop and Support highly-parallel, and fault-tolerant applications. Build and integrate scalable backend systems, services, platforms, and tools Contribute to the design and code of complex data pipelines operating on production data Optimize current approaches to efficiently handle ever-increasing volumes of data Build proof of concept using modern technologies and convert them into production-grade implementation. Create best-practice reports and dashboards based on data mining, analysis, and visualization Qualifications 5 + years of experience as a Software Engineer or Data Engineer using Python, java or any other programming language Expertise with SQL and data storage systems Experience and knowledge of modern data warehouse, pipeline and reporting/analytic techniques and tools such as Airflow, Presto/Hive, Spark, or any other scheduling frameworks, Tableau or other reporting tools Experience working on Amazon Web Services or other cloud computing platforms Bachelor's degree in Computer Science or related field. Preferred Qualifications: Experience in data visualization a plus. #LI-BD1 Additional Information Wish values diversity and is committed to creating an inclusive work environment. We provide equal employment opportunity for all applicants and employees. We do not discriminate based on any legally-protected class or characteristic. Employment decisions are made based on qualifications, merit, and business needs. If you need assistance or accommodation due to a disability, please let your recruiter know. For job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records. Individuals applying for positions at Wish, including California residents, can see our privacy policy here.",3.5,"Wish
3.5","San Francisco, CA",-1,501 to 1000 Employees,2011,Company - Private,Other Retail Stores,Retail,$1 to $2 billion (USD),-1
Data Engineer,$108k-$194k (Glassdoor Est.),"About REX - Real Estate Exchange, Inc. REX is a well-funded, game-changing real estate technology startup with offices in Austin, Los Angeles, and the Bay Area. With the goal of improving the lives of homebuyers and sellers, REX created a digital platform and real estate service that eliminates traditional agent commissions and shifts control away from agents over to those who matter most: consumers! REX saves homesellers thousands of dollars in fees by going around the MLS to target home-buyers directly with sophisticated marketing that has never been used in real estate. Since its launch in Southern California, REX has expanded to 17 states and over 250 employees. Throughout the years, REX has represented homes cumulatively valued at over $1 billion and in the process, saved customers over $20 million in fees they otherwise would have paid traditional brokers. About the Position As a Data Engineer, you will enable REX’s data team to operationalize models, organize data aggregation, and serve large data apis for our consumer experience team. This role will join a new team ready looking to scale our data platforms in scale and speed. With that said, applicants for this role will need to be flexible with new technologies, conversant in development operations practices, and ready to define data lifecycles from initial collection through serving. Experience & Qualifications Our ideal candidate brings the following: Significant experience with multiple data platforms, including SQL and NoSQL systems. Experience with ETL pipelines Comfortable coding in backend languages such as Java, Python, and Go AWS experience Experience with distributed batch data-processing techniques and tools Bonus Points: Experience with web-scale architectures Java proficiency Python proficiency Additional Information As a pioneer in our industry, REX is setting new standards in the marketplace – for quality, innovation, integrity, professionalism, drive, consumer happiness, and social good. Our culture, together with our business vision and goals, serve as an orientation for leadership and a guide for how we conduct ourselves in day-to-day business. They also form the foundation for hiring, encouraging and rewarding great people. In addition, REX has been committed to doing good things for real estate consumers and to providing homes for those in the greatest need, wherever they may be. For every 20 homes we sell, we provide a home for a family in need. We started by funding the construction of a home for a family in Cambodia at the end of 2015. In addition to funding homes, the REX team regularly provides hands-on support to local nonprofits that provide shelter to families. Compensation: Competitive salary with an equity stake in this fast-growing tech company. Contact: careers@rexchange.com Powered by JazzHR 8NcfFkfzMx",4.0,"REX
4.0","Redwood City, CA",-1,51 to 200 Employees,2015,Company - Private,Real Estate,Real Estate,Unknown / Non-Applicable,-1
Senior Data Engineer,$120k-$211k (Glassdoor Est.),"Senior Data Engineer

Data is our fuel at Turo. It is ever-more abundant and valuable, but it's a raw material. Harnessed by data scientists and machine learning engineers, it propels Turo on its mission to put the world's 1.5+ billion cars to better use, delighting our customers with matching the right car for their next adventure from an exceptionally diverse selection, and at the same time helping our marketplace remain safe.

About You

At Turo, you will have the opportunity to use the latest technologies to build robust scalable solutions for collecting, analyzing large data sets, creating and maintaining data pipelines, data structures and reports. In this role, you'll partner closely with software engineers, data analysts, and data scientists to power analytical data products, experimentation, and machine learning models.

Responsibilities
On a daily basis, you will work with members of the team to update our data engineering roadmap and execute upon those initiatives
Build new technology stack for highly scalable and available data pipelines used by Turo Product, Engineering, Data Scientists, Marketing, Customer Operations, and Finance teams
Design canonical data models for various business domains. Formulate a vision to connect all data in the Turo ecosystem
Develop, deploy and maintain workflow management tools such as Airflow, Jenkins etc in cloud environments.
Develop, deploy and maintain streaming solution such as Apache Spark streaming, Kafka and Apache Flink in cloud environments.
Using cloud technology such as AWS, Kubernetes, Docker, Redshift, EMR
Data security automation
Data microservices development
Requirements
Past experience building ETL processes
Strong programmer who views their code as a craft
Experience with a workflow manager — Airflow, Luigi, Jenkins, etc.
Experience working with data tools in the public cloud (AWS, GCP, Azure)
Able to understand technical details and communicate with other engineers, as well as communicating with less technical members from other teams.
Enjoys mentoring & teaching other engineers
3+ years of relevant experience
Benefits
Competitive salary and equity for all full-time employees
Employer paid medical, dental, and vision insurance
Generous paid time off, paid holidays, paid volunteer time off, and paid parental leave
Weekly catered lunch with a fully-stocked kitchen
Company-sponsored happy hours and team events
Turo host matching and vehicle reimbursement program
Turo travel credit every month
About Turo

Turo is the world's largest peer-to-peer car sharing marketplace where you can book any car you want, wherever you want it, from a vibrant community of trusted hosts across the US, Canada, and the UK. Guests choose from a totally unique selection of nearby cars, while hosts earn extra money to offset the costs of car ownership. A pioneer of the sharing economy and the travel industry, Turo is a safe, supportive community where the car you book is part of a story, not a fleet. Discover Turo at https://turo.com, the App Store, and Google Play, and check out our blog, Field Notes.

Turo has raised $450M to date from top-tier investors, including IAC, Daimler AG, Kleiner Perkins, GV, Canaan Partners, August Capital, and Shasta Ventures.

Turo cultivates a tight-knit team of smart, critical thinkers who care about their work and their colleagues. Our recruiting team is always on the lookout for supportive, down-to-earth, pioneering, and efficient candidates to grow our team's talent and enrich our culture.

Read more about the Turo culture according to Turo CEO, Andre Haddad.

We're an equal opportunity employer and value diversity at our company. We don't discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status. When in doubt, please apply!

#LI-ZS1",4.5,"Turo
4.5","San Francisco, CA",-1,201 to 500 Employees,2009,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,$68k-$128k (Glassdoor Est.),"Team Overview The Data Engineering team at Glu builds core data infrastructure and applications in support of all areas of our business, including our studio teams, user acquisition, monetization and finance. Glu is passionate about maximizing the value that data and analytics can provide to the business and is aggressively investing in new capabilities. Our team covers a lot of ground from data ingestion through to machine learning applications. Role Overview We leverage a cutting-edge tech stack to build both batch systems (YARN+Spark/Hive) and stream processing applications (Kinesis/Flink/Spark Streaming/Druid) that operate efficiently at high scale. The ideal candidate has a strong engineering background and has built robust data platforms and pipelines and takes complete ownership of their area of expertise. This is a fantastic opportunity to use your engineering skills to make a material impact on a highly valued analytics platform. You'll most often be: Taking ownership of and developing critical new features for our next-generation analytics platform, supporting Glu's worldwide studios and central functions such as marketing and finance Building scalable, accurate and extensible stream processing applications using cutting-edge technology such as Spark Streaming and Apache Flink Implementing complex and highly scalable end-to-end data pipelines, using Elastic Beanstalk, Kinesis, EMR, Spark, Hive, Druid, Cassandra Building data support for our personalization and experimentation efforts, solving problems from statistical test automation to building real-time M/L applications And your skills and experience include: Bachelor's degree in computer science/mathematics/engineering, or other fields with proven engineering experience More than 3 years of software engineering experience, especially working on back-end data infrastructure Proficiency with at least one of the following languages: Java, Python, Scala Experience with distributed stream processing technologies such as Flink, Spark Streaming and/or Kafka Streams Experience with AWS Ecosystem, especially Kinesis, EMR, Lambda, and Glue Knowledge of NoSQL application data stores i.e. Druid, HBase, Cassandra, DynamoDB, Redis Bonus points: Experience with high-scale machine learning, i.e. Spark M/L, SageMaker, etc Experience with SQL and SQL-like languages, especially Hive Experience with CI/CD process, testing framework, and containerization technology Experience building data-rich web applications, especially with technologies like Angular, Node.js, and Elastic Beanstalk",3.5,"Glu Mobile
3.5","San Francisco, CA",-1,501 to 1000 Employees,2001,Company - Public,Computer Hardware & Software,Information Technology,$100 to $500 million (USD),-1
Data Engineer,-1,"South San Francisco Bay, California Requirements: BS in Computer Science or Math or equivalent experience At least 2+ years of experience in cryptocurrency 4-7 years building data pipelines using Spark in Scala, Kafka, Yugabyte, Cassandra Required Skills for all Data Science Team Candidates: eSolid development foundation >= 7 years developing highly usable analytics tools Highly self-directed & able to work a project from conception through delivery (e.g., from formulation, execution and evaluation of the research through to development and integration into product) Good understanding of the blockchain technical space and data structures >= 4 years analyzing relevant data structures (i.e., in tabular or network forms) Clear dedication to continuous learning Python, Jupyter, Scala, Avro, Spark, Ignite, Kafka, Cassandra, Yugabyte > 2 years’ experience in large-scale network analysis, graph database implementation at scale We offer a comprehensive benefits package, including competitive salaries, stock options for senior roles, medical, dental and vision — which cover domestic partners as well — plus life and disability coverage, 20 vacation days, and a 401K plan. If you are interested, send your resume to contact@ciphertrace.com",3.7,"CipherTrace
3.7","San Francisco, CA",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Data Engineer,-1,"As a Data Engineer, you will design & build mission-critical data pipelines and platform that enable analytics for our internal & external customers on one of the biggest specialty healthcare data. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities Create and maintain optimal data pipeline & platform architecture. Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Work on fast iteration cycles and tight deadlines using an agile methodology. Requirements BS/CS, MS/CS or equivalent. Proficient coding skills in Python/ scripting languages. Expertise with Big Data technologies on AWS, i.e. EMR, Spark, Parquet, Kinesis, Hive, Presto, Airflow, etc. Bonus points for working with Infrastructure-as-Code (e.g. Terraform). Huge plus with experience on running ML/AI models in production on large scale data. Prior healthcare experience is an advantage. Excellent written and oral communication skills. Ability to set and manage priorities judiciously. Ability to articulate ideas to both technical and non-technical audiences. Exceptionally self-motivated and directed. Superior analytical, evaluative, and problem-solving abilities. To apply, please email your resume to: jobs@veranahealth.com",-1,Verana Health,"San Francisco, CA",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Data Engineer,$99k-$182k (Glassdoor Est.),"Numerator is a data and technology company reinventing market research. Headquartered in Chicago, IL, Numerator has 1,600 employees worldwide. The company blends proprietary data with advanced technology to create unique insights for the market research industry that has been slow to change. The majority of Fortune 100 companies are Numerator clients. Job Description Numerator is looking for a Senior Data Engineer who is a Big Data enthusiast and has a passion for working with an interesting robust set of data. This person will work in our platform to help ensure that our data quality is flawless. As a company, we have millions of new data points every day that come into our system. You will be working with a passionate team of engineers to solve challenging problems and ensure that we can deliver the best data to our customers, on-time. You will be using the latest cloud data warehouse technologies to build robust and reliable data pipelines. Duties/Responsibilities Include Develop expertise in the different upstream data stores and systems across Numerator Design, develop and maintain data integration pipelines for Numerators growing data sets and product offerings Collaborate with product and engineering teams to take requirements from prototype to production Build data validation testing frameworks to ensure high data quality and integrity Write and maintain documentation on data pipelines and schemas Skills & Requirements Requirements Bachelors degree in Computer Science or related field of study required; Masters degree preferred 5 + years of experience in the data warehouse space Knowledge of software engineering best practices across the development lifecycle, coding standards, code reviews, source management, build processes, testing, and operations Knowledge of and experience implementing data security and governance best practices Expert in SQL, including advanced analytical queries, window functions, CTEs and query optimization Advanced proficiency in Python (data structures, algorithms, object oriented programming, using APIs)Experience administering a cloud data warehouse (Redshift, Snowflake, Vertica) Experience with a data pipeline scheduling framework (Airflow) Experience with schema design and dimensional data modeling Exceptional candidates will have Amazon Web Services (EC2, DMS, RDS) experience Terraform and/or ansible (or similar) for infrastructure deployment Airflow - Experience building and monitoring DAGs, developing custom operators and using script templating solutions Experience supporting production systems and developing on-call/incident management playbooks Ability to work with team members located in multiple geographies and time zones. Curious and interested in learning about the latest in data warehouse technology Interest and willingness to mentor junior team members What we offer you An inclusive and collaborative company culture - we work in an open environment while working together to get things done, and adapt to the changing needs as they come. An opportunity to have an impact in a technologically data driven company. Ownership over platforms and environments of an industry leading product. Market competitive total compensation package. Volunteer time off and charitable donation matching. Strong support for career growth, including mentorship programs, leadership training, access to conferences and employee resource groups. Regular hackathons to build your own projects and Engineering Lunch and Learns. Great benefits package including health/vision/dental, unlimited PTO, 401k matching, travel reimbursement and more. If this sounds like something you would like to be part of, we’d love for you to apply! Don't worry if you think that you don't meet all the qualifications here. The tools, technology, and methodologies we use are constantly changing and we value talent and interest over specific experience. We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.",4.1,"Numerator
4.1","San Francisco, CA",-1,1001 to 5000 Employees,2004,Company - Private,IT Services,Information Technology,$100 to $500 million (USD),-1
Senior Data Engineer,$159k-$283k (Glassdoor Est.),"Los Gatos, California Infrastructure and Tooling Netflix makes up 1/3 of internet traffic, and we're proud to deliver entertainment that over 180 million global customers enjoy. Behind the scenes, Netflix is powered by several key pieces of technology, including Open Connect (our CDN), Video Encoding Technologies and Adaptive Streaming. Decisions on how to improve and evolve video streaming performance for our customers are driven by data. We're looking for someone to transform petabytes of incoming data on video performance, encoding efficiency and network efficiency into well-designed, high-quality data structures that empower critical decision-making for teams within Netflix. Once a member presses play, we track the entire interactive experience, including the quality of the video, and the network performance as each byte of data is transferred, so you'll work with data at incredible scale and collaborate with best-in-class data engineers and analytic experts. You'll become an authority in the world of video streaming delivery (no prior knowledge necessary, but curiosity to learn is a must), and the projects you'll work on will be truly impactful to Netflix members worldwide. In the meantime, learn more about the Streaming Data Engineering team. What you'll do You’ll take ownership and increase automation and scale of complex data sets that drive use cases by our analytical partners. This includes the netflix ‘playback’ data, application performance data, network throughput and device debug information. You’ll build robust data pipelines that output very high data quality at scale using any combination of Spark, Flink, Python and Scala. As someone who is working at the heart of understanding what is being watched on Netflix, you will partner closely with Content teams (who is watching what?), Network Teams (what kind of network performance are they getting?) and our Client Teams (how is my app performing on their device?) We need to process data more quickly than ever to enable rapid experimentation in an increasingly nimble engineering organization. You’ll help implement our business logic to be compatible with real-time/stream processing frameworks. Who you are Have several of the characteristics/skills listed below and have passion and self-drive to quickly learn in areas of less familiarity. We believe the experience in your years is more important than your years of experience. Enjoy a high level of autonomy in managing cross-functional engineering projects. We enjoy a culture of Freedom & Responsibility. Have experience building production data pipelines and/or strong knowledge of SQL (Knowledge of Spark, Flink or Hive/Hadoop is helpful). Have hands-on experience with schema design and data modeling. Have programming proficiency in Python, or Scala/Java. You have a software engineering mindset and strive to write elegant, maintainable code. You may even be a software engineer with a focus or passion for data-driven solutions. Have strong SQL skills and knowledge Have excellent communication in sharing context to effectively collaborate with analytical partners, domain experts and other consumers of your work, preferably in supporting an engineering or product function. We like to collaborate across teams and so should you. Ambitious and willing to take action, but not stubborn. Awareness to recognize when you're wrong and move past your own mistakes. We are humbly confident in ourselves and our work. Netflix Culture Our culture is unique, and we live by our values. You will need to be comfortable working in the most agile of environments. Requirements will be vague, and iterations will be rapid. You will need to be nimble and take smart risks. Learn more about Netflix’s culture. APPLY NOW Share this listing: LINK COPIED",4.1,"Netflix
4.1","Los Gatos, CA",-1,5001 to 10000 Employees,1997,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Data Engineer,$93k-$174k (Glassdoor Est.),"We’re looking for an experienced data engineer who will help us process, gather, and analyze large-scale datasets. This role is part of our Datalab team that is responsible for designing, developing, and deploying machine learning and deep learning solutions. This will include ensuring the availability of appropriate datasets to build sound, robust, and updated models. You’ll also make sure that the performance and stability of these models are appropriately monitored and measured.

What You’ll Do (but is not limited to)

Working closely within a team of machine learning engineers to apply innovative solutions to large unstructured datasets
Sourcing, analyzing, and processing data from various sources (e.g., BigQuery and BigTable) to build cohesive datasets
Developing tools to manage our dataset, data pipelines, and other artifacts created during the whole model life cycle
Supporting the deployment of machine learning models on our infrastructure, including containerization, instrumentation, and versioning
Supporting the whole life cycle of our machine learning models, including gathering data for (re)raining A/B testing, deployment, monitoring, retraining, and redeployments
Working closing within a distributed team to analyze and apply innovative solutions of billions of documents
Communicating your approach and results to a wider audience through articles and presentations

Your Qualifications

Documented success with data engineering at scale in a SaaS or Cloud environment
Ability to implement efficient data import, cleansing, and transformation functions on a large scale
Experience with large datasets and distributed computing, especially with the Google Cloud Platform
Good knowledge of any major cloud provider environment (such as CGB, AWS, Azure)
Excellence in SQL
Fluency in Python, Docker, Kubernetes

Bonus Skills

Sound knowledge of machine learning and deep learning
Experience with advanced analytical modeling and statistical forecasting techniques
Good understanding of No-SQL and Graph databases
Knowledge of Java, Scala, or Golang programming languages

Our Benefits

Competitive salaries
Stock Options
Comprehensive benefits for you and your family (low premiums and deductibles!)
Flexible hours and responsible time off
Gym, cell phone, and commute reimbursement
Healthy lunches, breakfast, and bottomless snacks and beverages
401(k) Retirement Plan (Traditional and Roth)
Employee Assistance Program
Perks including discounted pet insurance, theme park tickets, travel, and more!
Board Games -you name it, we got it! Take a break and get to know other Egnyters!
SoFi online financial services: student loan refinancing, personal loans, and investing
FREE Egnyte lifetime membership

Equal Opportunity Employment

Egnyte is an equal opportunity employer and values diversity at our company. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

About Egnyte

In a content critical age, Egnyte fuels business growth by enabling content-rich business processes, while also providing organizations with visibility and control over their content assets. Egnyte’s cloud-native content services platform leverages the industry’s leading content intelligence engine to deliver a simple, secure, and vendor-neutral foundation for managing enterprise content across business applications and storage repositories. More than 16,000 customers trust Egnyte to enhance employee productivity, automate data management, and reduce file-sharing cost and complexity. Investors include Google Ventures, Kleiner Perkins, Caufield & Byers, and Goldman Sachs. For more information, visit www.egnyte.com",4.0,"Egnyte
4.0","Mountain View, CA",-1,201 to 500 Employees,2008,Company - Private,Enterprise Software & Network Solutions,Information Technology,$50 to $100 million (USD),-1
Senior Data Engineer,$123k-$223k (Glassdoor Est.),"Founded in 2007, Lumos Labs is the leader in brain training. Over 100 million users have used our Lumosity application to train their cognitive skills, playing our games over 6 billion times. We are now applying our unique approach to building scientifically-validated, delightfully fun, and deeply engaging products to other aspects of brain health and wellness. We are incubating multiple “seed-stage startups” within the company, in areas like creativity, mindfulness, emotional intelligence, and more. We are building one of the premiere digital health brands and are looking for talented, mission-oriented people to join us on the journey. Lumos is looking for a senior engineer to join our Data Engineering team. This team powers all of our applications by building systems and infrastructure to effectively collect, distribute, and analyze data at scale, as well as creating next-generation tools to make sense of it all. We’re looking for an experienced engineer who has a passion for data, clean code, and operational efficiency. YOU WILL Drive the design and implementation of systems at the core of our data infrastructure on a small, but effective team of data engineers Work with our data scientists to build high-throughput systems that allow us to continually invest in a rich, engaging experience Help envision, implement, and evangelize new features, pipelines, and platforms that not only deliver data, but insight Collaborate with product, engineering, and analysis teams across the organization, as our systems are loosely-coupled, but highly utilized YOU HAVE A consistent track record of building highly-available back-end systems that scale A deep understanding of patterns of data architecture, back-end system design Proficiency in Scala, Python, Java, or other modern systems languages Experience with databases such as RDBMS or columnar DB Experience with Kafka, Spark, Flink, Storm, or similar Strong analytical and debugging skillsA dedication to engineering excellence Not required, but nice to haves: Exposure to the principles of functional programming (nice to have) Some experience with front-end technologies and/or statistical analysis tools (nice to have) Experience with A/B testing or machine learning infrastructure (nice to have) WE OFFER Competitive compensation, including salary and benefits on par with later-stage companies and equity compensation on par with earlier-stage ones. The excitement and fun of the early-stage startup paired with the maturity and stability of a later-stage business with substantial revenue and company infrastructure. An egoless environment that fosters collaboration of all types. We have a culture of ownership, trust, camaraderie, and fun. A focus on sustainability - in our products, our business, and for our employees. A culture that values coming together and sharing our varied interests to illuminate new and inspiring ideas, all with a focus on helping our users make meaningful improvements in their lives. The opportunity to learn, grow, and bond with a tight community of co-workers based in SF, and with teammates around the world (NY, Portland, Minnesota, Brazil, Serbia, Ukraine, India). All of the perks that one would expect from a modern-day SF tech company. Please review Lumosity's California Job Applicant Privacy Policy here.",4.2,"Lumos Labs, Inc.
4.2","San Francisco, CA",-1,51 to 200 Employees,2005,Company - Private,Internet,Information Technology,$1 to $5 million (USD),-1
Data Engineer,$81k-$149k (Glassdoor Est.),"Who We Are

Groundspeed Analytics, Inc. is a data analytics firm that is making sense of the unstructured information driving most decisions in the commercial insurance industry. We are team of insurance experts, data scientists, software engineers, and analysts working in a collaborative and fun environment. We work with some of the largest insurance companies in the world. Our clients treat us like partners and friends, and we are deeply committed to them.

We are also one of the fastest growing InsurTech firms in the world, recently recognized as one of the 250 fastest growing FinTech startups in 2018 by CB Insights. We are co-located in Ann Arbor, Michigan (named the Best Place to Live in 2018 by Livability) and Atlanta, Georgia.

What We Do

Groundspeed is a data science and artificial intelligence (AI) company serving commercial property and casualty insurance carriers, brokers and managing general agents (MGAs). Our solutions unlock the value in unstructured information such as loss runs, exposure schedules, and policy data to automate core processes and provide powerful predictive risk analytics that improve margins, identify profit pools, and improve customer experience.

We are committed to building innovative solutions that apply approaches from the fields of deep and classical machine learning, natural language processing, and analytics. Our insurance industry domain knowledge paired with our technology expertise positions us uniquely in the market to address this challenging and high-value problem space. To meet that goal, we are working to create a world-class engineering and data science team.

Job Summary

We are looking for creative, enthusiastic, and collaborative professionals who don't shy away from hard problems. Our team is made up of people from a diverse set of fields, industries, and backgrounds — all of whom are focused on bringing everything they can offer to make this a successful team. We look for people who have a personal sense of responsibility to produce the best products and services possible, and expect the same from their team members. We are a culture that believe in iteration and strives to add value each and every sprint, and to do so without ego.

Our ideal candidate is an experienced data engineer with a proven track record of owning the data flow of an organization for at least, the last five years. This isn't a ""connect-the-dots"" dev position — our engineers must be able to take loosely defined, complex ideas and run with them, working toward elegant results. You'll work with small teams to help define and, most importantly, build the systems and tooling for our data pipeline and analytics products,facilitating AI-driven automation, big data systems, analytical models, and the applications that tie it all together.
Responsibilities
Provide hands-on technical leadership designing, developing, testing, deploying, and improving software
Write reliable, scalable, maintainable software primarily using JavaScript (though we use other languages as well)
Build systems that empower internal clients to accomplish more
Manage individual projects priorities, deadlines and deliverables with your technical expertise
Participate in code reviews and provide mentorship in a variety of software engineering areas to other team members
Contribute to product design and architecture planning of major systems and features
Help lead and scale our engineering organization by collaborating with leadership and stakeholders on a regular basis
Requirements
At least three years of experience contributing significantly to the creation and maintenance of data models and ETL systems
Knowledge of agile software development and continuous integration / deployment principles
Persistent against challenges, quick to debug, and identifies problems to solve before users encounter issues
Chooses tools and technologies for efficiency and performance, and understands trade-offs associated with those choices
Understands the business impact of technical decisions and is empathetic to the user experience
Demonstrates knowledge through practice of documentation, code reviews, mentoring, and contextualizing work across teams
Ability to communicate with multiple stakeholders and enforce data standards and quality
Why Choose Groundspeed?
Participate in the engineering process from start to finish — we value a culture of responsibility
A collaborative team with mentors for every skill-level
Use the best tools — productivity is key
Immediate impact on challenging engineering tasks — rapid prototyping and iteration are essential to an efficient engineering process
A culture that is committed to work-life balance
A broad and growing benefits package that includes 3% contribution to a 401K plan, amazing health insurance, wellness reimbursement, snacks and drinks, and lunches every Friday
Contribute to one of the fastest growing startups in Michigan in terms of revenue, headcount, and tech
We are Committed to a Diverse Workforce

Groundspeed is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. For more information about Groundspeed, please visit www.groundspeed.com.

Groundspeed Analytics, Inc. is an E-Verify Employer.",3.3,"Groundspeed
3.3","San Francisco, CA",-1,51 to 200 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Data Engineer (Remote),-1,"Our client headquartered in San Francisco, CA is seeking a Data Engineer for a full time permanent opportunity open due to growth and expansion. This is open to 100% remote but must be able to work 9 am- 5pm (Pacific Time).

Responsibilities:

Design and implement reliable ETL data pipelines.
Enabling data-driven decision making across the company
Collaborating with other teams (Engineering, Data Science, Product, Customer Success) to understand their requirements for analysis and then implement solutions as needed
Designing, implementing, iterating on, and maintaining data pipelines and storage systems

Qualifications:

Proficient in one of the following programming languages: Ruby, Go, Python, Scala, Elixir, or Java
Strong experience with relational databases, including ER modeling and performance optimization such as PostgreSQL, Redis, and ElasticSearch
SaaS models and cloud services preferably AWS
Utilize container based technologies to help support a micro service based architecture
Experience designing and operating infrastructure (we use Terraform + Ansible to manage our stack on AWS)
Must be a US Citizen or Green Card Holder

To apply, please send your resume to Sheila@libertyjobs.com.

Sheila Shamloo
Liberty Personnel Services Inc.
Sheila@libertyjobs.com
https://www.linkedin.com/in/sheila-shamloo-026420114/

#LI-SS1

#IT

#midsenior",4.3,"Liberty Personnel Services
4.3","San Francisco, CA",-1,1 to 50 Employees,2003,Company - Private,Staffing & Outsourcing,Business Services,$1 to $5 million (USD),-1
Data & Analytics- Data Engineer,$65k-$121k (Glassdoor Est.),"About Slalom Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of what’s possible—together. Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to over 8,000 employees. We were named one of Fortune’s 100 Best Companies to Work For five consecutive years from 2016 - 2020 and are regularly recognized by our employees as a best place to work. You can find us in 35 cities across the U.S., U.K., Australia, and Canada. The Data & Analytics teams across Slalom Northern California are all hiring! Come make an impact with our East Bay, Sacramento, San Francisco, or Silicon Valley markets. Data Engineer Consultant As a Data Engineer for Slalom Consulting, you'll work in small teams to deliver data pipelines and data models for our clients. You will design and build highly scalable and reliable modern data platforms including data lakes and data warehouse using Amazon Web Services, Azure, Google Cloud. Your work will include a variety of core data warehousing tools, Hadoop, Spark, event stream platforms, and ETL tools such as Airflow. In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics. Who are you? You have passion for data! You’re a smart, collaborative person who is excited about technology and driven to get things done. You’re not afraid to be bring your authentic self to work. You embrace a continuous learner mentality. Who are we? We are engineers, makers, planners, architects, and designers. We choose to imagine things made better, and then set out on a journey to realize what’s possible. We’ll never trade the upside of wonder for the comfort of the familiar or the safety of convention. What technologies will you be using? Every element of a modern data & analytics stack. It’s about using the right technologies to solve problems and playing with new technologies to figure out how to apply them intelligently. We work with technologies across the board. Why do we work here? Each of us came to Slalom because we wanted something different. We wanted to make a difference, we wanted autonomy to own and drive our future while working with some of the best companies in San Francisco leveraging the coolest technologies. At Slalom, we found our people. Qualifications: Bachelor’s degree in Computer Engineering, Computer Science, Information Systems or related discipline 3+ years relevant experience Experience in capturing end users requirements and align technical solutions to the business objectives Understanding of different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.) Understanding of data architecture concepts such as data modeling, metadata, workflow management, ETL/ELT, real-time streaming), data quality 3+ years of experience working with SQL Experience with setting up and operating data pipelines using Python or SQL 1+ years of experience working on AWS, GCP or Azure Experience working with data warehouses such as Redshift, BigQuery and Snowflake Exposure to open source and proprietary cloud data pipeline tools such as Airflow, Glue and Dataflow Experience working with relational databases Experience with data serialization languages such as JSON, XML, YAML Experience with code management tools (e.g. Git, SVN) and DevOps tools (e.g. Docker, Bamboo, Jenkins) Strong analytical problem-solving ability Great presentation skills, written and verbal communication skills Self-starter with the ability to work independently or as part of a project team Capability to conduct performance analysis, troubleshooting and remediation",4.1,"Slalom Consulting
4.1","San Francisco, CA",-1,1001 to 5000 Employees,2001,Company - Private,Consulting,Business Services,$1 to $2 billion (USD),-1
Data Engineer,$77k-$139k (Glassdoor Est.),"Invitae is a healthcare technology company that leverages genetic information to empower doctors and patients to make informed medical decisions. Our software engineers work on a variety of projects ranging from innovations in healthcare systems to taming the chaos of biology. We're constantly improving our tools and technologies to deliver the highest quality actionable information for the patient. Our Data Infrastructure Team develops the data ingestion pipelines and data platform architecture that supports the analytical and reporting needs of internal business stakeholders, data scientists, and our machine learning team. This is a hands-on role. What you will do: Understand our complex data ecosystem and build ETL solutions Develop a real-time streaming infrastructure that supports critical business functions Design and develop tools to enable teams to consume and understand data faster Collaborate with multiple teams and own solutions from end-to-end We look for engineers who: Are self-starters and can work towards a larger goal with minimal guidance Prior experience utilizing data warehousing or building out data warehouses Have at least 3 years of hands-on experience working with large datasets, pipelines, and warehouses Have a focus on high-quality code, including automated testing and coding best practices Have experience with messaging/queuing systems or stream processing systems Have architected distributed systems with infrastructure automation, monitoring and alerting At Invitae, we value diversity and provide equal employment opportunities (EEO) to all employees and applicants without regard to race, color, religion, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance.",3.7,"Invitae
3.7","San Francisco, CA",-1,1001 to 5000 Employees,2010,Company - Public,Biotech & Pharmaceuticals,Biotech & Pharmaceuticals,$100 to $500 million (USD),-1
Data Engineer,$77k-$144k (Glassdoor Est.),"As member of the REAL System Data Team, this position will be responsible for building and maintaining data collection systems and non-structured data repositories. Under minimal supervision from the project lead, she or he will implement and enhance various data collection systems for our tablet and HMD applications. This role will work closely with the rest of the Data Team to provide Data Scientists with the data they need to develop machine learning models, and to deploy those models to once they are developed. What You'll Work On Implement tablet application click-tracking Implement event logging within our game system SDK Maintain and enhance data streams from body sensors and other devices Maintain and enhance data lake Deploy analytical models developed by data scientists, and other DevOps functions Implement wrappers for those analytical models in the game system SDK Write and maintain thorough technical documentation What You Bring Bachelor’s degree in a STEM field or equivalent work experience in software development Experience implementing and maintaining Apache Spark, Databricks, and other HDFS-based technologies Experience working with relational databases, preferably Postgres Experience with cloud based services such as Microsoft Azure, Kubernetes, Docker, Node.js., React, PostgreSQL, GraphQL preferred Experience with Python a plus High degree of accuracy and attention to detail Experience with Android mobile development a plus Exposure to game engines, like Unreal or Unity Strong oral, written and interpersonal communication skills Excellent technical documentation skill and organizational skills with ability to prioritize assignments Strong programming skills such as C, C++, Java, Python, SQL, Matlab Experience with game engine pipelines and technology stacks, including Unreal and Unity preferred Experience with VR headset technologies (Oculus Rift S, HTC Vive, etc.) preferred What We Offer A collaborative teamwork environment where learning is constant and performance is rewarded. The opportunity to be at the forefront of technology that is revolutionizing the treatment of some of the world's most devastating diseases. A generous benefits package that includes medical, dental, vision, and life insurance; a 401(k) match; and an Employee Stock Purchase Plan. Penumbra, Inc., headquartered in Alameda, California, is a global healthcare company focused on innovative therapies. Penumbra designs, develops, manufactures and markets novel products and has a broad portfolio that addresses challenging medical conditions in markets with significant unmet need. Penumbra sells its products to hospitals and healthcare providers primarily through its direct sales organization in the United States, most of Europe, Canada and Australia, and through distributors in select international markets. The Penumbra logo is a trademark of Penumbra, Inc. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status If you reside in the State of California, please also refer to Penumbra’s Privacy Notice for California Residents.",3.6,"Penumbra US
3.6","Alameda, CA",-1,1001 to 5000 Employees,2004,Company - Public,Health Care Services & Hospitals,Health Care,$100 to $500 million (USD),-1
Data Engineer,-1,"Title: Data Engineer

Location: SF 94107 (remote likely through end of yr) W2 contract only no 1099, or C2C no sponsorship provided.

Terms: 6 Month Contract + EXTS

Must Haves*:
5+ years of business and data engineering/analysis experience in Cloud Data Warehouse platforms.
8+ years of experience in gathering data/ETL requirements, analyze, profile data and building pipelines using ETL solutions such as Python and SQL
4+ years experience designing and developing complex ETL/ELT programs with the following Matillion, Python etc
Experience using Cloud database technologies such as RedShift, Snowflake
Intermediate python scripting ability – Must have experience sourcing data from APIs, Python framework development.
Plusses*:
Strong experience with SAP HANA
Strong experience working directly with business teams to understand/gather requirements
Understanding of business data analysis and build ETL/ELT frameworks.
Experience in AWS EC2 environments
Great communication skills and experience working with product owners and business users
Sales and/or Finance Data Domain experience.
Day 2 Day*:

A high tech Bay Area based client is looking for 3 strong Senior Data Engineer to help support it’s Data Technologies team. They need deep experience working with product owners and business users to understand requirements and develop solutions and someone who worked in assignments with reasonable longevity, current without breaks and well experienced (12+ years). This Data Engineer will work and report up to the Data Engineering Director, work cross functionally with many different business teams and help develop solutions using SQL, Python and Cloud Data Warehouse tools. This Data engineer will work to develop data warehouse solutions and build data pipelines from various SaaS based systems (Ananplan, Salesforce, RevPro) into a Snowflake based cloud data warehouse. Heavy SQL and intermediate python is required.
8+ years of experience in gathering data/ETL requirements, analyze, profile data and building pipelines using ETL solutions such as Python and SQL.
Develop Data models, Complex SQLs, ETL/ELT ingestion methods (Matillion, Python,SQL). Hands-on skills will be tested.
Support AWS Cloud platforms and databases such as Snowflake, Redshift.
Provides strategic advice for business processes and solutions.
Communicate complex data processes clearly and concisely both orally and in writing to technical and non-technical audiences.
Job Type: Contract

Pay: $60.00 - $80.00 per hour

Schedule:
Monday to Friday
Experience:
RedShift: 1 year (Required)
data engineering: 4 years (Required)
Python: 4 years (Required)
Work authorization:
United States (Required)
Contract Length:
5 - 6 months
Full Time Opportunity:
Yes
Work Location:
One location
Benefit Conditions:
Waiting period may apply
Work Remotely:
Temporarily due to COVID-19",3.9,"Insight Global
3.9","San Francisco, CA",-1,1001 to 5000 Employees,2001,Company - Private,Staffing & Outsourcing,Business Services,$1 to $2 billion (USD),-1
Staff BI and Data Engineer,$105k-$124k (Glassdoor Est.),"Job Description: Staff BI and Data Engineer
Location: San Jose, CA (or) New York, NY
Department: Data Engineering
Hours/Shift: Full Time
Reports To: VP, Data Engineering
Job Description:
Affinity Solutions is looking for a hands-on and self-driven Staff BI and Data Engineer, preferably with experience in the bank card loyalty/fin-tech/advertising/marketing space, to enhance and automate its data and analytics infrastructure to support its growing customer base and expanding partner ecosystem. The position is based in San Jose, CA., and reports into the Data Engineering division. The demand for advanced analytical solutions continues to grow exponentially, and this is your opportunity to grow with us in a fast-paced, collaborative environment.
The ideal candidate is a passionate and highly skilled individual, who can utilize programming and analytics tools such as SQL, Python, and Tableau to query and process large data sets to produce high quality customer facing data deliverables and insights. If you have experience designing and building business intelligence/analytics applications, especially around credit/debit card transactions, unified consumer behavioral and profile data, and excited about leveraging your experience to catapult a venture-backed company into hyper-growth, this job is for you.
Responsibilities:
Develop high quality analytical data assets with an eye towards process efficiency and automation through scripting. Experience in building data marts is a plus.
Build automated QA process to validate the quality of the data and report on data quality
Communicate and present data to both internal and external customers by developing reports/dashboards/charts using BI tools such as Tableau
Work closely with a dynamic and growing team of account managers, data engineers and data scientists to perform quantitative analysis of customer data, including gathering data requirements and validate data, applying judgement and statistical analysis to assist with planning and decision making.
Other responsibilities include but not limited to - data validation, troubleshooting issues, and process documentation.
Qualifications:
Bachelor’s or Master’s degree in Computer Science or related field such as Mathematics and Statistics, preferably with focus on Data Analytics.
At least 3 years of hands-on experience in designing and building data pipelines, analytical data applications and BI Reporting.
Proficient in SQL and Tableau, familiar with at least one coding language in Python/Shell scripting.
Experience in using Cloud based managed services and Big Data Environments for data warehousing/analytics is a big plus – e.g. Amazon RedShift, Google BigQuery, Spark, MapR etc.,
Very strong written and verbal communication skills; Ability to tell a story with the data
Analytical thinker, with an ability to evaluate multiple products/technologies to address various aspects of a big data platform.
Experience working on UNIX / Linux development and production environments
Experience working in Agile software development environments
Strong organization skills with attention to detail is a must.
Ability to manage multiple conflicting priorities, take proactive ownership of problems and outcomes, think outside the box
Knowledge of Retail and Financial verticals is useful but not required.",3.2,"Affinity Solutions
3.2","San Jose, CA",-1,51 to 200 Employees,1998,Company - Private,Advertising & Marketing,Business Services,Unknown / Non-Applicable,-1
Sr. Data Engineer - Ecommerce (Remote),$138k-$252k (Glassdoor Est.),"At CrowdStrike we’re on a mission - to stop breaches. Our groundbreaking technology, services delivery, and intelligence gathering together with our innovations in machine learning and behavioral-based detection, allow our customers to not only defend themselves, but do so in a future-proof manner. We’ve earned numerous honors and top rankings for our technology, organization and people – clearly confirming our industry leadership and our special culture driving it. We also offer flexible work arrangements to help our people manage their personal and professional lives in a way that works for them. So if you’re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to stop breaches and protect people globally, let’s talk.
About the Role: We're spinning up a brand new team and initiative at CrowdStrike to focus on Ecommerce. We'll be covering areas like the CrowdStrike App Marketplace and monetization (in-app purchasing/micro-transactions/etc), creative new ways to approach Licensing, Billing and Entitlement across the entire product portfolio, and all the backend analytics work to drive a better user experience and optimize engagement, conversion, cross-sell and up-sell. We'll also be spearheading the creation of new CrowdStrike offerings that are built on the existing world-class Falcon platform but target a completely ecommerce driven, low-touch, high engagement product experience. If you're passionate about building intuitive applications that customers will rave about, tell their friends about and can't wait to use daily, we want you on our team.
As a Data Engineer on the CrowdStrike Ecommerce team, you will be involved in building data systems at scale that ingest billions of events, decisions and transactions every day. In this role, you will help make it possible for internal and external customers to easily work with data we produce at massive scale. You will be part of a data platform team that is continuously developing and improving product features to protect our customers from increasingly complex threat actors attempting data breaches.
Job Responsibilities:
Design, develop, and maintain workflows that can process petabytes of data
Use advanced data analysis techniques and architectures that leverage technology to process large volumes of data to perform complex computations in a scalable, reproducible and automated manner.
Help us research and implement new ways for appropriate stakeholders to query their data efficiently and extract results in the format they desire
Participate in technical reviews of our products and help us develop new features and enhance stability
Continually help us improve the efficiency of our services so that we can delight our customers
Qualifications for Data Engineer: We are looking for a candidate with a BS and 5+ years or MS and 3+ years in Computer Science or related field. They should also have experience with the following software/tools -
A solid understanding of algorithms, distributed systems design and the software development lifecycle
Experience with relational SQL and NoSQL databases, including Postgres/MySQL, Cassandra, DynamoDB
Solid background in Web application development (C#, Java/Scala, Go, Node.JS) and scripting languages like Python
Golang experience is desirable
Experience building large scale data pipelines
Good test-driven development discipline
Reasonable proficiency with Linux administration tools
Proven ability to work effectively with remote teams
Experience with the following tools is desirable:
Strong familiarity with the Apache Hadoop ecosystem including: Spark, Kafka, Hive, etc
Recent hands-on experience with modern data platforms such as SQL, RDMS, or Graph DBs, and Data Visualization skills using BI tools such as Tableau, Power BI, Domo, or D3.
Kubernetes
Jenkins
#LI-JF1 #LI-Remote #Stack

Benefits of Working at CrowdStrike:
Market leader in compensation and equity awards
Competitive vacation policy
Comprehensive health benefits + 401k plan
Paid parental leave, including adoption
Flexible work environment
Wellness programs
Stocked fridges, coffee, soda, and lots of treats

We are committed to building an inclusive culture of belonging that not only embraces the diversity of our people but also reflects the diversity of the communities in which we work and the customers we serve. We know that the happiest and highest performing teams include people with diverse perspectives and ways of solving problems so we strive to attract and retain talent from all backgrounds and create workplaces where everyone feels empowered to bring their full, authentic selves to work.
CrowdStrike is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.

CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",3.9,"CrowdStrike
3.9","Sunnyvale, CA",-1,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$100 to $500 million (USD),-1
Data Engineer (Healthcare Map),$85k-$154k (Glassdoor Est.),"The Opportunity at Komodo Health Komodo aims to build the best healthcare data architecture in the industry. As we are maturing, we are investing in our core data assets, and continually expanding the number and diversity of the data sources we are ingesting. Our Data Ingestion team is a high-performing team working with big data at scale - they both ingest data and evolve the architecture to increase efficiency. The team is currently starting work on version 2.0 of their architecture, focused around improving efficiency, reducing operations and improving data quality. Our data comes in daily, meaning we do batch processing and not streaming. This role will be a key contributor to the design and implementation of data engineering pipelines to ingest billions of medical and prescription claims, as well as other data sources. Our data ingestion use cases are complex (hundreds of fields, many come as multiple data files that need to be linked), and implementing robust data quality processes and frameworks is crucial. Looking back on your first 12 months at Komodo Health, you will have… Owned ingestion of several complex data sources and / or enhanced existing data sources. Participated in design and implemented distributed data systems using Spark or similar technologies. Collaborated with product managers and data scientists to implement validation and/or data quality enhancements. Optimized the pipelines for performance and cost. Helped ensure smooth operations of the ingestion pipelines. Helped ensure patient privacy by implementing business rules. Some of the projects the team is currently working on: Building new architecture focused on improving consistency between different pipelines, improving ability to backfill, and increasing data quality. Ingesting new complex data sources. Many of our data sources consist of a large historical load, followed by regular incremental deliveries. Enabling data ingestion as a service - to enable other teams to self-serve their ingestion needs when they are less sophisticated What you bring to Komodo: Expertise with industry standard distributed systems (ie. Spark), data pipeline tools (ie. Airflow). Capable of quickly building expertise on an as-need basis on new tech stack. Demonstrable experience with Python. Understand and design for non-functional concerns such as performance, cost optimization, maintainability and developer experience. Strong communication with engineers, product managers. Ability to work as part of an agile collaborative team in a fast-paced environment. Experience with systems support, debugging and operations.",3.5,"Komodo Health
3.5","San Francisco, CA",-1,201 to 500 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Data Engineer- Remote,$76k-$140k (Glassdoor Est.),"Data Engineer, W2 Full-time, SFO Bay area or Austin, TX ( U.S Citizens, GC candidates) About the Role: Trianz is looking for passionate Data Engineers who are looking to tackle challenges and build solutions We are looking for a Data Engineer to not only build data pipelines but also extend the next generation of our data tools. As a Data Engineer, you will develop a clear sense of connection with our organization and leadership - as Data Engineering is the eyes through which they see the product. This is a partnership-heavy role. As a member of Infrastructure Strategy Data Engineering, you will belong to a centralized Data Science/Data Engineering team who partners closely with teams in Facebook’s Infrastructure organization. Through the consulting-nature of our team, you will contribute to a variety of projects and technologies, depending on partner needs. Projects include analytics, ML modeling, tooling, services, and more. The broad range of partners equates to a broad range of projects and deliverables: ML Models, datasets, measurements, services, tools and process. Minimum Qualifications: 5+ years of Python development experience. 5+ years of SQL experience. 3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M). 3+ years’ experience with Data Modeling. Experience analyzing data to discover opportunities and address gaps. 5+ years’ experience in custom ETL design, implementation and maintenance. Experience working with cloud or on-prem Big Data/MPP analytics platform(i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar). Need more details? Trianz is growing at a faster pace than the industry for the last five years. Read through some of the key industry recognitions we have received for our innovative execution and strategic client initiatives here. About Trianz Trianz simplifies digital evolutions through effective strategies and excellence in execution. Collaborating with business and technology leaders, we help formulate and execute operational strategies to achieve intended business outcomes by bringing the best of consulting, technology experiences and execution models. Powered by knowledge, research, and perspectives, we enable clients to transition to a digital enterprise by leveraging Cloud, Analytics, Digital, Infrastructure and Security paradigms. With offices in Silicon Valley, Washington DC Metro, Rosemont, Chicago, Austin, Boston, Denver, Irvine, Raleigh, San Francisco, Seattle, New York, Dubai, Bengaluru, Hyderabad and Chennai, we serve Fortune 1000 and emerging organizations across industries globally. For more information, visit www.trianz.com. Trianz is an Equal Opportunity Employer and does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law).",2.5,"Trianz
2.5","San Francisco, CA",-1,1001 to 5000 Employees,2001,Company - Private,IT Services,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,$84k-$151k (Glassdoor Est.),"Job Requisition ID #

20WD40715

Position Overview
Autodesk Construction Services (ACS) is seeking an exceptional data engineer to transform, optimize, test and maintain data pipelines and processing systems. This position will allow ACS to serve relevant data to all stakeholders, internally and externally. Also optimizing the infrastructure to ensure the data science and analytics teams can provide the best quality data to the business.

Responsibilities

Maintain/ develop a scalable database/ data warehouse through connecting disparate data tables housed across numerous organizational systems and different business lines
Maintain/ develop the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies
Optimize and maintain scripts on present data warehouses and present ETL
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader
Create and update financial models for decision support of new revenue-generating programs and initiatives

Minimum Qualifications

Bachelor's degree computer science, information systems, applied mathematics or a related discipline
5+ years of experience
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience with transforming, developing data structures, metadata, dependency and data workflows to support an Analytics function
Experience with salesforce administration
Experience with object-oriented/object function scripting languages: Python, Java, C++ etc.
Expertise in gathering data through multiple sources through API calls and scripting languages
Preference given to those with financial modeling experience

About Autodesk Construction Solutions
Autodesk has fully reimagined the construction business for the digital age, enabling companies to address the most important challenges they face today while preparing for new ways of working in the future. The Autodesk Construction Solutions (ACS) portfolio connects the office, trailer and field so customers can move seamlessly through each phase of a building’s lifecycle – from design and preconstruction to construction, turnover and operations – with best-in-class solutions that include Assemble Systems, BIM 360, Building Connected and PlanGrid. General contractors, subcontractors, and owners around the world rely on ACS to win more work, enhance collaboration, speed decision-making, reduce risk, and improve overall project outcomes.

#ACSCareers

At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law.

Are you an existing contractor or consultant with Autodesk? Please search for open jobs and apply internally (not on this external site). If you have any questions or require support, contact Autodesk Careers .",4.0,"Autodesk
4.0","San Francisco, CA",-1,5001 to 10000 Employees,1982,Company - Public,Computer Hardware & Software,Information Technology,$2 to $5 billion (USD),-1
Data Engineer,$69k-$128k (Glassdoor Est.),"ServiceChannel is the leading cloud-based service automation platform for facilities management. We offer a single platform to source, procure, manage and pay for repair and maintenance services from commercial contractors. We are a high growth, late stage startup with fantastic product-market fit, and trusted by more than 500 global brands like Cole Haan, Bloomin' Brands, CVS Health, Louis Vuitton, Allbirds, and Under Armour. We are committed to creating a great product for our customers and a great work environment for our employees to succeed professionally and personally, landing us on Wealthfront's Career-Launching Companies List for the second year in a row.

ServiceChannel Data Engineers are responsible for the design and implementation of enterprise data solutions across the organization using proven database architecture, modeling, engineering techniques and sound database principles. Prospective candidates must have extensive experience in developing data integration processes for enterprise data enablement and management systems. They must understand information and system development lifecycles. Deep knowledge of database, data warehouse, and various data management focus areas as specified in DMBOK (refer to DAMA – Data Management Association) Framework (or equivalent disciplines) are required. The candidate will work on enterprise data analytics, data mining and data science initiatives and other integration projects executing ServiceChannel's analytics strategy and implementation roadmap.

Key Responsibilities
Data Discovery - analyzing data values and data patterns to identify the relationships that link disparate data elements into logical units of information, or ""business objects"" (such as customer (subscriber), service provider, work orders, etc.). Identify the transformation rules that have been applied to a source system to populate a target such as transactional entities, operational data store or data warehouse. Create and maintain technical documentation
Data Architecture and Modeling - Create conceptual, logical and physical data models. Define data attributes, including domain constraints and privacy attributes. Discover, explore, and visualize the structure of data sources. Discover or identify relationships between disparate data sources. Compare and normalize the structures of various data sources to address data analytics, data mining, data warehouse and reporting requirements. Perform peer code review when assigned
Data Engineering and Data Warehouse – Build extensible data acquisition and integration solutions to meet the functional and non-functional requirements of the business. Implement processes and logic to extract, transform, and distribute data across one or more data stores from a wide variety of sources. Optimize data integration platform to provide optimal performance under increasing data volumes. Participate in research and development of recommendations regarding database components, including hardware, database systems, ETL software, metadata management tools and database design solutions.
Data Governance/Stewardship – Ensures the quality, completeness and accuracy of data definitions within developed codes. Identifies and manages the resolution of data quality and data security issues, such as uniqueness, integrity, accuracy, consistency, privacy and completeness in a cost-effective and timely fashion. Identify procedures for disaster recovery and data archiving to ensure effective protection and integrity of data assets.
Data Quality - Ensure the stability, integrity and efficiency of data access and data quality across the organization via ongoing database support and maintenance. Ensure data reconciliation unit testing procedures that will prove accuracy and correctness of data being processed.
Database Architecture, Administration and Development - Work with database development staff and DBAs to develop database architectures, coding standards, and quality assurance policies and procedures. Participate in testing and implementing database design and functionality, and tuning for performance.
Metadata - support information governance by providing reporting and traceability on data movement, modeling and business intelligence applications, as required by regulatory requirements. Analyze and view the impact of changes to the current information model, avoiding potentially disruptive modifications to existing processes.
Customer Implementation and Production Support – Assist Customer Implementation and Customer Success teams as Data SME during troubleshooting issues.
Required Skills & Experience
Bachelor's Degree in Computer Science, related field or equivalent experience (including completed courses in relevant areas such as computer science, computer languages, etc.
Strong understanding of database structures, theories, principles and practices, and hands-on experience with business requirements gathering, data analysis and data modeling
Strong experience managing/maintaining database schema in source code control
3+ years of experience related work experience which deals with following data domain - customer, service provider, service orders, invoices and other related attributes.
3+ years of experience in analyzing and developing data requirements and data specifications; hands-on experience in documenting understanding and analysis of databases and data models.
3+ years of experience in creating, mapping and documenting data within organization (source, targets, transformation rules, etc.); Experience with any off the shelf ETL technology to build mappings to extract and transform data. Understanding of ETL technology architecture; Understanding of ETL Repository. Worked on transformations to cleanse, format, join, aggregate and route data to appropriate targets. Perform error handling/trapping using ETL mappings or corresponding techniques. Experience using Workflow management to build and run workflows; Experience with performance tuning ETL mappings and workflow; Designed error handling strategies for use in a workflow. Worked with ETL logs and debugger for troubleshooting
4+ years of hands-on experience in writing SQL for querying databases. Ability to extract information from databases using complex query statements and advanced database tools.
Ability to work independently while working on multiple projects
Strong problem-solving and communication skills
Strong analytical, prioritizing, interpersonal, problem-solving, presentation, project management (from conception to completion) & planning skills
Demonstrated collaborative skills and ability to work well within a team
Ability to work in a fast-paced and deadline-oriented environment
Self-motivated with critical attention to detail, deadlines and reporting. Able to successfully work on problems of limited to diverse scope involving task specific or cross-functional teams.
Ability to analyze business processes, best practices, data lifecycles, and standard operating procedures
Preferred Skills & Experience

Facility management data domain knowledge/experience
Experience in cloud ETL and near-time data integration technology
3+ years of hands-on experience in logical data modeling
2+ years hands-on technical experience with designing, building, installing, configuring and supporting database servers including database tuning and troubleshooting experience
Advanced ETL techniques: Understanding structure and use of parameters. Experience using Lookup transformation. Applied memory optimization techniques
Worked with various relational and MPP database platforms
Experience with version control tools & release management
Applied experience in secure file transfer protocol (sFTP), web services integrations, Salesforce, and JIRA
Why Work for Us?

Our work environment is dynamic, collaborative, and entrepreneurial - We are firm believers in working to live and not living to work. Don't get us wrong, we work hard and do what it takes to get the job done, but we value a healthy work/life balance and have a lot of fun along the way. Our company is filled with foodies, music lovers, travelers and sports nuts. We are looking for energetic, positive, creative problem solvers who don't mind digging in and getting their proverbial hands dirty.

In addition to striving to create the best possible environment for our ServiceChannel team members, we also support them outside of work as well through generous medical, dental, and vision insurance, life insurance, 401k with match, a flexible time-off policy, and paid parental leave. Our greatest benefit, however, is our amazing ServiceChannel team!",3.5,"ServiceChannel
3.5","Pleasanton, CA",-1,201 to 500 Employees,1999,Company - Private,Enterprise Software & Network Solutions,Information Technology,$50 to $100 million (USD),-1
"Data Engineer, Analytics (Instagram Well Being)",$118k-$192k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Our more experienced Data Engineers are clearly characterized by in-depth technical experience and proven progression in leadership responsibility. If you have an interest in being responsible for the dynamics of a fast-paced environment, this is the right role for you. You will be working on many projects at a time, but also focused on the details while finding creative ways to pursue big picture challenges. You will leverage not just technical skills, but strong emphasis on program management, technical leadership, and communication. In this role, you will be responsible for the data for Instagram Trust & Privacy within the Well Being team. You will work closely with the product team including counterparts in data science, engineering, product, and others to support delivering comprehensive, accurate, and data artifacts specifically related to Privacy & product security. You will be responsible for thinking holistically about product & data privacy on Instagram and implementing a roadmap defining the data architecture, ownership model, pipelines and visualization to drive understanding.
Define and own the data engineering roadmap for Instagram Trust Pillar
Build product-focused datasets and scalable, fault-tolerant pipelines
Build data anomaly detection, data quality checks, and optimize pipelines for ideal compute and storage
Collaborate with Software Engineers and Data Scientists to design technical specification for logging and add logging to production code to generate metrics both online as well as offline
Work with different cross functional partners partners - Data Scientists, Infra Engineering, Logging Framework Infra Teams, Product Managers, Privacy
Build visualizations to provide insights into the data & metrics generated
Work with data infrastructure teams to suggest improvements and influence their roadmap
Able to immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions
Recommend improvements and modifications to existing data and ETL pipelines
Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership
Drive internal process improvements and automating manual processes for data quality and SLA management
Provide ongoing proactive communication and collaboration throughout the organization
Actively mentor team members in their careers
4+ years experience in the data warehouse space
4+ years experience working with either a MapReduce or an MPP system
7+ years experience in writing complex SQL and ETL processes
4+ years experience with object-oriented programming languages
7+ years experience with schema design and dimensional data modeling
BS/BA in Technical Field, Computer Science or Mathematics
Knowledge in Python or Java
Experience analyzing data to identify deliverables, gaps, and inconsistencies
Experience effectively collaborating and communicating complex technical concepts to a broad variety of audiences the data architecture, pipelines, visualization and anomaly detection to drive understanding
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","San Francisco, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Machine Learning Data Engineer,-1,"Labelbox’s mission is to build the best products for humans to advance artificial intelligence. As a ML Data Specialist, you will work across engineering, customer, and data labeling teams to create highly accurate datasets. This is a unique, cross functional position where you will act as a project/program manager ensuring successful data labeling and machine learning outcomes for customers. You are a creative problem solver with attention to detail.

Current Labelbox customers include American Family Insurance, Lytx, Airbus, Genius Sports, Keeptruckin and more. Labelbox is venture backed by Andreessen Horowitz, Gradient Ventures (Google’s AI-focused venture fund), Kleiner Perkins and First Round Capital and has been featured in Tech Crunch, Web Summit and Forbes.
Responsibilities
Build the world’s first data warehouse solution tailored towards increasing ML development velocity.
Help ML engineers around the world make sense of their model performance.
Contribute to a team culture obsessed with MLOps.
Design and optimize ETL pipelines + ML experimentation tooling
Deliver on large scale features that impact customers using data-hungry ML in production
Build a low-debt future for machine learning infrastructure by adhering to MLOps principles
Contribute enhancements to an API used by numerous ML teams
Key Attributes
Ability to thrive in a dynamic, fast paced startup environment
Enjoys the intricacies of big data
Basic Qualifications
Bachelors or Masters in CS preferred or equivalent experience
Excellent developer with experience building production-scale data pipelines
Clear history of delivering on data warehousing solutions
Nice to Have
Intimate experience with MLOps principles (Terraform, CI/CD, experiment management, data management)
Intimate experience with ETL pipelines (Dataflow, Spark, Beam, Airflow, etc.)
Intimate experience with data warehousing design (BigQuery, Cassandra, etc.)",3.9,"Labelbox
3.9","San Francisco, CA",-1,1 to 50 Employees,2018,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,$81k-$155k (Glassdoor Est.),"Data Engineer

_And who better than you to join the Trianz family?_

_OR Join the Trianz family for an exciting career in Digital Transformation_

At Trianz, we offer you an open and learning-oriented culture essential to emerge as a leader. Completely focused on the Digital Evolution philosophy and phenomenon, we view delivering our value proposition consistently as a non-negotiable commitment. Our enablers include Intelligent Team Formations, a Client-Centric Approach, Predictability in Execution, and establishing a Unique Relationship Experience. A culture of innovation, encouraging our people to create, and belief in the importance of training and development set us apart.

We are looking forward to see you bring the following to the table to craft your, and our, success story:

What you get to do in this role:

We are looking for a Data Engineer to not only build data pipelines but also extend the next generation of our data tools. As a Data Engineer, you will develop a clear sense of connection with our organization and leadership - as Data Engineering is the eyes through which they see the product.

This is a partnership-heavy role. As a member of Infrastructure Strategy Data Engineering, you will belong to a centralized Data Science/Data Engineering team who partners closely with teams in Infrastructure organization. Through the consulting-nature of our team, you will contribute to a variety of projects and technologies, depending on partner needs. Projects include analytics, ML modeling, tooling, services, and more. The broad range of partners equates to a broad range of projects and deliverables: ML Models, datasets, measurements, services, tools and process.

Data Engineer Responsibilities
Partner with leadership, engineers, program managers and data scientists to understand data needs.
Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.
Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.
Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.
Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.
Build data expertise and own data quality for your areas.
Minimum Qualifications
5+ years of Python development experience.
5+ years of SQL experience.
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
3+ years experience with Data Modeling.
Experience analyzing data to discover opportunities and address gaps.
5+ years experience in custom ETL design, implementation and maintenance.
Experience working with cloud or on-prem Big Data/MPP analytics platform(i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
Preferred Qualifications
Experience with more than one coding language.
Experience with designing and implementing real-time pipelines.
Experience with data quality and validation.
Experience with SQL performance tuning and E2E process optimization.
Experience with anomaly/outlier detection.
Experience with notebook-based Data Science workflow.
Experience with Airflow.
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.
_Need more details?_*
Trianz is growing at a faster pace than the industry for the last five years. Read through some of the key industry recognitions we have received for our innovative execution and strategic client initiatives here.
About Trianz*
Trianz* simplifies digital evolutions through effective strategies and excellence in execution. Collaborating with business and technology leaders, we help formulate and execute operational strategies to achieve intended business outcomes by bringing the best of consulting, technology experiences and execution models. Powered by knowledge, research, and perspectives, we enable clients to transition to a digital enterprise by leveraging Cloud, Analytics, Digital, Infrastructure and Security paradigms. With offices in Silicon Valley, Washington DC Metro, Rosemont, Chicago, Austin, Boston, Denver, Irvine, Raleigh, San Francisco, Seattle, New York, Dubai, Bengaluru, Hyderabad and Chennai, we serve Fortune 1000 and emerging organizations across industries globally. For more information, visit *www.trianz.com*.
Trianz is an Equal Opportunity Employer and does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law).

Job Type: Full-time

Benefits:
401(k)
401(k) Matching
Dental Insurance
Disability Insurance
Employee Assistance Program
Flexible Schedule
Flexible Spending Account
Health Insurance
Life Insurance
Paid Time Off
Parental Leave
Professional Development Assistance
Referral Program
Relocation Assistance
Retirement Plan
Tuition Reimbursement
Vision Insurance
Schedule:
Monday to Friday
Visa Sponsorship Potentially Available:
No: Not providing sponsorship for this job
Company's website:
www.trianz.com
Work Remotely:
Temporarily due to COVID-19",2.5,"Trianz
2.5","Santa Clara, CA",-1,1001 to 5000 Employees,2001,Company - Private,IT Services,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Core Automation Services",$84k-$157k (Glassdoor Est.),"Role Data is deeply embedded in the product and engineering culture at Tesla. We rely on data – lots of it – to improve manufacturing, to optimize hardware designs, to proactively detect faults, and to optimize load on the electrical grid. We collect data from each of our cars, superchargers, and stationary batteries and use it to make these products better and our customers safer. We're a small but fast-growing team which is building and operating the new data pipelines in the manufacturing environment at Tesla. We collect massive amounts of IoT data, provide storage, access, and high-volume processing. Our stack is built on top of open source technologies, including Spark, Kafka, and related. We're looking for a talented engineer to join us as a foundational member of the team to deliver new and improved big data services and infrastructure. Your work will affect many hundreds of Tesla engineers daily, as well as improving the functionality of our cars, chargers, and batteries worldwide. Responsibilities · Employ and improve industry-leading, scalable, distributed open-source technologies · Build back-end systems from scratch that are capable of handling trillion+ events per day · Facilitate operation of highly-available distributed systems at scale and across multiple locations · Facilitate others in deploying, operating, and extending upon your clean, tested code · Help define a platform that is highly leveraged, multi-tenant, and self-serviced · Work with data engineers and data scientists to drive efficient solutions from the platform · Help define the data story and enable data-driven solutions at Tesla, both technically and culturally Requirements · Strong programming fundamentals, particularly in data structures, concurrency, Go, Python, and Java · Deep understanding of a complex distributed system, such as Kafka, Spark, HBase, ElasticSearch · Have built and optimized highly available, scalable, distributed back-end services · Ability to break down and deeply understand complex problems and communicate complex matters efficiently · Strong problem solving skills, optimizing for the simplest, most robust yet practical solutions · Reliable, dependable, trustworthy, participating team member · Smart but humble, with a bias for action Tesla participates in the E-Verify Program",3.5,"Tesla
3.5","Fremont, CA",-1,10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$2 to $5 billion (USD),-1
Data Engineer,$91k-$167k (Glassdoor Est.),"Who we are: Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 286 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies. Job Description Summary: Global Product Data Services is a newly formed team in PayPal’s product and engineering organization under Customer Experiences and Technology. Its vision is to “Provide Enterprise Product Data at lower cost and better quality”. To achieve this vision, we are looking for people with a passion and curiosity to solve customer problems with data. The internal stakeholders for the Program include but are not limited to PayPal Inc.’s Product Teams, Finance, Risk, Compliance and Strategy Teams. Our External stakeholders include Customers and Regulators amongst others. This position is focused on delivering Core Data solutions using modern technology to serve the various needs of the business. The scope of the organization is global, and its data platforms serve a wide array of functions including Merchant, Partner, Operations, and Compliance business operations. At GPDS, we are committed to bringing innovation, passion and customer focus to the business of enterprise solutions. One of the charters of GPDS is to deliver on data driven company wide transformational initiatives to integrate PayPal Inc. data seamlessly using Big Data for both operational and analytical needs. In this position you will also have the opportunity to work with stakeholders and users to understand their needs and partner with engineering to deliver the solution. This position requires an individual who is comfortable working in cross-functional teams with a very high degree of analytical and technical skills. Job Description: In this role, the individual will be part of the engineering team in Global Product Data Services Organization and will be responsible for. Participating and collaborating with Product Owner/ Cross functional teams in the organization to understand the business requirements and to deliver solutions that can scale. Creativity and out of the box thinking is required. Proactively anticipating problems and keeping the team and management informed in a timely manner. Being flexible and being able to support all functions of product life cycle when required. Ability to use data to draw insights or drive decisions. Be able to explain complex technical concepts to management, product managers and other engineers. Motivated and interested in delivering results, especially in the area of writing high-performance, reliable and maintainable code. Ability to adapt to new development environments, changing business requirements and learning new systems highly desired. Good team player, able to effectively work across multiple teams on solutions that have complex dependencies and requirements in a fast-paced environment Excellent verbal and written communication skills. Skills and Experience 8+ years of experience in the IT industry, experience in data engg is Mandatory. Shell/ Perl scripting experience or proficiency in any programming language like Java/C/ C++ Hands on in Java programming Proficient in Frameworks – Spring, Maven, Hibernate Knowledge Of Real time Analytics Strong fundamentals of object-oriented design, data structures, algorithms and design patterns Expert in software engineering tools and best practices Expert in design/implementation for reliability, availability, scalability and performance Should have strong SQL programming skills Knowledge of data warehousing concepts Proficient in Big data Environments – Pig,Hive,MR Excellent written and oral communication skills Working experience in an Agile methodology is highly preferred. Experience with Tableau or other visualization tools is a plus. Knowledge of Scheduling Tools is a plus Intermediate level knowledge on following technologies, with expertise on few of them: Knowledge in MPP Databases/ Distributed systems Knowledge on Data Encryption Standards is a huge plus Exposure to Data Quality and Profiling tools is a plus. Exposure BI tools desired, but not required (Micro strategy, Business Objects) Basic level knowledge on following business domains is a plus: Payments and banking We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom. PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities.",4.0,"PayPal
4.0","San Jose, CA",-1,10000+ Employees,1998,Company - Public,Internet,Information Technology,$10+ billion (USD),-1
Sr Data Engineer #6580,$157k-$270k (Glassdoor Est.),"Company Overview

Fanatics is the global leader in licensed sports merchandise and changing the way fans purchase their favorite team apparel and jerseys. Through an innovative, tech-infused approach to making and selling fan gear in today's on-demand culture, Fanatics operates more than 300 online and offline stores, including the e-commerce business for all major professional sports leagues (NFL, MLB, NBA, NHL, NASCAR, MLS, PGA), major media brands (NBC Sports, CBS Sports, FOX Sports) and more than 200 collegiate and professional team properties, which include several of the biggest global soccer clubs (Manchester United, Real Madrid, Chelsea, Manchester City). Fanatics offers the largest collection of timeless and timely merchandise whether shopping online, on your phone, in stores, in stadiums or on-site at the world's biggest sporting events.

About the Team

Fanatics is first and foremost a technology company. We are powered by cutting-edge tech created by our small agile teams using the latest tools and technologies under our highly analytical, forward thinking, and open-minded leadership. As the global leader in licensed sports merchandise, we challenge ourselves by improving our new fully responsive NodeJS cloud commerce platform, Elasticsearch engine, and deep data science capabilities while building the best-in-class retail manufacturing and supply chain technologies. Our tech teams work together to revolutionize data science and engineering initiatives, provide highly scalable real-time and streaming platforms, and create secure e-commerce and in-stadium fan experience products. Our own e-commerce platform transacts in over 190 countries, 17 languages, and 14 currencies. Our motto is “#GSD”—get stuff done—and we do just that. If you want to be at the nexus of sports, commerce, and technology, come be a part of our industry-leading team here at Fanatics Tech.

Building data pipelines to manage marketing reports and optimize marketing spend.

Primary Responsibilities include:

•Design and deliver highly scalable multi-tiered distributed software applications.
•Coding. Living and Breathing awesome Code.
•Be part of an agile team and sharing responsibilities of testing and development as the role demands.
•Strong quality focus including automation, design reviews, and unit testing.
•Strong analytical, troubleshooting, and problem solving skills with orientation towards innovation and open source tools.
•Mentor and guide developers in coding, design methodology and design patterns.
•Conceptualizing, coding, deploying, and iterating on next generation prototypes.
•Flexible approach to analyzing technical issues and clearly communicating recommendations/solutions.
•Cross team development with Product managers, Project managers, engineers, and QA to deploy innovative solutions to meet business unit requirements
Required Skills
•An experienced Data engineer with Spark, Hive and Hadoop technologies
•Strong track record of excellence, and of delivering high quality innovative software!
•Outstanding coding skills in Java ,Scala and python
•Ability to trace the data transformations and tune them appropriately to reduce the time and cost of execution.
•Building distributed back-end systems using Java and related technologies is a plus
•Exposure to Data Science and experience integrating with science models

•Expertise in SQL technologies and RDBMS (Oracle, MySQL)
•Experience in noSQL technologies (MongoDB, CouchDB, Redis) is a plus
•Strong understanding & usage of algorithms and data structures in your designs
•Self-motivated, passionate for technology, and strong driver for results and continual improvement
•Team player - work well independently and in multi-group cross-discipline environments
•Flexible, adaptable, and able to autonomously manage multiple tasks in a dynamic, fast-paced environment.
•Strong skills for verbal & written communication targeting technical and non-technical audiences

Tryouts are open at Fanatics! Our team is passionate, talented, unified, and charged with creating the fan experience of tomorrow. The ball is in your court now.



NOTICE TO CALIFORNIA RESIDENTS/APPLICANTS: In connection with your application, we collect information that identifies, reasonably relates to or describes you (“Personal Information”). The categories of Personal Information that we collect include your name, government issued identification number(s), email address, mailing address, other contact information, emergency contact information, employment history, educational history, criminal record, and demographic information. We collect and use those categories of Personal Information about you for human resources and other business management purposes, including identifying and evaluating you as a candidate for potential or future employment or future contract positions, recordkeeping in relation to recruiting and hiring, conducting criminal background checks as permitted by law, conducting analytics, and ensuring compliance with applicable legal requirements and Company policies.",3.4,"Fanatics
3.4","San Mateo, CA",-1,5001 to 10000 Employees,1996,Company - Private,Sporting Goods Stores,Retail,$1 to $2 billion (USD),-1
Senior Data Engineer,$158k-$279k (Glassdoor Est.),"WePay’s API’s provide a flexible and robust payments system that seamlessly integrates into the software platforms of our customers while shielding them from fraud, so they can deliver the end-to-end user experiences they want without taking on the overhead they don’t want.

We are looking for an exceptional Senior Data Engineer to join our high-performing BI & Analytics team. Your work will focus on developing the data platform to support WePay’s analytics and data science capabilities and making data accessible to the entire company for enhanced data driven-decision making. You will contribute significantly to the vision of our analytics and data environment by using the cutting-edge cloud and big data technologies.

Please use a personal email such as gmail, yahoo, etc when applying to the role.
What You Will Do
Design and build analytics data marts and data warehouse for high-performance analytics, and provide ongoing optimization and enhancements
Partner with internal stakeholders to understand business requirements and develop data solutions to address their needs
Work with engineering team to ensure a scalable and streamlined process to extract data from the production environment into the analytics warehouse
Develop reporting and visualization tools using in-house tools as well as third-party tools
Define performance metrics and develop dashboards to evaluate business performance
Conduct ad hoc analyses to support key corporate initiatives
What We Are Looking For
Bachelor’s or Master’s degree in computer science or relevant technical field
MUST have 4+ years of industry experience as a Data Engineer, Business Intelligence Engineer or Data Scientist
4+ years experience in Python or Java
4+ years of experience in MySQL, NoSQL, BigQuery
2+ years experience in some of the following: Hadoop, Airflow, Kafka, Spark
Experienced with Tableau, Chartio or other BI tools is preferred
Fast-learner; ability to adapt to new development environments
Detailed-oriented with strong organizational skills
Creative problem solvers with can-do attitude
Excellent communication skills
About WePay

WePay’s mission is to make commerce seamless. Our products help software companies integrate payments into their applications – thereby empowering small businesses and individuals to get paid easily and quickly using their go-to apps and software. Our customers include BigCommerce, TouchBistro, Meetup and Freshbooks, just to name a few. By joining forces with JPMorgan Chase, a global financial services firm with over $2.5 trillion in assets that serves millions of customers worldwide, WePay is now able to connect our customers seamlessly into a range of banking services beyond payments. WePay is a unique place to work and offers the best of both worlds. WePay has a FinTech startup culture that emphasizes transparency, collaboration and career growth, with the ability to work on small, nimble teams. However, now combined with the power of JPMorgan Chase, employees are also able to create change at scale and have an opportunity to truly disrupt and shape FinTech.

You can find more information at wepay.com

To all recruitment agencies: WePay does not accept agency resumes. Please do not forward resumes to our jobs alias, WePay employees, or any other company location. WePay is not responsible for any fees related to unsolicited resumes.",3.7,"wepay
3.7","Redwood City, CA",-1,201 to 500 Employees,2008,Subsidiary or Business Segment,Financial Transaction Processing,Finance,Unknown / Non-Applicable,-1
Data Engineer II,$85k-$159k (Glassdoor Est.),"Power the Possibilities
The CDK Global technology team is looking for collaborative innovators who are passionate about making their mark on emerging enterprise software products. Were building and developing cloud technology for the automotive retail industry
that will change the landscape for automotive dealers, original equipment manufacturers (OEMs) and the customers they serve.

Be Part of Something Bigger
Each year, more than three percent of the U.S. gross domestic product (GDP) is attributed to the auto industry, which flows through our customer, the auto dealer. Its time you joined an evolving marketplace where research and development
investment is measured in the tens of billions. Its time you were a part of something bigger.

Were expanding our workforce engineers, architects, developers and more onboarding early adopters who can optimize, pivot and keep pace with ever-evolving development roadmaps and applications.

Join Our Team
Growth potential, flexibility and material impact on the success and quality of a next-gen, enterprise software product make CDK an excellent choice for those who thrive in challenging, fast-paced engineering environments.
The possibilities for impact are endless. We have exceptional opportunities to evolve our industry by driving change through new technology.

If youre ready for high-impact, youre ready for CDK.

Summary:

The most important thing to us about you is that you have a passion for working on cool stuff and can work well with cool people. We love the energy shown in your projects (and those side projects you do, just for you) and we love that you can get in a room with amazing developers and learn and teach and contribute and grow. Our Agile, collaborative approach is important to everyone here.

If you have worked on bringing a large software product to market, or have a desire to gain this experience, this role might be perfect for you.

We focus on building quality software in an agile and results-oriented environment.

Specific responsibilities include:
Evolve existing framework to support new scalability requirements as well as new functionality needed.
Work with the team to drive big data solutions.
Work with product owners to identify and iron out upcoming business needs and develop technical backlog to answer those needs in a timely manner.
Experience:

Qualified candidates will generally have 3+ years of software development experience, including:
5+ years of software development experience or Masters + 2yrs in CS.
Experience programming in Python, Java, writing ETL applications
Experience with Amazon AWS services such as S3, Lambda, StepFunctions
Knowledge in SQL and understanding relational database
Experience designing robust scalable applications
Experience in application design and implementation using agile practices & TDD
Experience with highly scalable / distributed systems desired
Experience with Kafka, Hadoop, and NoSQL datastore is a plus
Enthusiasm for solving interesting and complicated problems.
Education:
A BS or MS in Computer Science or equivalent education/experience
CDK Global knows you have passions outside of work. You have family, friends, sporting events, and lots of things going on. Thats why we offer a comprehensive benefits package to not only take care of you but your family as well. All of our benefits are effective the first day of employment including 401K matching, paid time off to re-energize, donate your time to volunteer in your community, and tuition reimbursement to name a few.

At CDK, we pride ourselves on having a diverse workforce. We value and celebrate the uniqueness of individuals and the different perspectives they provide. We offer equal opportunity employment regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, or protected veteran status.",3.1,"CDK Global
3.1","San Jose, CA",-1,5001 to 10000 Employees,2014,Company - Public,Computer Hardware & Software,Information Technology,$2 to $5 billion (USD),-1
Data Engineer,$75k-$136k (Glassdoor Est.),"About Us Named ""The Fastest Growing Retailer in the US"", Dolls Kill is a fashion brand that boldly empowers young people to celebrate their individuality. With over two million followers on Instagram, we've become a cult favorite among DJs, celebrities and artists around the world. Based in San Francisco, we have an amazing team of retail leaders, scrappy hackers, creative marketers and fashion trendsetters who are fun to work with and serious about what they do. We're also backed strong consumer-focused investors who share in our vision of building one of the world's next great consumer brands. Look for our latest press mentions in Business Insider, Racked, TeenVogue, PopSugar, Elle, and more. About the Role Dolls Kill is looking for a proven Data Engineer to help build out, improve and maintain all sorts of data connections and ELT/ETL processes. In this important role you’ll own important data pipelines that enable real-time decision making and automated customer experiences. Responsibilities Develop automated data pipelines for the extraction, preparation, and ingestion of data Work with APIs and setup Webhooks to ingest live data from various partner applications Ensure data quality and consistency; play an active role in establishing data governance around company KPIs Create and maintain technical documentation Work closely with analytics team to design, build & test end-to-end solutions Requirements BS in Computer Science, Information Systems, Mathematics or a related field 2+ years experience creating and maintaining automated data pipelines and ETLs Experience working with large data sets Experience with AWS Ability to quickly learn new technologies and business processes Strong analytical skills to determine effective approaches to business questions Experience working with BI reporting tools such as Looker, Tableau or Microstrategy Advanced level of proficiency in SQL development Brief cover letter required",3.6,"Dolls Kill
3.6","San Francisco, CA",-1,201 to 500 Employees,2011,Company - Private,Other Retail Stores,Retail,$1 to $5 million (USD),-1
Senior Data Engineer,$126k-$228k (Glassdoor Est.),"About us: Grand Rounds is a new kind of healthcare company. Founded in 2011, the company is on a mission to raise the standard of healthcare for everyone, everywhere. The Grand Rounds team goes above and beyond to connect and guide people to the highest quality healthcare available for themselves and their loved ones. Grand Rounds creates products and services that give people the best possible healthcare experience. Named a 2019 Best Place to Work by Glassdoor and Rock Health’s 2018 Fastest Growing Company, Grand Rounds works with inspiring employers and doctors to empower them to be the change agents we need to make our shared vision a reality. The Role: What’s the point of warehousing valuable data if your analysts and researchers can’t navigate its complexities? At Grand Rounds we’re focused on reducing the cognitive load of navigating complex, rich, valuable healthcare data. We have a two-pronged strategy: one, design a rational, intuitive data model; two, build a platform that allows analysts and researchers discover insights painlessly. This role will have ownership of the second prong of that strategy. We’re hiring an engineer to be one of two founding members of the insights platform team. In this role, you’ll be tasked with establishing the technical vision, driving the roadmap (one year and beyond), and doing the heavy lifting of building for immediate value and balancing prioritization decisions of moving fast with building to last. Success in this role means not only building the platform that creates a seamless and painless experience using Grand Rounds’ data, but shifting the culture of the organization to further embrace data-driven decision making in product roadmapping. Responsibilities: Work with users to deeply understand how they will use data, and pair closely with them to drive the business to excellent solutions. Deeply understand analyst and researcher workflows Prioritize the team’s work to deliver business impact Design the platform that navigates our modeled healthcare data (using HL7’s FHIR and projections from FHIR) Collaborate with our data infrastructure team to make opinionated recommendations to influence their roadmap and deliver more quickly on yours Build APIs and services Integrate open source, commercial and proprietary systems and services Qualifications 5+ years of full-stack software development Strong design and programming skills (spanning JVM, scripting, SQL, and frontend) Hands-on experience working in distributed data environments (Kafka, Presto, Spark) Experience with relational databases and data modeling Experience with data visualization platforms (Tableau, Looker) Working knowledge of data lifecycle and concepts like lineage, governance, privacy, security, retention, de-identification, etc. Experience with AWS Knowledge or experience in search databases (ElasticSearch) You’re not expected to have expertise in all of these areas! We have growth mindsets and expect you to as well! - Grand Rounds is an Equal Opportunity Employer and considers applicants for employment without regard to race, color, religion, sex, orientation, national origin, age, disability, genetics or any other basis forbidden under federal, state, or local law. Grand Rounds considers all qualified applicants in accordance with the San Francisco Fair Chance Ordinance.",4.2,"Grand Rounds
4.2","San Francisco, CA",-1,501 to 1000 Employees,2011,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Data Engineer,$94k-$174k (Glassdoor Est.),"6+ Years of Industry Work Experience
Experience extracting data from a variety of sources, and a desire to expand those skills (working knowledge in SQL and Hive is mandatory)
Need to have a good understanding of TeraData and Hadoop SQL with focus on migration from TeraData to Hadoop, hands on experience in Google Cloud Platform
Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark.
Experience handling Unix systems, for optimal usage to host enterprise web applications.
Excellent Communication Skills to Understand and Pass on Requirements.
Ability to work on standalone tasks without much guidance",3.7,"Altimetrik Corp
3.7","Santa Clara, CA",-1,1001 to 5000 Employees,2012,Company - Private,IT Services,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,-1,"About the Opportunity: Our data engineering team is at an exciting juncture where they are developing a data ecosystem that will work dynamically with hospital systems, research institutes and other cancer data systems. The data engineering team will be a partner to the data science team ensuring the data that is collected is curated for them to create the best predictive models in the cancer care industry. As an early engineer to join the Data Platform Team, you will develop complex distributed streaming systems that ingest and process data arriving from and delivered to our partner healthcare systems. Additionally you will develop a Data Insights Platform, that will power ML workloads, in partnership with the Data Science team. You will learn and build healthcare knowledge graphs that will be central to improving outcomes for cancer patients. We are looking for Sr. Data Engineers to join our dynamic team and bring their passion for data engineering with them! What You Will Do: Integrate with healthcare APIs such as HL7 & FHIR to build ELT/ETL pipelines that store, transform and aggregate data to power our data science workloads and fuel our provider- and patient-facing applications. Define the core data models abstracted from the Electronic Health Records (EHR) Assess the existing data ecosystem; leverage existing APIs & build new ones to consume structured and unstructured data sets. Partner, develop and own the cloud-based data management platform that will define and change the way we work with data. Be an active participant in: Architecture reviews, code reviews, general agile processes, the development and mentorship of engineers, and building a strong engineering culture while advocating practical sophistication. What We're Looking For: 5+ years of big data and software engineering experience Solid expertise and skills in object-oriented and functional programming paradigms and implementations using Scala, Python, Elixir or similar language. Microsoft Azure, Amazon Web Services, Google Cloud Platform or similar cloud platform architecture experience involving analytic systems. Hands-on experience with distributed data technologies: Kafka, Spark, Flink, etc. Experience with analytics applications using one or more of the database technologies, such as OLTP, OLAP, key-value, document-oriented, and graph. Experience building reliable, performant streaming & batch processing data pipelines. Strong communication skills, both written and verbal. Project Rōnin is filling one of the greatest gaps in patient care to date. Our mission is to increase the length and quality of every persons’ life who has been affected by cancer. This year alone there will be nearly 1.8 million new cancer diagnoses, and the tools oncologists are using haven’t been updated in 20 years. With our platform, communication between patients and their doctors is optimized through an individual care platform that is already having an immediate impact. Want to learn more? Linkedin Medium Avoid the confidence gap. If some of the above describes you, we’d love to chat and see where your skills can add to our team. We are an equal opportunity employer and value diversity in our company. We do not discriminate on the basis of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status.",4.9,"Project Ronin
4.9","San Mateo, CA",-1,1 to 50 Employees,2018,Company - Private,Computer Hardware & Software,Information Technology,Unknown / Non-Applicable,-1
eCom Senior Data Engineer,$88k-$156k (Glassdoor Est.),"Auto req ID: 219152BR
Job Description


As other sectors have shifted to eCommerce-first business models in recent years, food & beverage has continued to rely predominantly on traditional brick & mortar models, but this is changing rapidly and a period of extraordinary disruption is now underway. New technologies are transforming every aspect of reaching consumers, from the rise of digital marketing and online grocery platforms, to the creation of supply chain tools that enable speedy at-home delivery.

To seize this opportunity and lead the food & beverage industry into its remarkable next chapter, PepsiCo – the international food & beverage powerhouse with annual net revenue exceeding $64 billion and beloved brands including Frito-Lay, Gatorade, Pepsi-Cola, Quaker, and Tropicana – is expanding its Global eCommerce Team. As it needs the greatest minds in data & analytics, software development, machine learning optimization, and next-generation supply chain. Although PepsiCo is a large multinational, the PepsiCo Global eCommerce Team prides itself on having the entrepreneurial, action-oriented culture of an exciting startup business. As part of our group, alongside Silicon Valley veterans, founders of successful startup companies, and food & beverage experts to address a wide variety of the fascinating technical challenges facing our industry.

Given PepsiCo’s incredible global reach – our foods and beverages are enjoyed more than one billion times a day in more than 200 countries and territories, and our value chain involves diverse partners ranging from farmers and food scientists to retailers and logistics specialists – the challenges we’re addressing are complex and the solutions will be deeply impactful. The goal of the PepsiCo Global eCommerce Team is to build the technological products and capabilities that will reinvent our industry and make us the #1 food & beverage business in eCommerce for decades to come.

Accountabilities:
Lead problems assessment of eCommerce challenges to lead the development and design of technology solutions across functions involving computer hardware and software
Lead technology project evaluations as well as proposal feasibility with the different eCommerce businesses
Apply theoretical expertise and innovation to create or apply new technologies to apply to the entire digital landscape
Act as a consultant to the broader business users, management, vendors, and technicians to determine technology needs and system requirements.
Build new technologies and algorithms to optimize any business process
Develop data set processes and projects requirements
Use large data sets to resolve major business and functional issues whisle improving data reliability, efficiency and quality
Optimize processes implementing new technology and automation across eCommerce businesses and eCommerce functions
Qualifications/Requirements
BS or MS degree in Computer Science or a related technical field
6+ years of Python or Java development experience, Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases
6+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or a MPP system on any size/scale
Experience with specific AWS technologies (such as Glue, S3, Redshift, EMR, and Kinesis) a plus
Experience writing production code for Python or JVM-based systems, but you know a few other languages and like the right tool for the job
Knowledge of machine-learning tools and techniques
Experience optimizing larger applications to increase speed, scalability, and extensibility
Proven self-starter who can move projects forward by filling in the gaps on Agile teams, from leading a design session to doing some test automation, to mentoring a teammate struggling with a new technology
Requires Department of Transportation (DOT) certification and successful Motor Vehicle Report (MVR) review during the pre-onboarding process
Relocation Eligible: Not Eligible for Relocation
Job Type: Regular

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity

Our Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901 - 4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance.

If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy

Please view our Pay Transparency Statement",3.8,"PepsiCo
3.8","San Francisco, CA",-1,10000+ Employees,1965,Company - Public,Food & Beverage Manufacturing,Manufacturing,$10+ billion (USD),-1
Data Engineer,-1,"About Us At Coalition, we bring together cyber tools, data, and deep security expertise to help customers solve cyber risk. We have over 25,000 customers, ranging from small and mid-sized businesses to Fortune 500 companies, and that number is growing fast. The Coalition engineering team is growing rapidly, and we are looking for Data Engineers to work with the various data sources utilized in our business. The ideal candidate would have experience working with data engineering, data analysis, and statistical modeling, in the development of a real-time service. They would have experience working with structured and unstructured data systems, optimizing performance across large, disparate data sets, and streaming data systems. They would be a proficient developer who can deliver production-quality implementations. This is a versatile role that will touch many aspects of data at Coalition, including network security data, actuarial data, and growth analytics data. Our core platform is written mostly in Python with some services in Java and Go. We prefer to use the right tool for the job and make pragmatic decisions about how to scale and decouple systems as we continue to grow. We’re looking for someone who can navigate a cloud environment (AWS) with many moving pieces and systems to help the team understand how they fit into the broader puzzle. Responsibilities Implement risk models for various insurance products Evaluate, recommend, and implement data pipelines for a variety of data sources used at Coalition Deliver production-quality software implementations for ETL and streaming pipelines Explore new data sources and develop insights into existing data sources that improve business efficiency Requirements 3+ years working with large disparate data sets Deep understanding of ETL pipelines, statistical modeling, data analytics, and large scale data streaming Expert-level knowledge of SQL, Python, R, or similar language used for data engineering A proven track record of successfully automating business value from data insights Experience with at least one big data search tool, such as Elastic Excellent oral and written communications skills at all levels Bachelor’s degree in Computer Science or a related field preferred Bonus Points Prior experience with insurance or network security technologies In-depth knowledge of AWS or other cloud-hosted platforms relevant to data engineering Experience with data visualization technologies Perks Enjoy a highly fulfilling, mission-driven culture Health, dental, and vision benefits for you and your family Life insurance and disability benefits Paid Parental Leave 401(k) plan Wellness and commuter benefits Flexible working hours Open vacation days We embrace distributed work; some benefits will vary by location You are an owner! We offer stock options to each of our employees More details at https://www.coalitioninc.com/careers Why Coalition? We are all here to build something we believe in and to make a company that will last. We’re also assembling a team of expert incident responders, threat and malware researchers, and security analysts to protect our customers before, during, and after a cyber incident. Our goal is to harness the power of technology with the safety of insurance, to provide the first holistic solution to cyber risk. Coalition's culture is one that strongly values humility, authenticity, and diversity. We want to work with people of different backgrounds and different paths in life, and we trust our team members to take responsibility, share ownership and work for one another. We are always looking for collaborative, inquisitive and dedicated individuals to join our team. Coalition Engineering Our culture is one of character, humility, responsibility, purpose, and authenticity. We are growing rapidly and that growth is enabled by strong teamwork, communication, and mentorship. We want people who are passionate about becoming experts in both the business and the technologies that support it. Our core platform is written mostly in Python with some services in Java and Go. We prefer to use the right tool for the job and make pragmatic decisions about how to scale and de-couple systems as we continue to grow. We’re looking for someone who can navigate a cloud environment (AWS) with many moving pieces and systems to help the team understand how they fit into the broader puzzle. Recent press releases: https://news.crunchbase.com/news/coalition-secures-90m-series-c-at-890m-valuation-to-grow-cyber-insurance-platform/ https://www.forbes.com/sites/amyfeldman/2020/05/28/next-billion-dollar-startups-2020/ Coalition is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.",3.9,"Coalition
3.9","San Francisco, CA",-1,201 to 500 Employees,-1,Unknown,-1,-1,Less than $1 million (USD),-1
Senior Data Engineer,-1,"Oura is an award-winning and fast-growing startup that helps people track all stages of sleep, activity, and physiologic functioning using the Oura Ring and connected app. We have a unique and high-quality set of biosignals that can be leveraged to help hundreds of thousands of people improve their sleep, monitor sickness, understand their bodies, and transform their health. We’re on a mission to empower every person to own their inner potential, and we’re seeking candidates who want to make an impact on our journey. As an experienced data engineer with Oura, you will lead the architecture of complex technical solutions that allow us to handle sensitive, health-relevant data from the ring, the app, and external research sources in a secure fashion. You will develop strategies to manage large-scale signal data, and provide the tools and environments that enable algorithm development at scale. You’ll work closely with our Data Scientists, Engineers, and Dev Ops specialists as a technical leader and resource. You will help investigate new technologies and select the right tool for the job, with an eye toward robust, scalable, and flexible architectures that facilitate teamwork and ensure high quality data. What You’ll Do with Us Lead the organization of the data to enable rapid access, security, and quality control needed to support algorithm development Architect strategies to manage data-heavy processes and optimize their throughput and latency throughout our platform. Explore and prototype the latest technologies to identify the best solutions Create data processing pipelines to automate processes, including quality assurance checks, entity resolution, exporting and importing of novel sources of data Scale our architecture and technology to match the growing needs of our customers and algorithm development team. Act as an expert technical resource in cross-functional interactions with Data Scientists, Researchers, and the Cloud Team Requirements What You’ll Bring to the Team 5+ years of extensive experience running, monitoring and debugging production systems at scale on AWS and utilizing a range of its services Expert knowledge of Python. Other languages, like Java and Scala, are a plus Good architectural understanding of event driven architectures, workflows, and database systems to help us scale along with our growing user base. Experience developing and maintaining applications running on Docker A solid understanding of security and an understanding of how to architect systems to handle sensitive health-related information Enjoy writing maintainable and well-tested code Have solid experience of standard software development tools and practices: version control (git), issue tracking, unit testing and agile development processes Have a pragmatic can-do attitude and delivery-focused mindset: you can handle tradeoffs between short-term goals and long-term tech debt Are happy both executing on your expertise as well as learning new skills Enjoys recruiting, mentoring, and motivating engineers Together, we will define Oura as a company where you can: Be productive and do your best work Be part of a world-class engineering team following and helping define best practices Be happy and have great work-life balance - we work efficiently, remove the fluff and pointless meetings, so that work continues to inspire and motivate you Grow on your chosen career path, be it technical, people, managerial or leadership skills, or any mix of those Benefits At Oura, we care about you and your wellbeing. Everyone here at Oura has a ring of their own and are continually looking to improve their health and add to our benefits! What we offer: Competitive salary and equity Health, dental, and vision insurance Wellness benefits Flexible working hours + work-life balance An Oura ring of your own Beautiful workspace in San Francisco by Pier 31 20 days of PTO Amazing culture of collaborative and passionate coworkers",4.4,"Oura
4.4","San Francisco, CA",-1,51 to 200 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Staff Data Engineer,-1,"Join SADA as a Staff Data Engineer! Your Mission As a Staff Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work, and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses. You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will be expected to tackle all technical challenges on whole projects and to mentor less experienced Data Engineers. You should be able to work independently, but should be a major participant in team reviews. You should be recognized as having technical mastery within the practice with an established reputation with Google and our customers. You should have demonstrable experience with public facing activities such as blogs, presentations, webinars, and OSS contributions. You will ensure the best architecture and engineering approach is applied. You will be expected to repeatedly deliver complex projects, and will be the owner of the complete customer outcome, including complex technical components of the engagement. You will actively participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects. Pathway to Success #BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs. Your success starts by positively impacting the direction of a fast growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions. As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks. Expectations Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted. Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives. Training - Ongoing with first week orientation at HQ followed by a 90 day onboarding schedule. Details of timeline can be shared. Job Requirements Required Credentials: Google Professional Data Engineer Certified or able to complete within the first 45 days of employment A secondary Google Cloud certification in any other specialization. Expert or Professional level certifications in either or both AWS and Azure. Required Qualifications: Mastery in at least 2-3 of the following domain areas: Big Data: managing Hadoop clusters (all included services), troubleshooting cluster operation issues, migrating Hadoop workloads, architecting solutions on Hadoop, experience with NoSQL data stores like Cassandra and HBase, building batch/streaming ETL pipelines with frameworks such as Spark, Spark Streaming and Apache Beam, and working with messaging systems like Pub/Sub, Kafka and RabbitMQ. Data warehouse modernization: building complete data warehouse solutions on BigQuery, including technical architectures, star/snowflake schema designs, query optimization, ETL/ELT pipelines and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive). Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for minimizing downtime. May involve conversion between relational and NoSQL data stores, or vice versa. Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. 5+ years of experience writing software in at least two or more languages such as Python, Java, Scala, or Go Experience in building production-grade data solutions (relational and NoSQL) Experience with systems monitoring/alerting, capacity planning and performance tuning Experience with BI tool like Tableau, Looker etc. Experience in technical consulting or other customer facing role Useful Qualifications: Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc) Experience with IoT architectures and building real-time data streaming pipelines Experience operationalizing machine learning models on large datasets Demonstrated leadership and self-direction - willingness to teach others and learn new techniques Demonstrated skills in selecting the right statistical tools given a data analysis problem About SADA Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer. Make them rave Be data driven Be one step ahead Be a change agent Do the right thing Work with the best: SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the 2018 Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal! Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs. Business Performance: SADA has been named to the INC 5000 Fastest Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.",3.8,"SADA
3.8","San Francisco, CA",-1,51 to 200 Employees,-1,Company - Private,Logistics & Supply Chain,Transportation & Logistics,Less than $1 million (USD),-1
Data Engineer,-1,"We are looking for an experienced Data Engineering thought leader to own our data platform at LaunchDarkly. This team is part of the Infrastructure Delivery organization. We are a passionate group of engineers who's mission is to support, empower, provide force multiplication for all employees of LaunchDarkly. We are growing rapidly, and need reliable and efficient answers for questions key to our future. You'll be instrumental in developing our practice for the next stage of growth. We are looking for someone with a track record of strong and trusted relationships with stakeholder, an ability to communicate the results from analyses and influence roadmaps. The ideal fit is someone who can dig into the business challenge and ask the core questions, ensure data integrity and statistical significance, and build a convincing case to take action. Responsibilities: Build and maintain data pipelines that are trustworthy and reliable Own and maintain our data lake infrastructure Support and improve our toolset to serve the data needs of our internal customers Help answer the business' most pressing questions Build tools and teach people how to find answers to their own questions Basic Qualifications: 5+ years experience in data engineering for a growing company Solid understanding of data modeling - dbt experience preferred Experience working with cloud-native data store - we use Snowflake Experience with modern ETL/ELT tools and job scheduling - we use Fivetran and Airflow Expertise in SQL and proficiency in another data programming language (Python, R, etc.) Deep experience in data instrumentation and history of strong partnership with engineering teams Champion for engineering and operational excellence, including infrastructure-as-code, change control, testing, and automation Preferred Qualifications: Experience in both enterprise (or B2B) as well as consumer (or B2C) environments MS, MBA or PhD degree in a relevant topic About LaunchDarkly: LaunchDarkly is a Feature Management Platform that serves over 100+ billion feature flags daily to help software teams build better software, faster. Feature flagging is an industry standard methodology of wrapping a new or risky section of code or infrastructure change with a flag. Each flag can easily be turned off independent of code deployment (aka ""dark launching""). LaunchDarkly has SDKs for all major web and mobile platforms. We are building a diverse team so that we can offer robust products and services. Our team culture is dynamic, friendly, and supportive. Our headquarters are in Oakland. At LaunchDarkly, we believe in the power of teams. We're building a team that is humble, open, collaborative, respectful and kind. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status. We've partnered with KeyValues to help demonstrate the amazing culture we've built here at LaunchDarkly, find more info at https://www.keyvalues.com/launchdarkly",5.0,"LaunchDarkly
5.0","Oakland, CA",-1,51 to 200 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, BI",$112k-$192k (Glassdoor Est.),"Company Description Square builds common business tools in unconventional ways so more people can start, run, and grow their businesses. When Square started, it was difficult and expensive (or just plain impossible) for some businesses to take credit cards. Square made credit card payments possible for all by turning a mobile phone into a credit card reader. Since then Square has been building an entire business toolkit of both hardware and software products including Square Capital, Square Terminal, Square Payroll, and more. We’re working to find new and better ways to help businesses succeed on their own terms—and we’re looking for people like you to help shape tomorrow at Square. Job Description As a Business Intelligence Data Engineer, you will develop and manage curated datasets and key metrics across functional units at Square. You will architect and manage Data Models and ETLs across the Square ecosystem. You will: Partner with functional leads to understand their data and reporting requirements, and translate them into definitions and technical specifications (PRD) Develop curated datasets with standardized metrics across the company Develop and maintain ETL jobs and visualizations Work with the data platform engineering team to develop data structures and reliable data pipelines Troubleshoot technical issues Perform ad hoc analysis, insight requests and data extractions to resolve important business issues Qualifications 8+ years of direct BI Data Engineering experience Expert knowledge in data modeling concepts and implementation Technical accomplishments in SQL, ETLs and familiarity with technologies such as Airflow MySQL, Snowflake, Redshift, or similar data handling experience Experience processing extremely large datasets Experience with Python Experience with Linux/OSX command line, version control software (git), and general software development Expertise in visualization technologies including Tableau and/or Looker Knowledge of payment network data and SaaS Additional Information At Square, we value diversity and always treat all employees and job applicants based on merit, qualifications, competence, and talent. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance. Applicants in need of special assistance or accommodation during the interview process or in accessing our website may contact us by sending an email to assistance(at)squareup.com. We will treat your request as confidentially as possible. In your email, please include your name and preferred method of contact, and we will respond as soon as possible. Perks At Square, we want you to be well and thrive. Our global benefits package includes: Healthcare coverage Retirement Plans Employee Stock Purchase Program Wellness perks Paid parental leave Paid time off Learning and Development resources",4.0,"Square
4.0","San Francisco, CA",-1,1001 to 5000 Employees,2009,Company - Public,Computer Hardware & Software,Information Technology,$1 to $2 billion (USD),-1
Data Engineer,$66k-$123k (Glassdoor Est.),"Yapstone is looking for a Data Engineer to be part of the Data Services Team. The Data Services team works very closely with all aspects of data, both internal and external. We are looking for a Data Engineer with the Software Engineering skills to build data pipelines for efficient and reliable data movement across systems, and also to build the next generation of data tools in public cloud to enable us to take full advantage of this data. In this role, your work will broadly influence the company's data consumers, executives and analysts. This is a full-time position based in our office in Walnut Creek. Key Responsibilities: Design, build and launch extremely efficient & reliable data pipelines to move data to our Data Warehouse/Data Mart. Own end-to-end data quality for the data pipelines you build Develop ETL routines to populate databases from multiple disparate data sources and create aggregates Create and run data migrations across different servers and different databases including Enterprise CRM and ERP applications. Perform complex data transformations, create/update stored procedures/functions, and optimize existing stored procedures/functions using indexing, temp tables, views, logic changes, etc. Design/develop new systems and tools to enable stakeholders to consume and understand data faster Data cleansing and manipulation using your expert SQL & programming skills Troubleshoot data issues and present solutions to the issues Prepare activity and progress reports regarding database & data health and status Design and improve agile development processes as it applies to data and data structure design Design, code and automate data quality checks, metrics, standards and guidelines Work across multiple teams in high visibility roles and own the solution end-to-end Requirements: BS or MS in Computer Science, Information Management, or related field 5+ years of experience as a Data Engineer. Candidate must have a deep understanding of logical and physical data modeling for OLTP and OLAP systems. Ability to translate a logical data model into a relational or non-relational solution as appropriate Familiar with multiple relational platforms, recent MSSQL Server experience is required. Hands-on expertise in database development using views, T-SQL, MSSQL and/or SQL scripts and SSIS packages and transformations. Experience building and troubleshooting SSAS cubes. Fluent in using tools like SQL Server Management Studio or similar. Recent experience in SQL tuning, indexing, partitioning, data access patterns and scaling strategies Programming/Scripting experience in Windows (C#, PowerShell) as well as Unix/Linux environments (Python, Bash) Experience in NoSQL/Big Data technologies (Couchbase or MongoDB) Excellent analytical problem solving and decision-making skills Experience working with large complex sets of data in a high-availability environment Experience with agile methodology process and development practices Plus to have: Payment or e-commerce industry experience Experience working with Informatica Intelligent Cloud Services (IICS) Experience in Business Intelligence tools and technologies Experience with Snowflake – Snowpipe, SnowSQL, Snowflake Procedures Experience in building out BI solutions in Looker",2.9,"Yapstone
2.9","Walnut Creek, CA",-1,201 to 500 Employees,1999,Company - Private,Financial Transaction Processing,Finance,$100 to $500 million (USD),-1
Staff Data Engineer,-1,"The most successful companies understand that data and analysis are the foundation of any effective business strategy. Modern enterprises live and breathe data that they leverage through all sorts of dedicated analytics platforms: customer, product, supply chain, finance, etc. But until now, there's never been an effective software solution to understand, measure, and optimize companies' most important asset: their brand. Survata is a venture-funded startup building the Brand Intelligence Platform. We offer a comprehensive set of tools that help our Fortune 100 clients understand and improve how they're perceived in the marketplace, how aware consumers are of the value they provide, and how much consumers trust them to provide the value they promise. Everything we do rests on a foundation of data, and as a data engineer you'll help us keep extending that foundation and keep it robust and strong. We deal with complex, fast-moving data at massive scale and you'll need every trick in the distributed systems book to make sure our clients always have the data they need, when they need it, how they need to see it in order to successfully allocate tens of millions of dollars marketing budget. Survata's business is growing rapidly and our data systems have started to become victims of their own success. We're looking for a deeply experienced, deeply knowledgeable technical leader who can help us architect and build a new generation of big data systems to carry our business into the next phase of growth. Our systems deal with tens of billions of data points each month, and you'll be the primary person responsible for making sure that our capacity goes up even as our costs go down. Although this isn't a management role (though it might grow into one, if and only if you're interested), we do expect you to play a key role in training the rest of our data team and in setting and maintaining the highest technical standards while also serving as a role-model for healthy, collaborative communication. Your responsibilities will include: Creating and maintaining data ingestion, transformation, storage, and analysis systems that are critical to our ability to meet our obligations to our customers and to deliver market-leading new products and features. Introducing and applying cutting-edge technologies and techniques around big data, distributed systems, analytics, microservices, data pipelines, and observability. Owning projects through their entire lifecycle, from conception to architecture to implementation to maintenance. Growing our team both through helping shape our future hiring and through inspiring, mentoring, and upskilling our existing engineers. Embodying and modeling the discipline of practical, professional engineering in your day to day development process and practices. What we're looking for: You get excited about billions of events and terabytes of data. You know how to design large scale systems that are reliable, scalable, efficient, maintainable, extensible, and elegant. You deeply understand the power and promise of distributed systems, and you have enough experience building them to know where the pitfalls are and how to avoid them. Idempotence excites you. You love the feeling of seeing a powerful computing cluster fully and efficiently utilized. You are deeply familiar with the Apache big data ecosystem. No one has used every tool, but you should have used several of them extensively in production (and you should know the basics of most of the major players). You should be able to intelligently discuss the tradeoffs that different tools make, and e.g. when to use Hadoop vs. Spark or Hive vs. Presto or Hbase vs. Cassandra. You've used a lot of the services in a major public cloud (preferably AWS). You love some, are frustrated by others, and can't wait to try the next one. You love to learn. You understand that nothing stands still in technology, and you're excited when newly available tools unlock new and radically better ways of solving problems. But you also know when to apply tried and tested tools and techniques. You have strong hands-on experience writing enterprise-grade applications and services in Java or another language on the JVM. People tend to look to you as a leader and respect your expertise, even in roles where you don't have formal authority. You understand that your software runs on computers (but you architect things so you spend the minimum amount of time possible thinking about that). You are a strong believer in the best software development principles (e.g. SOLID OO-design, design patterns) and processes (e.g. TDD, peer reviews, automation) You have experience mentoring junior developers, and understand that healthy human systems are essential to developing and maintaining healthy technical systems. You thrive on the energy of operating in a fast-paced, ever-changing startup atmosphere. You are a self-starter and you love working self-driven in a dynamic team. The typical candidate who's reached the necessary level for this role has more than 8 years of professional experience and more than 4 working specifically on distributed systems and/or with the Apache big data ecosystem. But we care more about what you've accomplished than about how many years you've spent doing it. Bonus Points: Experience with Groovy, Grails, Spring Boot, DynamoDB, Athena, Cassandra, EMR, Kubernetes, Hive, Presto. Expertise in designing and developing scalable, robust and high performing backend services including efficient and optimized API and protocol design. You know when and how to develop utilize microservices but you also understand the trade-offs when applying SOA Solid DevOps experience. You understand that operational systems are everyone's responsibility, and perhaps you've deployed and maintained your own cluster. Experience with marketing or advertising technology. Survata is the Brand Intelligence Platform. We make brand marketing more impactful. Survata plans, measures, and optimizes brand marketing. With our Brand Campaign Measurement and Instant Insights products, we provide a software & data platform to the world's largest brand marketers. Our customers are Fortune 500 companies across multiple verticals - including CPG, food & beverage, consumer technology & telecom, and financial services - as well as the world's largest advertising agencies and media platforms. We are unapologetically supportive of brand advertising, and work hard every day to prove its value; we know if companies can measure the value of those dollars, they'll spend more. Brand advertising pays for not only the movies we watch and music we hear, but the journalism we read and the information we access. In short, brand advertising supports the free flow of information through society. So, we're proud to be the first company dedicated to using data science to show enterprises the true effectiveness of their brand spend. We are backed by leading venture investors (Y Combinator, Uncork Capital, Bloomberg Beta, Initialized Capital, PivotNorth, Ridge Ventures, Industry Ventures, Conductive Ventures,) and leading MarTech founders & CEO. We're a humble but ambitious team that takes its work seriously but never ourselves. Come join us.",4.0,"Survata
4.0","San Francisco, CA",-1,51 to 200 Employees,2012,Company - Private,Internet,Information Technology,$5 to $10 million (USD),-1
Data Engineer 3,$101k-$184k (Glassdoor Est.),"Who we are: Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 286 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies. Job Description Summary: Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s hundreds of millions of active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Job Description: Job Description: The Oversight and Impact Measurement team is seeking an experienced Tableau Developer and Data Engineer to help drive the Product & Platform Health Dashboard forward. We are looking for a Tableau Developer and Data Engineer to work with the Business and Product/Platforms development teams. The responsibilities of a Tableau Developer and Data Engineer include creating technical solutions, creating data storage tools, and conducting tests. To be successful as a Tableau Developer and Data Engineer, you should have a broad understanding of the business technology landscape, the ability to design reports, and strong analytical skills. Ultimately, a top-notch Tableau Developer and Data Engineer should have an aptitude for using computers, strong written and verbal communication, and proficiency in reporting analysis tools. Responsibilities: Developing, maintaining, and managing advanced reporting, analytics, dashboards and other BI solutions. Performing and documenting data analysis, data validation, and data mapping/design. Reviewing and improving existing systems and collaborating with teams to integrate new systems. Conducting unit tests and developing database queries to analyze the effects and troubleshoot any issues. A solid understanding of SQL, rational databases, and normalization. Will be expected to write DDL and DML as needed including: create tables, create views, analyze execution plans, and create indexes. Requirements: Degree in Mathematics, Computer Science, Information Systems, or related field. Relevant work experience. A solid understanding of SQL, rational databases, and normalization. Experience in database schema design preferred. Proficiency in use of query and reporting analysis tools. Competency in Excel (macros, pivot tables, etc.) Extensive experience in developing, maintaining and managing Tableau driven dashboards & analytics and working knowledge of Tableau administration/architecture. Experience with Power Query, Power Pivot, SQL, Power BI a plus Ability to utilize a diversity of software packages, applications and automated systems for collecting and managing data Minimum Experience: Bachelor’s degree and 6-9 years of directly applicable experience; Master’s degree and 4+ years experience preferred. Additional Skills: Excellent written and verbal communication skills. Self-starter demonstrates independence, initiative and follow-through. Delivers on time while meeting quality objectives and customer expectations Work across the company to drive to decisions, build consensus and bridge gaps. Builds rapport, credibility and relationships with multiple stakeholders. Great team player. Must be detail-oriented, conscientious, thorough and accurate Must be able to step back and analyze at a broader level, while still being detailed oriented, conscientious, thorough and accurate Strong interpersonal skills, diplomacy, and client service-oriented attitude and mindset Ability to work globally and cross-functionally and lead change in a fast-paced environment Proven ability to function well independently as well as in a team Be comfortable in a fast paced and dynamic environment with a high degree of accuracy Comfortable working in a fluid environment where roles and responsibilities are still evolving Good project management and reporting skills Ability to facilitate group discussions and run meetings Ability to handle multiple projects under pressure Well-developed sense of urgency and follow through Ability to resolve complex issues and settle disputes equitably Ability to determine when to escalate to management and identify the right stakeholders for decision making Proficient with tools such as Power Point, Microsoft Project, MS Word, Excel, Access and Visio We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom. PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities.",4.0,"PayPal
4.0","San Jose, CA",-1,10000+ Employees,1998,Company - Public,Internet,Information Technology,$10+ billion (USD),-1
Data Engineer,-1,"3+ years of experience in Data Engineering
Collaborate and implement the overall enterprise data strategy.
Deliver data services and projects from India, ensuring that these solutions align with the organization’s strategic direction and technology standards.
Hands on in design, develop and maintain conceptual, logical and physical data models for enterprise data warehouse
Technical expertise on Big Data technologies like MQTT, Kafka, Spark, Sqoop, Spark and Kafka.
Deploy solution using AWS, S3, Redshift and Docker/Kubernetes
Deploy solution using AWS, S3, Redshift and Docker/Kubernetes
Own SLA’s for data availability in enterprise data warehouse by ensuring all data jobs run successfully • Forecast and plan resource requirements. Allocate work across team to ensure timely delivery of projects / enhancements
Serve as a subject matter expertise in the areas of data warehousing, data modeling ...
Expert in tools such as Apache Spark, Apache Airflow, Presto
Job Types: Full-time, Contract

Pay: $115,603.00 - $191,059.00 per year

Benefits:
Employee assistance program
Health insurance
Parental leave
Professional development assistance
Referral program
Schedule:
8 hour shift
Monday to Friday
Experience:
Kafka: 2 years (Preferred)
MQTT: 2 years (Preferred)",-1,SNAPIDEA SYSTEMS,"San Francisco, CA",-1,-1,-1,-1,-1,-1,-1,-1
"Data Engineer, Analytics, Intern",-1,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Are you passionate about Facebook’s product, analytics and technology? The Analytics, Data Engineering team is looking for fast-moving analytics candidates and data junkies who want to make an impact. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. You will work with some of the brightest minds in the industry, and you'll have the opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.

Architect, implement and deploy new data models and data processes in production
Perform data analysis to generate business insights
Interface with Engineers, Product Managers and Product Analysts to understand product goals and data needs
Build data expertise and own data quality for allocated areas of ownership
Manage data warehouse plans for a product or a group of products
Support critical data processes running in production

Currently has, or is in the process of obtaining, a Bachelors or Masters degree in Computer Science, Mathematics, or related technical field
Programming knowledge in Python or Java
Knowledge of SQL
Knowledge of database systems
Must obtain work authorization in country of employment at the time of hire, and maintain ongoing work authorization during employment

Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.

Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Data Engineer,$68k-$126k (Glassdoor Est.),"San Francisco, California - United States | Posted - 07/24/20 Overview Data Engineer Job Description and Responsibilities. 1.Experience in service management in Google Cloud Platform and exposure to GCP, AWS or Azure. 2.Proficiency in web service design using RAML, YAML (with JSON) and ODATA. 3.Experience in building common services framework for logging, error handling, auditing, policy management, authentication and authorization. 4.Experience in building microservices using Springboot, GraphQL, Camel, etc. 5.Good understanding of Microservice architecture with experience of server administration and physical deployment (On Cloud, On Premise and Hybrid). 6.Experience in integration with CRM (Microsoft Dynamics and Salesforce), Cloud solutions (Adobe Campaign), Tibco EMS, Databases (Cassandra and RDBMS) and Streaming platforms (Kafka). 7.Experience in source control management using GIT 8.Experience in Continuous integration and Continuous deployment using Maven, Jenkins, Docker, Kubernetes and Springboot. 9.Experience in other programming languages like python, scala, etc 10.Experience in integration with logging and monitoring tools like Splunk, Prometheus, Grafana, etc. 11.Perform IT resiliency experiments by injecting failures into distributed system to improve the systems resiliency 12.UI experience is an added advantage. GCP, Kubernetes, Docker, Springboot, GraphQL, Istio, Spinnaker, Cassandra",3.2,"iknowvate technologies
3.2","San Francisco, CA",-1,1 to 50 Employees,2001,Company - Private,Computer Hardware & Software,Information Technology,$1 to $5 million (USD),-1
Data Engineer,-1,"Pixelberry Studios, a division of Nexon, is the creator of the hit game ""Choices"". Our team has been around since 2011, and is 3-for-3 launching games into the AppStore's Top 25 Grossing chart. We believe that innovative game design, commercial success, and making a difference all go hand in hand.

Our world-class development team is looking for a Data Engineer. The winning candidate will help us build reliable and secure analysis and reporting that drive our endeavors. Working alongside our tight, razor-sharp team with a high degree of autonomy, you will take ownership of analytics reporting, data cleanliness, and ad-hoc analytics projects. We will rely on you to deliver mission-critical components, serving our millions of fans worldwide. This would be for a full time, eventually on-site position in Mountain View, CA. Pre-covid, we worked from home two times a week, so our team is used to the productivity of remote work but also cares about building culture from in-office interactions.

Responsibilities
Partner with leadership, engineers, program managers and data scientists to understand data needs.
Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms
Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.
Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.
Build data expertise and own data quality for your areas.

Requirements

3+ years of python development experience
3+ years of database experience and strong SQL knowledge
2+ years of experience in cloud infrastructure and Microservices
2+ years of big data experience and knowledge of data warehouse architecture
Strong understanding of Architecture, Design Patterns and Software Principles
Computer Science degree, or equivalent work experience
Good communication skills
Bonus Points
Custom real-time ETL design experience
Redshift experience
Experience with the AWS stack (S3, EC2, ECS, Lambda, Step Functions, etc)
Experience of providing SQL debugging and support to analysts/BI
Handling data from multiple sources with different formats
Knowledge of mobile gaming metric

Benefits

Competitive salary.
High level of autonomy and freedom.
Work from home Tuesdays and Thursdays (Pre-Covid)
Full benefits package (PTO, 401k, Medical, Dental, Vision).
Free food and snacks, movie nights, and field trips (Pre-Covid)
How This Job is Different
Feel good about what you do.
Great environment to grow and makes a difference
Learn about the business side of mobile games. Our team is very good at what we do.
Good work/life balance. We are family-friendly and look at things in the long term.

We believe that the unique contributions of all our team is the driver of our success. To make sure that our products and culture continue to incorporate everyone’s perspectives and experience we never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines. Pixelberry is an Equal Opportunity Employer.",4.9,"Pixelberry Studios
4.9","Mountain View, CA",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Senior Data Engineer,$96k-$175k (Glassdoor Est.),"About Spin

Spin is a fast-growing micromobility company committed to a world with clean air, liveable cities, safe streets, and widespread access to convenient, affordable transportation. If you’re searching for a top-tier career where you can make a difference and dynamically collaborate within a creative, lively environment—Spin welcomes you!

We’re passionate about transportation and technology. The work we accomplish fulfills a sense of meaning and purpose, which goes beyond the traditional dimensions of a workplace. Our products and initiatives directly influence people’s lives, which is proven by our customers' feedback.

Founded in 2017, and now a subsidiary of Ford Motor Company, we have worked together with cities, campuses, community groups, and businesses to expand internationally. Even as we expand, we maintain the close feeling of a small business, with “fireside chats” chats led by our company’s founders, weekly OKR pop-up videos, music-infused presentations at our All Hands meetings, and more. During this time of working remotely, our creativity and shared goals have kept us united and bonded within our virtual community.

We are a diverse team of artists, engineers, designers, urban planners, policymakers, marketers, and operators. We believe in inclusivity and build powerful alliances to fulfill our goals and move our mission forward. Above all, we at Spin are connected by our values and #BeOrange spirit, which represents unity, purpose, passion, and creativity.

About the Role

Being a data informed company, data helps us create exceptional experience for our customers and provide insights into the effectiveness of our product.

We are looking for Data Engineers that will build and maintain our data warehouse and data pipelines, collect data from multiple sources, and expose services that make data a first class citizen at Spin. You will be building, architecting and launching highly reliable and scalable data pipelines to support data processing and analytics needs. Your efforts will allow access to business and user behavior insights.

Our engineering team consists engineers that are passionate about creating finely polished and intuitive experiences and, at the same time, obsess over performance and reliability of what we build. We challenge the status quo and strive towards finding the best way to solve problems.

We promote being a more well rounded engineer by working on different parts of the engineering stack. We also work in very small groups to keep processes and overhead low, so we have a lot of trust and accountability to perform the work required to build the best product.

Responsibilities
Build and maintain our data warehouse and data pipelines
Scaling up our data infrastructure to meet business needs
Deploy sophisticated analytics programs, machine learning and statistical methods
Work cross-functionally with our product, business, finance and engineering teams

Qualifications

Minimum of 8+ years of relevant experience in building and architecting data solutions
Deep understanding of distributed systems
Expert in SQL and high-level languages such as Python, Java, or Scala
Built and maintained data warehouses and ETL pipelines
Experience with Data modeling
You have worked with big data solutions like Redshift, Snowflake, Hadoop or Hive
Experience with realtime data streaming infrastructure like AWS Kinesis, Spark or Kafka
Worked with Cloud-based architecture such as AWS or Google Cloud
Benefits & Perks
Opportunity to join a fast-growing startup and help shape and establish the company’s industry leadership
Competitive health benefits
Unlimited PTO for salaried roles
Pre-tax commuter benefits
Monthly cell phone bill stipend
Wellness perk for salaried roles
Spin is an equal opportunity employer and will not discriminate against any employee or applicant for employment in an unlawful matter. We celebrate diversity and are committed to creating an inclusive environment for all individuals. Spin treats all employees and job applicants on the basis of merit, qualifications, and competence without regard to any qualified individuals' sex, race, color, religion, national origin, ancestry, gender (including pregnancy, breastfeeding, or related medical condition), sexual orientation, gender identity, gender expression, age, physical or mental disability, medical condition, genetic characteristic or information, marital status, military and veteran status, or any other characteristic protected by state or federal law. Spin also considers qualified applicants with criminal histories, consistent with applicable local, state, and federal law.

Spin is committed to providing reasonable accommodations for qualified individuals with disabilities in its job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at job_accommodations@spin.pm.",4.0,"Spin Electric Scooters
4.0","San Francisco, CA",-1,501 to 1000 Employees,2016,Company - Private,Computer Hardware & Software,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,-1,"The Opportunity

Data Engineering plays a key role in insitro’s approach to rethinking drug development. Our team is responsible for ensuring our biological data factory’s robots and instruments produce high quality data, optimizing storage, queries, and analysis of petabytes of scientific experimental results, and building the infrastructure to train powerful models that solve key problems in the drug development process. You will work closely with a cross-functional team of scientists, bioengineers, and data scientists to identify areas where data engineering can make a difference, by developing data architectures and systems on cutting edge, high throughput platforms that enable our scientists to be maximally productive. You will design, implement, and deploy novel methods that use a broad spectrum of data engineering approaches, including techniques at the forefront of the field. You will work as part of a team to rigorously design our data platform, identify key architectural performance improvements and support ongoing discovery and automation platforms.

You will be joining as the founding team of a biotech startup that has long-term stability due to significant funding, but yet is very much in formation. A lot can change in this early and exciting phase, providing many opportunities for significant impact. You will work closely with a very talented team, learn a broad range of skills, and help shape insitro’s culture, strategic direction, and outcomes. Join us, and help make a difference to patients!

About You

BS, MS, or Ph.D. in computer science, statistics, mathematics, physics, engineering, or equivalent practical experience
Expertise in one or more general-purpose programming languages (such as Python, C/C++, or Go)
Demonstrated ability to write high-quality, production-ready code (readable, well-tested, with well-designed APIs)
Familiarity with cloud computing services (AWS or GCP)
Familiarity with database technologies, data pipelines, workflow engines, distributed computing technologies (Spark, Hadoop, etc).
Familiarity with web services and application frameworks (Django, Flask).
Ability to communicate effectively and collaborate with people of diverse backgrounds and job functions
Proficiency in Linux environment (including shell scripting), experience with database languages (e.g., SQL, No-SQL) and experience with version control practices and tools (Git, Mercurial, etc.)
Passion for making a difference in the world

Nice to Have

Experience with biological data (DNA sequences, RNAseq, proteomics, microscopy images, etc.)
Experience with medium-sized data sets (100TB+)
Experience with the SciPy/PyData ecosystem (numpy, pandas, scipy, dask, etc.)
Demonstrated ability to develop novel data engineering methods that go beyond putting together of existing code, and to apply problem-solving skills to complex issues
4+ years of real-world work experience in software development for high-end data processing engines

Benefits at insitro

Excellent medical, dental, and vision coverage
Open vacation policy
Team lunches (catered daily)
Commuter benefits
Paid parental leave

About insitro

insitro is a data-driven drug discovery and development company using machine learning and high-throughput biology to transform the way that drugs are discovered and delivered to patients. The company is applying state-of-the-art technologies from bioengineering to create massive data sets that enable the power of modern machine learning methods to be brought to bear on key bottlenecks in pharmaceutical R&D. The resulting predictive models are used to accelerate target selection, to design and develop effective therapeutics, and to inform clinical strategy. insitro was launched in 2018 with a Series A of $100M funded by top investors including a16z, Arch Venture Partners, Foresite Capital, GV, and Third Rock Ventures. In 2019 the company announced a collaboration with Gilead Sciences in the area of NASH and, in mid 2020, announced a Series B financing of $143M including current investors and new investors Canada Pension Plan Investment Board (CPP Investments), T. Rowe Price, BlackRock, Casdin Capital and other leading investors. The company is located in South San Francisco, CA. For more information about insitro, please visit the company’s website at www. insitro.com.",-1,insitro,"South San Francisco, CA",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Data Engineer,-1,"What you'll do
Apply AI/ML to our wildly-fascinating (valuable) data to predict price movements and generate alpha
Apply NLP to millions of messages and derive high-signal, actionable sentiment analysis of assets
Own the ETL pipeline for our Market Data and Broker services while optimizing queries
Set up and tune databases and validate data to ensure it's up to the minute
Build sexy, interactive visualizations and dashboards to represent our proprietary data insights to users
Publish research based on the amazing trends you’ll find buried in our data (these will go viral across Bloomberg/WSJ/CNBC/r/dataisbeautiful)
Adapt ML algorithms and SQL dashboards to solve problems across several teams, such as product, operations, customer support, compliance, and security
Build statistical models to predict user behavior and drive business intelligence
Integrate multifaceted data streams such as rapidly changing market data, user data based on app activity, and brokerage operations data to perfect our processes and workflows
Combine knowledge of several research domains to power product and strategic decisions
Explore new ways to equip users with investing insights
Help improve information architecture behind the billions of market data calls we process, reducing latency and improving accuracy/integrity
What you're like
3+ years production data science experience
Intense technological know-how combined with insatiable curiosity, creativity, and user empathy
Preferably (not required) a graduate degree in a quantitative field such as mathematics, statistics, machine learning, computational statistics, NLP, artificial intelligence or similar fields
Strong collaborative mindset and experience working with external clients to translate their needs into effective engineering solutions
Solid understanding of statistical analysis and machine learning algorithms.
Excellent programming skills, including expert level familiarity with either Python or R programming languages
You spend your free time in r/dataisbeautiful and Kaggle
Exquisite SQL
Strong grasp of available data pipeline and machine learning technologies (Spark, Tensorflow, AirFlow, SageMaker etc. - experience with AWS ecosystem a plus)
Ability to lead technical architecture discussions and help drive technical decisions, as well as implement day-to-day changes
Why you're mission-critical
We’ve created an outrageously valuable data set that’s never existed before, we’re confident we can generate alpha. In order to reach our potential, we need to apply state-of-the-art machine learning and data science techniques towards natural language processing, security, anomaly detection, optimization, and time series forecasting.
We’re also a data-driven company: every product decision is backed by data. We can’t build a world-changing product without a world-class data scientist to provide insight and guidance.
Our current data stack
PostgreSQL
Python + R
Metabase
AWS
IEX
Mixpanel
Datadog",-1,CommonStock,"San Francisco, CA",-1,-1,-1,-1,-1,-1,-1,-1
"Data Engineer, SEO",$112k-$196k (Glassdoor Est.),"About Pinterest: Millions of people across the world come to Pinterest to find new ideas every day. It’s where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. As a Pinterest employee, you’ll be challenged to take on work that upholds this mission and pushes Pinterest forward. You’ll grow as a person and leader in your field, all the while helping users make their lives better in the positive corner of the internet. Specifically, our Growth Search Traffic team is on a mission to make Pinterest the best destination for search engines users. This is the opportunity to join the team that brings a great deal of users to Pinterest. We’re looking for a strong data engineer to help us architect, build, and maintain the systems to identify the best content on Pinterest and publish it to the search engines. What you’ll do: Build scalable and reliable systems that efficiently process big data Collaborate with teams across the Growth organization to deliver innovative products that help improve content sharing, user-to-user communication and user retention Ensure we have the right data and work with the team to develop workflows to supply the data Continue to expand our SEO experimentation framework to understand the impact of our content selection, interlinking changes and more What we’re looking for: 5+ years of data engineering experience, preferably in Growth teams Strong experience in writing reliable, low-maintenance and powerful code that may be used by many other engineers Desire to understand search engine ecosystems and learn metrics-driven approach to software development #LI-PB1219",3.9,"Pinterest
3.9","San Francisco, CA",-1,1001 to 5000 Employees,2010,Company - Public,Internet,Information Technology,$100 to $500 million (USD),-1
"Staff Data Engineer/Tech Lead, Data Engineering",$121k-$215k (Glassdoor Est.),"Join us on the tech team of an extremely fast-growing, technology-driven startup that's making effective skincare accessible to everyone. Senior/Staff Data Engineer, Data Platforms Have you ever changed someone's life with a data pipeline? As a company we help people to face the world with skin they are confident in; as a team, Data Engineering enables teams throughout the company to make decisions with data they feel confident in. Data underpins virtually everything we do at Curology—from a truly individualized patient experience, to efficient business operations, to cutting edge Marketing workflows. Here at Curology we see you and recognize that data engineering is the foundation a data driven company is built on. Why this role? 1. Data is a product. We believe the true potential of Data teams lies in a product-oriented mindset and that this is even more relevant to data. 2. Exceptional impact. Data engineers are force-multipliers that enable others to work better and faster. Data is deeply integrated into what we do, this role and team are key to our continued success. 3. Develop a modern data stack. We use the best tools for the job and you will be part of growing and cultivating our modern data stack. AWS, Redshift, our S3 Data Lake, we build for the future. 4. Work with a talented and passionate team. Our small team of data engineers has achieved outsize results by maintaining a high bar for ownership and product quality. 5. Join us at a magical time. We've tripled our business in the last year and we aren't planning on slowing down any time soon. Company Mission We want to make effective skincare accessible to everyone. Humans want to feel confident in their own skin, but it's not easy to see a dermatologist. Curology is revolutionizing dermatology by making effective skincare accessible to everyone. We're part healthtech startup, part skincare lab — and completely focused on helping hundreds of thousands of people get medical care previously available to only a tiny percentage of the population. What will you do as a Staff Data Engineer/Tech Lead, Data Engineering at Curology? You'll design, manage and optimize the flow of data throughout the organization. You'll design a modern stack to build a cloud-first product — we love the cloud and modern tools like Serverless. You'll work with engineers, who are just as excited about data as you are to ensure data quality, integrity and availability. You'll lead the team to integrate consistent and high coding standards. You'll automate manual processes by working closely with teams like Marketing, BizOps, and Product to discover opportunities for programatic efficiency. You'll work on initiatives to keep our system elegant and productivity high — such as improving our metrics, analytics, and experimentation infrastructure. Continue to keep Privacy and Data Protection (PDP) the first-order consideration of data. You might be a good fit if... You have at least 10 years professional experience building and designing software with Python. At least 10 years professional experience modeling SQL and noSQL data. You are willing to use other languages if needed for the right tool. (ie. basic javascript) Have used Airflow to develop and monitor batch data pipelines. Have worked with permissions and regulated or controlled data. (HIPAA/GDPR/CCPA/FDA) Are passionate about getting the right data to the right person. Willing to lead in areas of strength and learn new skills when needed. You've designed systems at scale with AWS Lambda or a similar serverless technology. You've mentored teammates You have worked as a tech lead You'll love working at Curology because... Amazing team culture and environment. Awarded Great Place To Work & Inc.’s Best Workplace Competitive salary and stock options. Unlimited, flexible PTO for exempt employees. Excellent medical, dental and vision insurance. 401(k) to help you save for the future Paid maternity and paternity leave. Free catered daily lunch and a kitchen stocked with delicious snacks, drinks, and coffee. Company-sponsored happy hours and outings. A free subscription to Curology!",5.0,"Curology
5.0","San Francisco, CA",-1,201 to 500 Employees,2014,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Data Engineer,-1,"About Hive

Hive is a full-stack deep learning platform helping to bring companies into the AI era. We take complex visual challenges and build custom machine learning models to solve them. For AI to work, companies need large volumes of high quality training data. We generate this data through Hive Data, our proprietary data labeling platform with over 1,000,000 globally distributed workers, generating millions of high quality pieces of data per day. We then use this training data to build machine learning models for verticals such as Media, Autonomous Driving, Security, and Retail. Today, we work with some of the largest companies in the world to redefine how they think about unstructured visual data. Together, we build solutions that incorporate AI into their businesses to completely transform industries.

We are fortunate that investors like Peter Thiel (Founders Fund), General Catalyst, 8VC, and others see Hive's potential to be groundbreaking in AI business solutions. We have over 160 talented individuals globally in our San Francisco and Delhi offices. Please reach out if you are interested in joining the AI revolution!

Data Engineer Role

In order to execute our vision, we need to grow our team of best-in-class data engineers. We are looking for developers who conduct impeccable data practices and implement high quality data infrastructures. We value hard workers who are comfortable improvising solutions to big data challenges while building a system that can stand the test of time. Our ideal candidate has experience building data infrastructure from the ground up, contributes innovative ideas and ingenious implementations to the team, and is capable of planning out scalable, maintainable data pipelines.

As a data engineer, you would at first work primarily on our Hive Media product, taking real-time data from hundreds of television streams and turning them into a combination of real-time and scheduled outputs, especially our signature ads feed. Your work would improve the quality of our results while reducing computational cost and latency. Expect truly novel challenges.
Responsibilities
Writing scheduled Spark pipelines that perform sophisticated queries on the entirety of our datasets
Writing real-time pipelines that execute complex operations on incoming data
Synchronizing large amounts of data between unstructured and structured formats on various data sources
Creating testing and alerting for data pipelines
Building out our data infrastructure and managing dependencies between data pipelines
Defining and implementing metrics that provide visibility into our data quality
Requirements
You have an undergraduate and / or graduate degree in computer science or a similar technical field, with a sound understanding of statistics
You have 1-2 years of industry experience as a data engineer
You have hands-on experience doing ETL and have written data pipelines in either Spark, Hadoop, or similar technologies
You have a sound understanding of SQL
You have worked with data lakes such as S3 or HDFS
You have worked with various databases, such as Postgres, Cassandra, or Redshift before, and understand their pros and cons
You have a working knowledge of the following technologies, or are not afraid of picking them up on the fly: Mesos, Chronos, Marathon, Jenkins
You are fluent in at least one scripting language (preferably NodeJS or python) and one compiled language (such as Scala, Java, or C)
You have great communication skills and ability to work with others
You are a strong team player, with a do-whatever-it-takes attitude
What We Offer You

We are a group of ambitious individuals who are passionate about creating a revolutionary machine learning company. At Hive, you will have a significant career development opportunity and a chance to contribute to one of the fastest growing AI startups in San Francisco. The work you do here will have a noticeable and direct impact on the development of Hive.

Our benefits include competitive pay, equity, health / vision / dental insurance, catered lunch and dinner, a corporate gym membership, etc.

Thank you for your interest in Hive.",3.0,"Hive
3.0","San Francisco, CA",-1,Unknown,2019,Company - Private,Telecommunications Services,Telecommunications,Unknown / Non-Applicable,-1
"Data Engineer, Cell Engineering",$88k-$163k (Glassdoor Est.),"The Role

As a member of the Cell Data Engineering team, you will build a diverse set of tools that power our battery cell development. Your work will help scale our efforts to drive down the price of battery cells and improve their performance. As an engineer on the Cell Data team, you bring top-notch software engineering skills and can contribute to our systems immediately. A strong candidate will either be an excellent software generalist, or a backend engineer who has an exceptionally strong database background.

Responsibilities
Design and implement backend services and tools that handle fleet data collection and batch processing
Help construct machine learning infrastructure to help increase our learnings and experimental throughput
Build systems to monitor and validate our battery reliability and performance tests to make sure that we are making decisions on quality experiment data
Improve the robustness of our experimental pipeline from the time we receive cells, to test, to results
Build tables with the optimum indices for fast querying. These tables will also be constrained to reduce the chance of bad data leaking into our system
Requirements
BS/MS in Computer Science or related area, with strong software engineering experience.
Experience in building and maintaining databases
Experience with Python
Experience working in a Linux command line environment.

· Experience with JavaScript and building UI's is a plus

Experience with and MATLAB is a plus

Tesla participates in the E-Verify Program",3.5,"Tesla
3.5","Palo Alto, CA",-1,10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$2 to $5 billion (USD),-1
Data Engineer,-1,"You know Kiva (Oakland) as a mission-driven platform focused on expanding access to finance to help underserved entrepreneurs and communities thrive. We provide the data layer that enables financial institutions like Kiva to operate successfully.

At Pngme, we are building a data platform to change the way individuals, entrepreneurs, and businesses access finance in sub-Saharan Africa. Financial institutions use our data to make mission critical decisions that drive access to finance in sub-Saharan Africa. While capital is plentiful, it does not flow equally. Why? Without access to the right data, financial institutions cannot properly measure risk or develop financial products tailored to their customers! Africa is the fastest growing developer ecosystem in the world, but there is still a high barrier to entry for developers to access holistic financial data on their customers. It is Pngme’s mission to democratize developers’ access to data and close the finance gap for millions of individuals, entrepreneurs, and businesses in sub-Saharan Africa.

We see value in providing equal access to capital so people can increase their wealth and opportunity, despite their limited resources. By providing access to data, we enable financial institutions to comfortably deploy their capital and make data-driven decisions to drive the growth of their business. It isn’t just access to data, it’s also a matter of putting data ownership back into the hands of people. With equal access and ownership of data, we give people financial choice and freedom.

We are growing our tribe and looking for a diverse slate of individuals to help us democratize access to the financial data that reduces perceived risk in Africa and beyond. We love our product, but we love our people more, recognizing that without them, none of this would be possible. We know there is so much talent out there, with so much to offer, but who feel lost or unappreciated in their current position. We want to invite those folks to join Pngme. In case you missed it, we’re a Data product. As it so happens, we are in need of talented Data Engineers.

As a member of the Data Engineering team you will provide great value by building the infrastructure needed to make sense of our data. You are going to handle data that is completely novel in nature and incredibly rare. Success in this role will only come from a relationship based in trust.

You can trust us to...

provide a playground of opportunity, and the tools you need to be productive
treat you as the expert you are
allow you to move fast and make mistakes
communicate our expectations and keep you updated as plans change
regard you as an integral and valued member of our Tribe
trust you

We will trust you to...

have the experience and desire to build in an empty playground
be someone who can come through the fire
feel comfortable taking risks
learn and innovate quickly
be the best friend of progress
always iterate as needed
take raw data and build products and insights to inform, measure performance, and discover its greatest value
trust us

Some of the technologies you like and what you’ll be working with

You are a Python ninja
Familiarity with TypeScript & GraphQL
Knowledge of AWS EMR and AWS Lambda
An interest in Apache Spark and ETL development
C#, Java, and Scala chops are nice to have :)",-1,Pngme,"San Francisco, CA",-1,-1,-1,-1,-1,-1,-1,-1
Data Engineer,-1,"San Francisco / New York / Remote Fulltime About Huckleberry Huckleberry is rebuilding small business insurance from the ground up. In a multi-trillion dollar industry where paper forms and fax machines still predominate, and customers are wasting countless hours navigating byzantine processes, we provide small business owners with the capability to manage all of their insurance needs through a single, elegant interface. Our team is rethinking every aspect of the experience, from pricing, to underwriting to claims. We're backed by Tribe Capital, Uncork Capital, Crosslink Capital, e.ventures, Postmates CEO Bastian Lehman, Apartment List CEO John Kobs, and several others. We’re looking for a Data Engineer to join our growing team of insurance innovators in San Francisco, CA. As an early member of the Huckleberry team, you will have full ownership of finding the best solutions to design, architect and implement across our stack. Our technology stack is built on Python/Node/Hapi/Postgres/React sitting on AWS. We write a lot of tests, use automated deployment, Github for code reviews, and Sketch/Invision for mockup prototyping. Responsibilities Work closely with product and engineering teams to identify important questions / processes that can be answered or improved with data. Define, improve, and maintain our data infrastructure and any related architecture. Drive the collection of new data and the refinement of existing data sources. Build tooling that can be used by other engineers to capture essential product-related data. Build tooling that can be used by product teams to analyze and evaluate business needs. Develop analytical solutions using machine learning / statistical modeling. Requirements 3+ years of professional experience working with and analyzing large data sets to solve problems. Expert mastery of Python; scientific computing frameworks such as scipy, numpy, pandas, and/or scikit-learn; and tools such as jupyter notebooks. Experience with data workflow frameworks such as Airflow Experience with data processing frameworks such as Hadoop, MapReduce, and associated tools. Experience building ETL pipelines and integrating with APIs that use REST, SOAP, and other technologies. Familiarity with Node.js/JavaScript a plus. Understanding of trade-offs in database and infrastructure design choices. Strong commitment to quality designs, automated testing, and documentation. Good communication skills in English, both written and spoken. Sense of ownership and ability to drive issues and new ideas. Why You'll Love Working at Huckleberry Comprehensive medical, dental and vision insurance with 95% of premiums paid for by Huckleberry Free One Medical subscription Flexible Spending Account and 401k Commuter benefits Paid maternity and paternity leave Free daily lunch and a kitchen stocked with delicious snacks, drinks, and coffee Company-sponsored happy hours and outings",5.0,"Huckleberry Insurance
5.0","San Francisco, CA",-1,1 to 50 Employees,2017,Company - Private,-1,-1,Less than $1 million (USD),-1
"Senior Data Engineer, Data Ocean Platform",$122k-$204k (Glassdoor Est.),"PlayStation isn't just the Best Place to Play —it's also the Best Place to Work. We've thrilled gamers since 1994, when we launched the original PlayStation. Today, we're recognized as a global leader in interactive and digital entertainment. The PlayStation brand falls under Sony Interactive Entertainment, a wholly-owned subsidiary of Sony Corporation. Senior Data Engineer, Data Ocean Platform San Francisco, CA This is a senior level position with the PlayStation Data Platform engineering team, to develop our next generation data platform serving the global, fast growing, PlayStation Network. Within the Data Platform team, we pioneer new insights to support the growth and usability of the platform supporting the various Data Science teams within the SIE organization. Let's talk, if: The idea of crafting processes that will handle very large scale data appeals to you ! You are passionate about making distributed systems faster and more efficient! You enjoy tackling complicated design challenges. You have implemented standard design methodologies for measuring, understanding and improving the system. You are willing to build a positive relationship with other SIE teams to help influence technology decisions. You enjoy participating in a development process, establishing and influencing quality engineering standards. Requirements: Play a senior role in the evolution of a highly performant data platform and associated processes, applying industry standards to enable highly available, extensible data services for the SIE platform Design and develop internal tools to increase awareness of Sony's Data Platform, to improve both the visibility of our data as well as establishing systems and procedures make it easier to consume. You will be part of a team developing data processing services such as data reshaping, automated feature extraction and data measurement required to shorten the time to value for the data consumers. This position requires extensive hands-on technical domain expertise in data infrastructure along with excellent communication skills You will develop test scripts and scenarios to verify that your code meets Sony standards. You will participate in product road-map discussions and identify key areas for improvement. Willing to do what others do not do. Be Amazing Qualifications: BS Degree in Engineering, Computer Science or equivalent experience. 5+ years' experience in software development using Scala, Python or Java. Experience with distributed processing systems such as Apache Spark. Strong foundation in data engineering, data structures and software design. Experience with storage formats such as, Parquet / ORC / AVRO Experience with a hive compatible meta store catalog such as AWS Lake Formation. Interest in working with the data science and machine learning teams. Experience with agile TDD development methodologies. Possess the drive and passion for quality with the ability to inspire, excite and motivate other team members. Strong verbal and written communication skills, and be able to work with others at all levels, effective at working with geographically remote and culturally diverse teams. Sony is an Equal Opportunity Employer. All persons will receive consideration for employment without regard to race, color, religion, gender, pregnancy, national origin, ancestry, citizenship, age, legally protected physical or mental disability, covered veteran status, status in the U.S. uniformed services, sexual orientation, gender identity, marital status, genetic information or membership in any other legally protected category. We strive to create an inclusive environment, empower employees and embrace diversity. We encourage everyone to respond. We sincerely appreciate the time and effort you spent in contacting us and we thank you for your interest in PlayStation. #LI-CD1",3.7,"Sony Interactive Entertainment PlayStation
3.7","San Francisco, CA",-1,5001 to 10000 Employees,1994,Subsidiary or Business Segment,Video Games,Media,$10+ billion (USD),-1
"Sr. Data Engineer, Data Platform",$114k-$204k (Glassdoor Est.),"Who we are: MasterClass is transforming online education by enabling anyone in the world to learn from the very best. We are deconstructing what makes an actor able to cry on demand, how an athlete defies gravity, and what it takes to write a bestseller. Our online learning content is available to students anywhere anytime, which supports our mission to ignite the greatness in others. We are a quickly growing VC-funded startup based in San Francisco, with additional offices in Los Angeles. We have created online classes taught by famous masters— Gordon Ramsay, Serena Williams, Neil deGrasse Tyson, Sara Blakely, David Sedaris, Bobbi Brown, Timbaland, Anna Wintour and many more to come. Since launching in 2015, we have been growing our team. Apply now to find out more about what we are doing behind the scenes. What we are looking for: Data, and how they are used, play a central role at MasterClass and are at the heart of how we make business, product, content, and operational decisions. Our growing Analytics, Data Science, and Data Engineering teams sit at the core of the company and collaborate with a variety of departments to drive decisions and provide direction for future growth at MasterClass. The team tackles challenging problems across many technical disciplines, including time series forecasting, causal inference, optimization, and machine learning. We are looking for an exceptional Senior Data Engineer to help build our data platform to scale the business and enable our Analytics and Data organization in solving those challenges. Responsibilities of the role: Proactively drive the execution of our data engineering, business intelligence, and data warehouse roadmap Understand and translate business needs into data models to support long-term, scalable, and reliable solutions Create logical and physical data models using best practices to ensure high data quality and reduced redundancy Drive data quality across the organization; develop best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking Define and manage SLA's for data sets and processes running in production Continuously improve our data infrastructure and stay ahead of technology Design a system for data backup in case of system failure Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs Requirements: 4+ years of experience in Data Engineering and Data Warehousing Bachelor's degree in a quantitative field, e.g. Computer Science, Math, Physics Experience scaling data environments with distributed/RT systems and self-serve visualization environments Advanced proficiency with SQL, Perl, Python, Postgres, REST/GraphQL Experience designing and implementing cloud based and SaaS data warehouse (e.g. WS, Hadoop, NoSQL) and developing ETL/ELT pipelines Experience integrating and building data platform in support of BI, Analytics, Data Science, and real-time applications Strong communication skills, with the ability to initiate and drive projects proactively and accurately Work full-time in our San Francisco office At MasterClass, we believe we put our best work forward when our employees bring together ideas that are diverse in thought. We are proud to be an equal opportunity workplace and are committed to equal employment opportunity regardless of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or any other characteristic protected by applicable federal, state or local law. In addition, MasterClass will provide reasonable accommodations for qualified individuals with disabilities. If you have a disability or special need, we would like to know how we can better accommodate you.",3.8,"MasterClass
3.8","San Francisco, CA",-1,51 to 200 Employees,2015,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,-1,"*Data Engineer*
*Location: * Freemont CA (Remote work in the beginning)
*Term: * 6 months with chance of extensions (long term)
*US Citizenship status: * Any legal status authorized to work, including CPTs / OPTs
*Skills: *
Advance SQL, Python for data pipeline, data warehouse

*Description: *
Around 4 years of experience working as data engineer / data warhousing projects managing petabytes of data.

*Compensation Range: * $40 - $50 per hour

Why Work at This Company? One of the fastest growing company with excellent work environment

Job Types: Full-time, Contract

Pay: $40.00 - $50.00 per hour

Schedule:
* 8 hour shift
* Monday to Friday

COVID-19 considerations:
Remote work is allowed for now because of COVID-19

Experience:
* data engineering: 3 years (Preferred)

Contract Length:
* 7 - 11 months

Contract Renewal:
* Likely

Visa Sponsorship Potentially Available:
* No: Not providing sponsorship for this job

Work Remotely:
* Temporarily due to COVID-19",5.0,"Pixl Inc.
5.0","Menlo Park, CA",-1,1 to 50 Employees,-1,Company - Private,"Department, Clothing, & Shoe Stores",Retail,Less than $1 million (USD),-1
Data Engineer,-1,"Introduction SymphonyRM helps health systems thrive in the rapidly evolving US Healthcare industry by keeping patients healthy and physicians happy. By analyzing large amounts of data from many sources, we empower clients to make smarter decisions at every turn in their business. Our clients love SymphonyRM’s ability to guide them to take the next best action for both patients and physicians. As a Data Engineer, you’ll play a critical role in developing tools to automate data ETL processing, assess data quality from multiple sources, and build a powerful data pipeline, while working closely with our other engineers. We are looking for someone excited to join a small but mighty team that writes efficient, maintainable code to improve the lives of patients and increase physician satisfaction. We care deeply about building long-term careers and offer opportunities for our employees to grow towards project leadership, engineering management, or other roles to make a difference in the lives of patients. We are particularly interested in working with people from backgrounds that are underrepresented in the tech community, including engineers of color and of all genders. You’d be a great addition if… You have a Master’s degree in Computer Science, Statistics or similar degree OR Bachelor’s degree with at least 1-2 year related experience with Python programming OR 4+ years related experience with Python programming (e.g. completion of a Python-focused bootcamp). You seek to fully understand “big data” problems, and strategize to produce efficient, workable solutions. You are curious about emerging technologies, and like to evaluate and adapt where you see fit. You’re motivated by high-impact projects via automation or scaling data operations to drive business value. You’re excited to work with cross-functional teams in an agile environment. You appreciate working with people from diverse backgrounds. Bonus qualifications if... You’ve worked on large-scale databases using cloud computing platforms like Amazon AWS You have strong knowledge with SQL/Relational databases You have experience in using data visualization tools (Looker, Matplotlib, Excel, etc.) Experience with Apache Airflow and/or Pandas You have studied or have experience designing data pipelines and loading large datasets into databases What You’ll Do: You will work closely alongside a small team of engineers to drive continuous improvements to our Python-based data platform. Write and deploy Python code to automate data ingestion using Apache Airflow. Support internal and external stakeholders in troubleshooting and resolving issues. Contribute new ideas in design to development to our data infrastructure - we are always looking to improve. Due to Covid-19, you will be working remotely for the time being. We are actively interviewing and hiring this position based out of our Palo Alto, CA office.",3.6,"SymphonyRM
3.6","Palo Alto, CA",-1,51 to 200 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,$83k-$156k (Glassdoor Est.),"The Stanford University Clinical Excellence Research Center (CERC) strives to lower healthcare spending in the United States by designing, pilot testing, and evaluating innovative models of care delivery. Our design work is enabled by research in areas including human factors, artificial intelligence, and positive deviance. CERC has partnered with the Stanford School of Engineering for its PAC program. The PAC research mission is to deliver Artificial Intelligence based healthcare service delivery solutions that improve cost efficiency and healthcare quality. To disseminate insights from this cutting edge work, CERC and various sponsors are collaborating to translate these findings into practical solutions and evaluate the effectiveness of these interventions on quality and cost. CERC is seeking a Data Engineer to provide highly specialized technical research and support, expert instruction, and consultation; identify and design solutions to a wide range of unique problems and other research and development activities. This position will work in collaboration with faculty, senior research staff members, and graduate students. Please provide a link to your Github portfolio, or equivalent, on your resume. Duties include: Design, develop and implement complex and specialized computer vision technologies in multiple healthcare domains: includes contributing to RFP process and technical specifications development. Oversee technical installation requirements. Develop research mechanisms to collect and store data; work with multiple stakeholders to facilitate preprocessing and analysis of data. Implement pipelines for data collection and data repositories to enable analysis from clinical entities for Stanford Healthcare and external partners. Contribute technical expertise to hardware, software, and algorithm development. Design detailed and high-level communication materials relating to data collection and preprocessing pipelines for internal and external audiences. Lead computer vision hardware updates and maintenance in conjunction with sensor vendors and clinical staff. Guide Computer Science students on technical and non-technical (e.g., communications, literature reviews) aspects of research projects. *- The job duties listed are typical examples of work performed by positions in this job classification and are not designed to contain or be interpreted as a comprehensive inventory of all duties, tasks, and responsibilities. Specific duties and responsibilities may vary depending on department or program needs without changing the general nature and scope of the job or level of responsibility. Employees may also perform other duties as assigned. DESIRED QUALIFICATIONS: Master’s degree in a related field (e.g., Computer Science and Engineering, Electrical Engineering, Data Science). Thorough knowledge of the principles of engineering and health sciences. Knowledge of computer vision models and techniques. Understanding of computer vision data annotation pipelines. Python programing, scikitlearn, and pytorch. Knowledge of multi-process and multi-thread programming. Data science methods and big data processing knowledge. Knowledge of cloud computing services and technologies. Demonstrated project management experience. Ability to effectively communicate technical information to a broad, layperson audience. EDUCATION & EXPERIENCE (REQUIRED): Bachelor's degree and three years of relevant experience, or combination of education and relevant experience. KNOWLEDGE, SKILLS AND ABILITIES (REQUIRED): Demonstrated ability to prioritize own work and multitask. Demonstrated excellent organizational skills. Demonstrated ability to take initiative and ownership of projects. Ability to communicate effectively both orally and in writing. Ability to routinely and independently exercise sound judgment in making decisions. Demonstrated experience working independently and as part of a team. Relevant subject matter knowledge. PHYSICAL REQUIREMENTS*: Frequently grasp lightly/fine manipulation, perform desk-based computer tasks, lift/carry/push/pull objects that weigh up to 10 pounds. Occasionally stand/walk, sit, twist/bend/stoop/squat, grasp forcefully. Rarely kneel/crawl, climb (ladders, scaffolds, or other), reach/work above shoulders, use a telephone, writing by hand, sort/file paperwork or parts, operate foot and/or hand controls, lift/carry/push/pull objects that weigh >40 pounds. * - Consistent with its obligations under the law, the University will provide reasonable accommodation to any employee with a disability who requires accommodation to perform the essential functions of his or her job. WORKING CONDITIONS: May be exposed to high voltage electricity, radiation or electromagnetic fields, lasers, noise > 80dB TWA, Allergens/Biohazards/Chemicals /Asbestos, confined spaces, working at heights ?10 feet, temperature extremes, heavy metals, unusual work hours or routine overtime and/or inclement weather. May require travel. WORK STANDARDS: Interpersonal Skills: Demonstrates the ability to work well with Stanford colleagues and clients and with external organizations. Promote Culture of Safety: Demonstrates commitment to personal responsibility and value for safety; communicates safety concerns; uses and promotes safe behaviors based on training and lessons learned. Subject to and expected to comply with all applicable University policies and procedures, including but not limited to the personnel policies and other policies found in the University's Administrative Guide, https://adminguide.stanford.edu. Why Stanford is for You: Imagine a world without search engines or social platforms. Consider lives saved through first-ever organ transplants and research to cure illnesses. Stanford University has revolutionized the way we live and enrich the world. Supporting this mission is our diverse and dedicated 17,000 staff. We seek talent driven to impact the future of our legacy. Our culture and unique perks empower you with: Freedom to grow. We offer career development programs, tuition reimbursement, or audit a course. Join a TedTalk, film screening, or listen to a renowned author or global leader speak. A caring culture. We provide superb retirement plans, generous time-off, and family care resources. A healthier you. Climb our rock wall, or choose from hundreds of health or fitness classes at our world-class exercise facilities. We also provide excellent health care benefits. Discovery and fun. Stroll through historic sculptures, trails, and museums. Enviable resources. Enjoy free commuter programs, ridesharing incentives, discounts and more. *- Stanford is an equal employment opportunity and affirmative action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic protected by law.",4.3,"Stanford University
4.3","Stanford, CA",-1,10000+ Employees,1891,College / University,Colleges & Universities,Education,$10+ billion (USD),-1
Senior Data Engineer,-1,"About Rippling Rippling is the first way for businesses to manage their HR & IT — from payroll and benefits, to employee computers and apps — all in one, modern system. In just 90-seconds, a company can set up (or disable) an employee's payroll, health insurance, work computer, and third-party apps, like Gmail, Microsoft Office, and Slack. It's the only platform that truly unifies every employee system, and automates all of the administrative work. Rippling is headquartered in San Francisco and has raised over $200M from top-tier investors, including Founders Fund, Greenoaks Capital, Coatue Management, Kleiner Perkins, and YCombinator. About The Role As the first full-time data engineer at Rippling, you'll make foundational contributions to our burgeoning data engineering efforts. You'll design and build systems that will enable analytics, experimentation, and user-facing features. What You'll Do: Help architect, build, and scale our initial data engineering pipeline from our production systems and other internal systems to our data warehouse Build foundational blocks to migrate our analytics stack including warehouse and reporting solutions, and the ETL engines to pipe data from transactional to analytical db Work closely with product engineering as well as other stakeholders from sales, marketing, and product to understand their needs and create systems that enable better product and growth decision-making. Help select the tools that we use for our data pipeline and analytics Provide data analytics for each function in the company Qualifications: 5+ Years experience in Data and Software Engineering Expertise building complex, distributed services with Python or similar language Experience in analytics, dimensional modeling, and ETL optimization preferred BS/BA in a technical field such as Computer Science or Mathematics preferred If you don't necessarily meet all of the requirements listed here, we still encourage you to apply because skills can be used in lots of different ways, your life experience is equally important sometimes. Benefits: Medical, Dental, Vision, FSA, HSA, Commuter, Life and Disability Benefits Uncapped PTO Flexible work hours Generous parental leave Transgender health insurance coverage Great compensation package (salary, equity) Meeting-light culture Onsite meals Rippling is an equal opportunity employer.",4.9,"Rippling
4.9","San Francisco, CA",-1,51 to 200 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Data Engineer,$99k-$153k (Glassdoor Est.),"We are looking for Data Engineer who has a passion for their customers and a passion for working with data. You like working with your customers, understanding their challenges, and partnering with them to invent great solutions. You like working with large data sets, and bringing data together from multiple systems to answer critical business questions and drive change. You are analytical and creative. You should also have the following skills or experiences:Bachelor’s degree in Computer Science, Mathematics, Statistics, Finance or related technical field. · 3+ years developing end-to-end Business Intelligence solutions: data modeling, ETL and reporting · 3+ years in relational database concepts with a solid knowledge of star schema, Oracle, SQL, PL/SQL, SQL Tuning, OLAP, Big Data technologies · Experience with coding languages like Python/Java/Scala · Experience in working with business customers to drive requirements analysis · Have analytical skills and be creative · Experience with Big Data solutions: Hadoop, Hive or other frameworks · Exposure to large databases, BI applications, data quality and performance tuning · Excellent written and spoken communication skill Hundreds of millions of customers, billions of transactions, petabytes of data… How to use the world’s richest collection of e-commerce data to provide superior value and better paying experience to customers ? The Amazon Payments Team manages all Amazon branded payment offerings globally. These offerings are growing rapidly and we are continuously adding new market-leading features and launching new products. Amazon.com has a culture of data-driven decision-making and demands business intelligence that is timely, accurate, and actionable. This team provides a fast-paced environment where every day brings new challenges and new opportunities. Our team of high caliber software developers, data scientists, statisticians and product managers use rigorous quantitative approaches to ensure that we target the right product to the right customer at the right moment, managing tradeoffs between click through rate, approval rates and lifetime value. In order to accomplish this we leverage the wealth of Amazon’s information to build a wide range of probabilistic models, set up experiments that ensure that we are thriving to reach global optimums and leverage Amazon’s technological infrastructure to display the right offerings in real time. As a Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. You should be passionate about working with huge data sets and be someone who loves to bring datasets together to answer business questions. You should have deep expertise in creation and management of datasets. You will build data analytical solutions that will address increasingly complex business questions. You should be expert at implementing and operating stable, scalable data flow solutions from production systems into end-user facing applications/reports. These solutions will be fault tolerant, self-healing and adaptive. You will be working on developing solutions that provide some of the unique challenges of space, size and speed. You will implement data analytics using cutting edge analytics patterns and technologies that are inclusive of but not limited to various AWS Offerings -EMR, Lambda, Kinesis, and Spectrum. You will extract huge volumes of structured and unstructured data from various sources (Relational /Non-relational/No-SQL database) and message streams and construct complex analyses. You will write scalable code and tune performance running over billion of rows of data. You will implement data flow solutions that process data on Spark ,Redshift and store in Redshift ,Filebased system (S3) for reporting and adhoc analysis. You should be detail-oriented and must have an aptitude for solving unstructured problems. You should work in a self-directed environment, own tasks and drive them to completion. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions. You own customer relationship about data and execute tasks that are manifestations of such ownership, like ensuring high data availability, low latency, documenting data details and transformations and handling user notifications and training. · Experience partnering with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies. · Experience with web technology to develop dashboards. · Practical Knowledge of Linux or Unix shell scripting · Experience in processing large volume of data. · Strong troubleshooting and problem solving skills · Demonstrated experience in dealing with Senior Management on addressing their reporting and metrics requirements Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.",3.9,"Amazon.com Services LLC
3.9","Santa Clara, CA",-1,10000+ Employees,1994,Company - Public,Internet,Information Technology,$10+ billion (USD),-1
"Senior Data Engineer, Cheminformatics/Bioinformatics",-1,"At Atomwise, we invented the first deep learning neural networks for structure-based small molecule drug discovery and we’re currently deploying it in one of the largest applications of machine learning for life sciences. We work on Alzheimer’s, cancer, diabetes, drug-resistant antibiotics, safe pesticides, among treatments for other diseases. We’ve partnered with 4 of the top-10 US pharma companies, raised over $50M from top VCs, and have 100+ diverse projects currently running.

We are looking for a data engineer or data scientist with strong database and software engineering skills, ideally in a scientific domain such as cheminformatics or bioinformatics. You will be contributing to the design and development of massive cheminformatic datasets for machine learning, with the opportunity to construct novel analyses which inform our ML directions for drug discovery research. As a member of the R&D team, you’ll have the chance to interact daily with software engineers, machine learning scientists, computational chemists, and fellow cheminformaticians.

Required qualifications

M.Sc/Ph.D. in Computer Science, Statistics, Cheminformatics, Bioinformatics, Computational Biology or B.S. in Computer Science with 7+ years experience.
Strong computer science fundamentals
5+ years of experience in database engineering, data processing pipelines, and HPC
Strong database design and software-engineering best practices
Strong knowledge of statistics, data analytics, and data visualization
Strong coding skills in at least one high-level programming language (Python, Java, C++, etc)
Good familiarity with Linux command-line environment

Preferred qualifications

Experience in bioinformatics or cheminformatics, working with ingestion of third-party and internal data sources
Experience working with scalable algorithms utilizing large amounts of data
Experience with cloud computing environments (AWS/Azure/GCE)
Experience with non-relational databases
Familiarity with organic chemistry

Compensation & benefits

Competitive salary, commensurate with experience.
Stock compensation plan – you’ll be an Atomwise co-owner.
Platinum health, dental, and vision benefits.
401k with 4% match.
Flexible work schedule.
Generous parental leave.
Relaxed work environment.
Great colleagues.

Atomwise is an equal opportunity employer and strives to foster an inclusive workplace. Our mission is to develop better medicines faster, and we know that we need a diverse team to develop medicines that serve diverse populations. Accordingly, the Company does not make any employment decisions (including but not limited to, hiring, compensation, and promotions) on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, veteran status, disability status, or any other characteristics protected by applicable federal, state, and local law.

We strongly encourage people of diverse backgrounds and perspectives to apply.

Atomwise is not currently offering visa sponsorships for any position. Please only apply if eligible to work in the U.S.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.",5.0,"Atomwise
5.0","San Francisco, CA",-1,1 to 50 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Biotech & Pharmaceuticals,Unknown / Non-Applicable,-1
Senior/Staff Data Engineer,$111k-$193k (Glassdoor Est.),"Change.org is searching for a Staff Data Engineer to help us build a next-generation data processing and machine learning platform and support all our internal data-facing operations. We’re a social impact business (a public benefit company), and the largest tech platform focused on civic action in the world with 80m monthly users, 50,000 campaigns launched on the site every month, 150 staff, and a new revenue model that has grown by 500% in 2 years. We’re profitable, growing quickly, and our users win campaigns for change once every hour. From strengthening hate crime legislation in South Africa; fighting corruption in Indonesia, Italy, and Brazil; to fighting violence against women in India. We are looking for a Senior Data Engineer who is great at developing Spark jobs in Scala, setting up modern cloud infrastructure - DevOps, managing big data warehouses - Cloud at scale and has a passion for expanding our platform to mobilize hundreds of millions of people to take deeper civic action. You will be based out of our San Francisco headquarters and report to our Head of Data Intelligence. As a key member of our Data Intelligence pack, you’ll help us screen for individual defamation, discrimination and disinformation types of content, find the best recommendations for our supporters & enable our staff to help petition starters at scale. You’re a perfect fit for this role if your superpower is building scalable maintainable big data compute pipelines, an expert at using and setting up tools for secure and performant service deployments & being a force-multiplier on teamwork, collaboration, and knowledge sharing. Key responsibilities: Design, development, and implementation of Spark Applications using Scala Produce unit and system tests for all code Participate in design discussions to improve our existing data processing, storage, querying, and machine learning frameworks Use modern infrastructure tools to set up and improve our deployment processes Maintain, debug, tune and monitor our Redshift warehouse, data/ML pipelines, and internal tools The most important capabilities for this role are: Expert level experience with building pipelines with Spark (Flink/Storm) & Scala Expert level experience working with cloud infrastructure and AWS Big Data/Cloud compute/Data Warehousing technologies Solid experience using modern DevOps tools (Jenkins, Docker, Kubernetes, etc.) for setting up performant scalable service deployments Solid experience with distributed systems and object-oriented design Additional capabilities we'll consider: Experience working with Hadoop, HBase, Hive and similar large scale DB Technologies Previous experience working experience in Agile development processes 1+ years of experience with Machine Learning concepts Interested? Great! Here's what you should know: This is a full-time role based in San Francisco, CA (USA) in our office in Potrero Hill. Our team is high impact, low ego, and has an amazing culture to be part of. We are accepting applications until March 16, 2020. We especially encourage applicants of different backgrounds, cultures, genders, experiences, abilities, and perspectives to apply. We’re actively working to increase the diversity of experience and perspectives on our team and are looking for someone who can help continue to lead that process. Change.org is committed to being a diverse and inclusive workplace. All qualified applicants will receive consideration for employment without regard to race, color, national origin, religion, sexual orientation, gender, gender identity, age, physical disability, or length of time spent unemployed. Apply for this job",4.2,"Change.org
4.2","San Francisco, CA",-1,51 to 200 Employees,2007,Company - Private,Internet,Information Technology,$10 to $25 million (USD),-1
"Data Engineer, Product Analytics",$110k-$179k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Do you like working with big data? Do you want to use data to influence product decisions for products being used by over half a billion people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. In this role, you will work with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.This is a full-time position based in our office in Menlo Park.
Manage data warehouse plans for a product or a group of products.
Interface with engineers, product managers and product analysts to understand data needs.
Build data expertise and own data quality for allocated areas of ownership.
Design, build and launch new data models in production.
Design, build and launch new data extraction, transformation and loading processes in production.
Support existing processes running in production.
Define and manage SLA for all data sets in allocated areas of ownership.
Work with data infrastructure to triage infra issues and drive to resolution.
2+ years experience in the data warehouse space.
2+ years experience in custom ETL design, implementation and maintenance.
2+ years experience working with either a MapReduce or an MPP system.
2+ years experience with object-oriented programming languages.
2+ years experience with schema design and dimensional data modeling.
2+ years experience in writing SQL statements.
Experience analyzing data to identify deliverables, gaps and inconsistencies.
Experience managing and communicating data warehouse plans to internal clients.
BS/BA in Technical Field, Computer Science or Mathematics.
Knowledge in Python or Java.
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Sunnyvale, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
"Data Engineer, Music Data Experience",$92k-$144k (Glassdoor Est.),"· 3+ years of experience as a Data Engineer or in a similar role · Experience with data modeling, data warehousing, and building ETL pipelines · Experience in SQL · Bachelor’s degree in Data Science, Applied Science, Computer Science, Computer Engineering or related technical discipline · 3+ years of experience as a data/software developer/scientist or related technical job · Experience with SQL and Spark based data pipelines · Experience with traditional and cloud data modeling techniques · A passion for improving customer experience · Excellent verbal and written communication skills and technical writing skills Amazon Music is awash in data! To help make sense of it all, the Music Data Experience team enables repeatable, easy, in depth analysis of music customer behaviors. We reduce the cost in time and effort of analysis, data set building, model building, and user segmentation. Our goal is to empower all teams at Amazon Music to make data driven decisions and effectively measure their results by providing high quality, high availability data, and democratized data access through self-service tools. If you love the challenges that come with big data then this role is for you. We collect billions of events a day, manage petabyte scale data on Redshift and S3, and develop data pipelines using Spark/Scala EMR, SQL based ETL, and Java services. You are a talented, seasoned, and detail-oriented Data Engineer, BI Engineer, or Data Scientist who wants to take on big data challenges in an agile way. Duties include designing events and signals, building big data pipelines, creating efficient data models, performing analysis, and statistical/ML modeling. We manage Amazon Music's most important data pipelines and data sets, and are expanding our self-service data knowledge and capabilities through an Amazon Music data university. This role requires you to focus on the data end to end, from producers to consumers. You will develop an understanding of our data, analytical techniques, and how to connect insights to the business, and you will gain practical experience in insisting on highest standards on operations in ETL and big data pipelines. With our Amazon Music Unlimited and Prime Music services, and our top music provider spot on the Alexa platform, providing high quality, high availability data to our internal customers is critical to our customer experiences. Music Data Experience team develops data specifically for a set of key business domains like personalization and marketing and provides and protects a robust self-service core data experience for all internal customers. We deal in AWS technologies like Redshift, S3, EMR, EC2, DynamoDB, Kinesis Firehose, and Lambda. In 2019 this team will migrate Amazon Music's information model and data pipelines to a data lake storage and EMR/Spark processing layer. You'll build our data university and partner with Product, Marketing, BI, and ML teams to build new behavioral events, pipelines, datasets, models, and reporting to support their initiatives. You'll also continue to develop our offline analytics capabilities in Tableau and build out our real time dashboarding capabilities. Amazon Music Imagine being a part of an agile team where your ideas have the potential to reach millions. Picture working on cutting-edge consumer-facing products, where every single team member is a critical in the decision-making process. Envision being able to leverage the resources of a Fortune-500 company within the atmosphere of a start-up. Welcome to Amazon Music, where ideas are born and come to life as Amazon Music Unlimited, Prime Music, and so much more. Everyone on our team has a meaningful impact on product features, new directions in music streaming, and customer engagement. We are looking for new team members across a variety of job functions including software engineering/development, marketing, design, ops and more. Come join us as we make history by launching exciting new projects in the coming year. Our team is focused on building a personalized, curated, and seamless music experience. We want to help our customers discover up-and-coming artists, while also having access to their favorite established musicians. We build systems that are distributed on a large scale, spanning our music apps, web player, and -forward audio engagement on and Amazon Echo , powered by Alexa to support our customer base. Amazon Music offerings are available in countries around the world, and our applications support our mission of delivering music to customers in new and exciting ways that enhance their day-to-day lives. Come innovate with the Amazon Music team! · Graduate degree in a related technical field · 5+ years of experience as a data/software developer/scientist or related technical job · Experience with Agile Development · Experience with statistical modeling or machine learning (Classification, Collaborative Filtering) · Experience with AWS services · A love of music! Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. #MusicJobs",3.9,"Amazon.com Services LLC
3.9","San Francisco, CA",-1,10000+ Employees,1994,Company - Public,Internet,Information Technology,$10+ billion (USD),-1
Data Engineer,$75k-$137k (Glassdoor Est.),"Data Engineer (REMOTE, until otherwise noted) - Greater Seattle Area About the role. . . In order to continue and accelerate our growth, we are looking for a Data Engineer with Cloud Solutions background to add to our Seattle, Washington-based team. The engineer is responsible for building a large-scale data pipeline in cloud platform. This may involve in automation of manual processes to cloud environment. Candidate would direct the initiatives for creation of data sets. Delivering client value and ensuring high client satisfaction. Core responsibilities for this position include, but are not limited to the following: Extracts data from various databases; perform exploratory data analysis, cleanses, massages, and aggregates data Employs scaling & automation to data preparation techniques - Introduces incremental improvements to data analysis, visualization, and presentation techniques to communicate discoveries Researches relevant emerging empirical methods and quantitative tools Possesses in-depth business knowledge in order to initiate and drive discussions with business partners to identify business issues needing analytic solutions Leads innovative packaging and presentation of insights to business and broader analytics community Develops processes to automate and scale insights operationalization Develops and drives multiple cross-departmental projects Establishes brand and team as subject matter experts in advanced analytics across departments. Mentors data scientists in pioneering techniques and business acumen Required Qualifications: Cloud solution implementation experience with Azure Data Lake and Spark preferred Minimum 5 years hands-on experience with SQL At least one year of experience in scripting languages such as Python Demonstrated experience in a cloud-based -computing environment such as AWS, Azure, or Google Cloud Platform Big data processing techniques, preferred Can work independently in ambiguous environment About Logic20/20. . . Logic20/20 is one of Seattle’s fastest-growing consulting firms. We hire remarkable people to create simple, efficient solutions for complex problems. Although we make it look like magic, our success is due to our approach (methodical and structured) and the people we hire (smart, motivated, and team oriented). Together, these enable us to consistently exceed client expectations—and our reputation is growing. For the past five years, we’ve placed in the top ten of Seattle Business magazine’s “Best Companies to Work For”. From engaged leadership and wide-ranging benefits to career mentorship and diverse internal opportunities, we pride ourselves on being one of the best companies to work with and work for. We hire people that are self-motivated, comfortable conceiving strategies on the fly, and enjoy working individually and as part of a team. Our work is high-energy and demanding, but new hires will quickly feel at home among colleagues as friendly and focused as they are. We bring our best to every opportunity, driving change in industries across the West Coast. Join us and you can, too.",3.5,"Logic20/20
3.5","San Francisco, CA",-1,201 to 500 Employees,2005,Company - Private,Consulting,Business Services,$25 to $50 million (USD),-1
Data Engineer - Cloud (Remote),$121k-$223k (Glassdoor Est.),"At CrowdStrike we’re on a mission - to stop breaches. Our groundbreaking technology, services delivery, and intelligence gathering together with our innovations in machine learning and behavioral-based detection, allow our customers to not only defend themselves, but do so in a future-proof manner. We’ve earned numerous honors and top rankings for our technology, organization and people – clearly confirming our industry leadership and our special culture driving it. We also offer flexible work arrangements to help our people manage their personal and professional lives in a way that works for them. So if you’re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to stop breaches and protect people globally, let’s talk.

About the Role:
The data engineer in the cloud security product group will take a pivotal role in a hyper-scale data platform and pipeline for real-time security detections. This is a hands-on data engineering role that spans design and development for both sql and no-sql databases as a foundation of core cloud security capability.

Job Responsibilities

Design, develop and maintain a data platform that data pipeline at scale.

Participate in technical reviews of our products and help us develop new features and enhance stability

Continually help us improve the efficiency of our services so that we can delight our customers

Help us research and implement new ways for both internal stakeholders as well as customers to query their data efficiently and extract results in the format they desire

Qualifications For Data Engineer
We are looking for a candidate with a BS and 5+ years or MS and 3+ years in Computer Science or related field. They should also have experience with the following software/tools -

A solid understanding of algorithms, distributed systems design, and the software development lifecycle

Extensive experience with Graph Data design and development.

Solid background in Java/Scala and hands-on experience of building large data streaming pipelines (i.e., Spark, Kafka or others)

Extensive experience with the Apache Hadoop ecosystem including Hive, Presto, etc.

Extensive experience with relational SQL including Postgres. NoSQL including Cassandra.

Experience with AWS or GCP tools (i.e., EMR, BigQuery, EC2, Lambda, S3, etc.) is a plus.

Good test-driven development discipline

Reasonable proficiency with Linux administration tools

Proven ability to work effectively with remote teams

Experience With The Following Tools Is Desirable

Kubernetes

Jenkins

Parquet

Protocol Buffers/GRPC
#LI-DK1
#LI-Remote

Benefits of Working at CrowdStrike:
Market leader in compensation and equity awards
Competitive vacation policy
Comprehensive health benefits + 401k plan
Paid parental leave, including adoption
Flexible work environment
Wellness programs
Stocked fridges, coffee, soda, and lots of treats

We are committed to building an inclusive culture of belonging that not only embraces the diversity of our people but also reflects the diversity of the communities in which we work and the customers we serve. We know that the happiest and highest performing teams include people with diverse perspectives and ways of solving problems so we strive to attract and retain talent from all backgrounds and create workplaces where everyone feels empowered to bring their full, authentic selves to work.
CrowdStrike is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.

CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",3.9,"CrowdStrike
3.9","Sunnyvale, CA",-1,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$100 to $500 million (USD),-1
"Senior Data Engineer, Ingestion",$121k-$218k (Glassdoor Est.),"Chime is the largest and fastest-growing player in the challenger-banking space, providing mobile and online banking technology in the U.S. on behalf of partner banks and facilitating over 10M accounts with no physical branches. We're a technology company relentlessly focused on helping our members achieve financial peace of mind. That's why we offer access to an award-winning bank account that doesn't charge a ton of traditional bank fees, can give members early access to their paychecks, and enables members to grow their savings automatically. And we're just getting started. We are proud of our mission, devoted to our members, and passionate about applying technology to the challenge of making financial health a reality for everyone.

We have one of the most experienced management teams in Fintech and have raised over $800M in funding from DST, General Atlantic, Iconiq, Coatue, Dragoneer, Menlo, Access, Forerunner, and others. If you're looking to join a fast-growing company with a beloved, daily-use product and an authentic mission that puts people first, we want to meet you.

About the Role

In this role, you'll be responsible overseeing the design, development and operations of large-scale, real-time data systems. The ideal candidate will be excited by the prospect of owning, optimizing or even re-designing our company's data architecture and building out a team to support our next generation of products and data initiatives. The data engineering team will support our software developers, database architects, data analysts, data scientists, and machine learning engineers on back-end data initiatives.

Responsibilities

We're looking for a leader who is self-directed and comfortable supporting the data needs of multiple teams, systems and products. Your responsibilities will include:
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies
Work with stakeholders including the Analytics, Risk, Machine Learning, and Technical Operations teams to assist with data-related technical issues and support their data infrastructure needs
Participate in key technical and design discussions with technical leads in the team as a hands-on manager
Act as a project manager for the projects that your team is responsible for
Provide technical leadership to your team by giving guidance on designs and coding when time allows
Help manage and consult in designing our data governance initiatives
Requirements
6+ years of experience in designing, implementing, optimizing and operationalizing real-time big data analytics systems including data-pipelines, data warehouses and enterprise wide data-flows
You've earned a bachelor's degree in Computer Science or other technical field
Understanding of the tradeoffs between different big data solutions
Strong analytic skills related to working with unstructured datasets
Experience with a variety of traditional, streaming, and big data tools such as:
Hadoop ecosystem: Hadoop, Hive, Spark
Kafka, Sqoop, Flume
Airflow or other workflow scheduler
Data Warehouses: Redshift, Snowflake (preferred)
SQL databases, including MySQL, Postgres
AWS cloud services: EC2, EMR, RDS
Stream-processing systems: Storm or Spark-Streaming or equivalent.
Advanced SQL, Python, Java and/or Scala
What we offer
Competitive salary based on experience, with medical and dental benefits.
Free snacks and drinks, plus weekly catered lunches. (when back in office!)
Flexible vacation policy.
Monthly happy hours and company events.
A challenging and fulfilling opportunity to join one of the most experienced teams in FinTech and help create a completely new kind of banking service.
We know great work isn't done alone. We're building a team of individuals to Chime in with their different strengths to benefit our employees and members. We strongly believe that different backgrounds and ideas are a competitive advantage; we hire candidates of any race, color, ancestry, religion, sex, national origin, sexual orientation, gender identity, age, marital or family status, disability, Veteran status, and any other status. Chime is proud to be an Equal Opportunity Employer and will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance. If you have a disability or special need that requires accommodation, please let us know. To learn more about how Chime collects and uses your personal information during the application process, please see the Chime Applicant Privacy Notice.",4.8,"Chime
4.8","San Francisco, CA",-1,501 to 1000 Employees,2013,Company - Private,Banks & Credit Unions,Finance,Unknown / Non-Applicable,-1
Entry level Data engineer,-1,"Company Description Our company is a leading IT services, consulting and outsourcing company delivering exceptional business solutions and customer value to its customers worldwide. We offers end-to-end innovative and leading edge solutions to help corporations leverage technologies, outperform their competition and acquire sustainable growth. We have proven capabilities in new and emerging technologies and extensive experience across a broad range of industries and domains which enable us to deliver world-class, secure, scalable and reliable business systems. We have achieved many distinctions and milestones of outstanding success during the course of its evolutionary journey. Job Description Expand and optimize our data collection and data pipeline architecture Create data tools for analytics and data scientist team members Monitor and support data pipelines and ETL workflows Cloud (Azure) infrastructure administration: network configurations, access and permissions, cloud services Oversee, participate in and manage production deployments. Interface with global vendor and internal teams. Ensure cloud environment reliability, availability, performance and security Define and implement monitoring and alerting for cloud infrastructure and applications Qualifications null Additional Information All your information will be kept confidential according to EEO guidelines.",-1,Cloudbigdata Technologies,"San Francisco, CA",-1,-1,-1,-1,-1,-1,-1,-1
Data Engineer,$100k-$186k (Glassdoor Est.),"Overview

We are looking for a Data Engineer to work with a team of Machine Learning Developers in SIG’s new Machine Learning Team. This group will apply machine learning techniques to forecasting opportunities including time series applications, natural language understanding and more. The team will be responsible for spearheading the application of deep learning to our daily trading activity.

In this role, you will

Collaborate with other researchers, developers, and traders to improve existing strategies and conduct research and development supporting the application of new algorithms designed for scalable implementation
Use your software development and data engineering skills to build data sets, data quality metrics, and automation tools to enhance our research and system development

This work will be challenging, fast-paced, and competitive. Your interest and drive to apply cutting-edge machine learning techniques to large financial data sets will enable this team to expand quantitative research at SIG.

Founded in 1987, we are one of the top privately held trading firms globally. We have some of the best data resources in the industry, which includes petabytes of curated data from global markets with up to decades of coverage, and a world class infrastructure for working at this scale.

What we're looking for

Advanced degree in Computer Science, Statistics, Machine Learning, Physics, Applied Mathematics or related field
Strong object oriented programming skills
Experience developing scalable pipelines and model implementations suitable for high volume data sets
Interest in working on a team applying machine learning and deep learning in a professional research environment
Visa sponsorship is available for this position

SIG is a global quantitative trading firm founded with an entrepreneurial mindset and a rigorous analytical approach to decision making. We commit our own capital to trade financial products around the world. Building virtually all of our own trading technology from scratch, we are leaders and innovators in high performance, low latency trading. Our traders, quants, developers, and systems engineers work side by side to develop and implement our trading strategies. Each individual brings their unique expertise every day to help us make optimal decisions in the global financial markets.

For more information, please contact/send your resume to Mike Pachella at mike.pachella@sig.com.

We don’t post salary ranges externally so any salary estimate you see listed here was not provided by SIG and may not be accurate.

SIG is not accepting unsolicited resumes from search firms. All resumes submitted by search firms to any employee at SIG via-email, the Internet or directly without a valid written search agreement will be deemed the sole property of SIG, and no fee will be paid in the event the candidate is hired by SIG.",3.6,"Susquehanna International Group
3.6","Alameda, CA",-1,1001 to 5000 Employees,1987,Company - Private,Stock Exchanges,Finance,Unknown / Non-Applicable,-1
Audio Data Engineer,$96k-$149k (Glassdoor Est.),"· Experience in manual testing · 4+ year of experience working as a Quality Assurance Engineer · Experience in automation testing · Bachelor’s degree in Computer Science, Computer Engineering, Electrical Engineering or equivalent · 3+ years of software testing experience · Experience testing or speech related features in consumer electronics products · Experience with instrumentation Amazon Lab126 is an inventive research and development company that designs and engineers high-profile consumer electronics. Lab126 began in 2004 as a subsidiary of Amazon.com, Inc., originally creating the best-selling Kindle family of products. Since then, we have produced groundbreaking devices like Fire tablets, Fire TV, and Amazon Echo. What will you help us create? Work hard. Have fun. Make history. The Role: As an Data Engineer you will: · Closely work with our SDE and Researchers to capture requirements and execute on collections of data that will be used for the development and performance qualification of our products · Closely work with our software and teams to perform quality assessments · Work relentlessly to identify opportunities for continuous improvement and task automation · Interpret results and publish findings, and be a vocal proponent for quality in every phase of the product qualification process. · Demonstrated product experience (consumer electronics or automotive) · Experience with agile methodology · Experience with at least one object oriented Language · Experience with scripting languages (, Bash) · Hands-on experience in testing Linux and based devices · Familiarity with data collection paradigms · Familiarity with quality assessments · Exposure to concepts: acoustics, psycho-acoustics, signal processing, and sound reproduction · Exposure to speech recognition · Highly methodical approach to testing · Creativity and initiative to drive improvements in product performance assessments Lab126 is part of the Amazon.com, Inc. group of companies and is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age #LI-AN1",3.9,"Amazon.com Services LLC
3.9","Sunnyvale, CA",-1,10000+ Employees,1994,Company - Public,Internet,Information Technology,$10+ billion (USD),-1
Data Engineer,-1,"Nyansa is a fast-growing innovator of advanced IT infrastructure analytics software based in Palo Alto, California. Founded in September 2013 by technology professionals from MIT, Meraki, Aruba Networks and Google, Nyansa is credited with developing the first cloud sourced, vendor-agnostic network analytics and IoT security platform, called Voyance. We embrace simplicity and take following to heart on everything we do: ""Any intelligent fool can make things bigger, more complex, and more violent. It takes a touch of genius - and a lot of courage - to move in the opposite direction."" - Einstein Nyansa is looking for a data engineer to join the team that is building a new, vendor-agnostic IT network analytics service purpose built for CIOs, network operations and helpdesk personnel managing heterogeneous enterprise environments. Our product is focused on the end user experience by helping IT staff gain new insights into client access conditions, network service behavior and enterprise applications issues that impact user performance. Our current big data analytics system analyzes billions of streaming events per day using advanced algorithms. Going forward, we aim to scale the system extensively and are looking for radical ideas to achieve this. The company is well funded and provides competitive compensation package, stock options, benefits, catered lunch, and a fun work environment. We’re located within a 1 min walk from the Palo Alto Caltrain station. Responsibilities: • Design and develop highly scalable and available real time analytics platform using Spark, Kafka, Cassandra, and Elasticsearch for large data input streams • Work closely with data science and UI teams to define and implement various analytics features related to product • Configure, monitor, and optimize Spark and related infrastructure Requirements: • Strong desire to work for an early stage startup and be a part of its success • Strong in Map-Reduce, parallelizing computations, and identifying bottleneck computations • Strong in Scala and Python • Experience in configuring and tuning Spark and Kafka systems • Good understanding on Spark UI to extract useful information on application stages, and identify bottlenecks • B.S. or higher degree in Computer Science or equivalent Pluses: • Experience with Cassandra, Elasticsearch, Mongo • Experience with Ganglia and able to correlate information from various UIs to diagnose efficiency issues • Experience working with AWS • Experience with Spray to build REST endpoints",3.9,"Nyansa
3.9","Palo Alto, CA",-1,1 to 50 Employees,2013,Company - Private,Enterprise Software & Network Solutions,Information Technology,$1 to $5 million (USD),-1
Data Engineer,-1,"Organization Description
Tides is a philanthropic partner and nonprofit accelerator dedicated to building a world of shared prosperity and social justice. Tides works at the nexus of funders, changemakers, and policy, with extensive impact solutions including philanthropic giving and grant making, impact investing, fiscal sponsorship for social ventures, collaborative workspaces, and policy initiatives. Our extensive tools and know-how give our partners the freedom to hit the ground running and drive change faster than they can on their own. For more information, please visit www.tides.org.

Position Summary
We are seeking a service-oriented and self-motivated professional to join Tides’ IT team. In this role, you will be responsible for managing our data architecture, optimizing our data flows between existing systems, and working with corss functional teams around continuous improvements. You will be tasked with maintaining our ERDs, helping map and support our integrations, data security, and data governance requirements.

Essential Functions
• Map existing data models on SaaS and internal databases and systems as required.
• Manage change management across systems integrations, workflows, and automations to assure proper data management and protections are stabilized.
• Partner with all Tides departments on refining data needs and contribution across the systems, process, and people continuous improvement model.
• Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Drive data classification, handling, and security efforts across the organization and with our partnerships.
• Remain effective and process-oriented with great documentation skills.

Knowledge, Skills, and Abilities
• Strong organizational, problem-solving, and analytical skills; ability to manage situations, reprioritize, and meet demanding deadlines
• Strong analytical skills around datasets and correlation
• Project management skills around scoping, timelines, and expectation management
• Demonstrated experience around data mapping, ERD development, and integrations management
• Excellent verbal and written communication skills
• Ability to work collaboratively with a diversity of individuals at all organizational levels
• SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Excellent interpersonal skills with proven ability to develop effective working relationships with individuals possessing a variety of communication styles in a multicultural environment
• Ability to make timely and sound decisions and maintain confidentiality
• Experience working for and/or with nonprofits is a plus
• Proven ability to provide exceptional customer service to a variety of clients with varying degrees of technical expertise
• Ability to diagnose and resolve wide-ranging technical issues
• Proﬁciency in English
• Experience with schema design and dimensional data modeling
• Ability in managing and communicating data warehouse plans to internal clients
• Experience designing, building, and maintaining data processing systems

Education and Experience

• 3+ years of experience in a data management role
• Bachelor’s Degree in Computer Science, Statistics, Informatics, Information Systems, or a similar field
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Experience building with big data tools and analytics systems

Application Instructions
Please submit a resume and a thoughtful cover letter online. Your cover letter should express your interest in working for Tides and your qualifications for the role. You may also share your detailed LinkedIn profile with us. Tides is an Equal Opportunity employer. We value diversity and inclusion and we look forward to reviewing applications from all who are qualified to apply.

Equal Employment Opportunity
Tides is an equal opportunity employer. We strongly encourage applications from women, people of color, and bilingual and bicultural individuals, as well as members of the lesbian, gay, bisexual, and transgender communities. Applicants shall not be discriminated against because of race, religion, sex, national origin, ethnicity, age, disability, political affiliation, sexual orientation, gender identity, color, marital status, or medical condition including acquired immune deficiency syndrome (AIDS) and AIDS-related conditions. Also pursuant to the San Francisco Fair Chance Ordinance, we encourage and will consider for employment qualified applicants with arrest and conviction records.

Applicants with Disabilities
Reasonable accommodation will be made so that qualified disabled applicants may participate in the application process. Please advise in writing of special needs at the time of application.",-1,The Tides,"San Francisco, CA",-1,1 to 50 Employees,-1,Unknown,"Hotels, Motels, & Resorts",Travel & Tourism,Less than $1 million (USD),-1
Lead Data Engineer,-1,"Are you up to the challenge of helping solve one of the world's largest problems of food waste? Recent research has found that an incredible 30-40% of food is wasted every year around the globe. Full Harvest is a high-growth, VC-backed AgTech startup solving this food waste problem. We've built the B2B marketplace connecting farms with food & beverage businesses to sell ugly and surplus produce that would have otherwise gone to waste. We are rapidly growing the team, after raising a $8.5 million Series A, led by Spark Capital. Full Harvest has been recognized in major publications including TechCrunch, WSJ, Forbes, the Economist and featured on Bloomberg. We have received the United Fresh Award for Innovation and our CEO, Christine Moseley was named '#2 Most Innovative Women in Food and Drink' by Food & Wine Magazine. The Role You will be the founding member of our data engineering team, and will own and implement our data collection and reporting systems across all levels of the stack. Reporting to the Head of Engineering, you will be working with a highly interdisciplinary team of engineers, product managers, analysts, and marketers. This role will require a mixture of data engineering, customer insight, analytical mindset, and strong collaboration skills to be successful. Our headquarters are in downtown San Francisco. Responsibilities: Help architect, build, and scale our data platform. Through collaboration with different departments, define critical metrics for our business, and build out data marts to accurately report them. Gather insights from complex data, and make recommendations that drive meaningful business impact. Build data pipelines to aggregate data across various internal subsystems. Drive automation for self-service insights that create leverage across departments (Sales, Marketing, Product, Finance, Operations). What we are looking for: Exceptional track record in delivering production grade data engineering solutions at all levels of the stack. Strong data modeling fundamentals (relational and non-relational). The ability to quickly manipulate complex data through SQL, python, R (or other similar languages). Practical experience building out a data warehousing ecosystem. Expertise in data analysis and visualization tools (Looker, Tableau, D3 etc). A highly analytical mindset. How We Operate: As a team, we are focused on our mission of empowering sustainability solving food waste and we strive to do this by executing in the following ways: We are results focused and rely on each other to accomplish our goals We deliver consistently strong performance We go to great lengths to understand and delight customers We give and receive candid and direct feedback We admit mistakes openly and are not afraid to ask for help We look for solutions in unexpected places What We Offer: Opportunity to work on a great mission - solving food waste A fun, challenging environment that will give you the chance to significantly grow and learn Equity in a high-growth startup The most up-to-date technology, including company-issued Macs, the latest software and other tools needed to excel at your job Medical, Dental and Vision coverage 401k plan Commuter Benefits In-Office Lunch several days a week Looking for more information about us? We recently closed our Series A with Tier 1 tech and ag firms, Spark Capital and Cultivian Sandbox. Learn more in this TechCrunch article. Full Harvest is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status or disability status.",-1,Full Harvest,"San Francisco, CA",-1,1 to 50 Employees,-1,Company - Private,Wholesale,Business Services,Less than $1 million (USD),-1
Computing Systems Data Engineer,$95k-$169k (Glassdoor Est.),"Berkeley Lab’s Computational Science Department has an immediate opening for one or more Computing Systems Data Engineers to assist in the implementation and documentation of cross-facility workflows in the Physics Sciences. Experimental and observational science pipelines are increasingly turning to supercomputing resources like the National Energy Research Scientific Computing Center (NERSC) to handle their large-scale data analysis. Many of these pipelines serve experiments that are running 24/7, and must either shutdown or find alternative locations for their real-time data analysis during NERSC’s necessary maintenance and outages. Berkeley Lab’s Computational Research Division is conducting a research program to run these workflows at multiple computing sites. In this process, we will identify the pain points and key future research topics for automated workflow migration, and lead the way for a future where fully automated workflows can run across the Department of Energy (DOE) complex and the commercial cloud. ""We value and strive for diversity in backgrounds, experiences, and perspectives"" What You Will Do: Collaborate with Berkeley Lab physicists and computer scientists to port their analysis software from NERSC, to Berkeley Lab’s Dirac compute cluster, Argonne Leadership Computing Facility, Oakridge Leadership Computing Facility and Amazon AWS. Investigate the use of container technology at these sites. Investigate data management strategies across multiple computing sites. Communicate clearly results to both domain scientists and computer systems engineers. What is Required: Bachelor’s Degree in Physics, Computational Physics, Chemistry, Biology,Computer Science, or Applied Mathematics; or a related field, and a minimum of five years of related experience. Strong programming background in multiple languages including C/C++, Python and shell scripting. Prior experience managing computational needs and resources, either as a user or as an administrator. Excellent communication and interpersonal skills. Desired Qualifications: A Master’s Degree and three years of related experience. Experience working with experiments that have real-time and/or large-scale computing requirements. Experience with HPC systems, including use of a batch scheduling system. Understanding of Big Data-type problems, including system I/O, data transfer and containers. Software development experience, including version control and debugging. Prior experience in working on interdisciplinary projects involving multiple stakeholders and institutions. The posting shall remain open until the position is filled. Notes: This is a full-time, 3 year, term appointment with the possibility of extension or conversion to Career appointment based upon satisfactory job performance, continuing availability of funds and ongoing operational needs. This position will be hired at a level commensurate with the business needs, skills, knowledge, and abilities of the successful candidate. This position may be subject to a background check. Any convictions will be evaluated to determine if they directly relate to the responsibilities and requirements of the position. Having a conviction history will not automatically disqualify an applicant from being considered for employment. Work will be primarily performed at Lawrence Berkeley National Lab, 1 Cyclotron Road, Berkeley, CA. Learn About Us: Working at Berkeley Lab has many rewards including a competitive compensation program, excellent health and welfare programs, a retirement program that is second to none, and outstanding development opportunities. To view information about the many rewards that are offered at Berkeley Lab- Click Here. Berkeley Lab (LBNL) addresses the world’s most urgent scientific challenges by advancing sustainable energy, protecting human health, creating new materials, and revealing the origin and fate of the universe. Founded in 1931, Berkeley Lab’s scientific expertise has been recognized with 13 Nobel prizes. The University of California manages Berkeley Lab for the U.S. Department of Energy’s Office of Science. Equal Employment Opportunity: Berkeley Lab is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, or protected veteran status. Berkeley Lab is in compliance with the Pay Transparency Nondiscrimination Provision under 41 CFR 60-1.4. Click here to view the poster and supplement: ""Equal Employment Opportunity is the Law."" Lawrence Berkeley National Laboratory encourages applications from women, minorities, veterans, and other underrepresented groups presently considering scientific research careers.",4.2,"Lawrence Berkeley National Laboratory
4.2","San Francisco, CA",-1,5001 to 10000 Employees,1931,Government,State & Regional Agencies,Government,$100 to $500 million (USD),-1
Data Engineer,$84k-$156k (Glassdoor Est.),"What if you could support the solutions that will change the way the world communicates? What if you had the freedom of a startup and the resources of a global enterprise? You’d break new ground. Raise the bar for performance. And do career-defining work. That’s exactly the kind of environment we’re building together at RingCentral. RingCentral's cloud-based communications platform connects more than 2 million users around the world, in ways that bring people, ideas, companies and customers together. As a member of the RingCentral Cloud Operations Data Operations Team , you'll will be part of the team that is responsible for building out the architecture for data ingestion. You will take responsibility for all things related to system maintenance, application support, deployment and pipeline engineering and support. We’re as proud of our working environment as we are of our market success. You’ll find all the training, opportunity and resources you could ever want here - with all the work/life benefits you expect, and none of the micromanagement. RingCentral regularly brings home Best Place To Work awards from locations all over the world, and outstanding company ratings on Glassdoor and Comparably! RingCentral surrounds you with world-class technology and talent, in a people-first environment built from the ground up to help you do the best work of your career. We’re not just changing the nature of communication and teamwork. We’re winning, together. Data Engineer: (Belmont, Denver, or remote) We are looking to hire a Data Engineer to join our Data Operations team. You will be part of the team that is responsible for building out the architecture for ingestion. You will take responsibility for all things related to system maintenance, application support, deployment and pipeline engineering and support. This includes learning and understanding upstream processes, pipelines and source systems. The role will work in conjunction with other cross functional teams to help derive analytics and dashboards. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. Responsibilities for Data Engineer Architecting, Design, Building, Supporting systems for data ingestion Work with a cross functional team of engineers, analysts and scientist to understand business requirements Design and build data pipelines from various sources to data warehouse. Documenting architecture, systems, and pipelines. Documenting database design including data modeling, metadata and process flow diagrams. What we’re looking for 4+ years Systems Administration experience 4+ years of Application Administration experience Experience with Kubernetes, Docker, Kafka, Redis, Hazelcast Programming ( Java, Python, Groovy ) Has experience with ANSI SQL Has experience with NOSQL ( Elastic, Clickhouse ) Ability to create and maintain data pipelines Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Participate in on-call rotation to support data pipelines and tools · Qualifications for a Data Engineer B.S in Computer Science etc. plus 2-3 years of experience Experience with management data pipelines/big data tools: StreamSets, Airflow, ELK etc Experience with one of AWS/Google public clouds Programming experience (at least one): Python, Java, etc Nice to have devops experience: Ansible, Jenkins, Chef etc Nice to have Anomaly Detection/Machine Learning experience Nice to have Telecom background Ability to work in very diverse multicultural environment Good communication skills Good team player with self-starter ability About RingCentral RingCentral is the worldwide leader in cloud-based communications. Our software communications platform delivers phone, group chat, mobile communications, video calls, videoconference, contact center and AI-driven digital engagement. It’s a powerful, global presence that allows businesses to communicate anywhere, anytime with anyone. RingCentral is headquartered in Belmont, California and has offices around the world. RingCentral is an equal opportunity employer that truly values diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",4.6,"RingCentral
4.6","Belmont, CA",-1,5001 to 10000 Employees,2003,Company - Public,Telecommunications Services,Telecommunications,$1 to $2 billion (USD),-1
Senior Data Engineer,-1,"WHO WE ARE

TapResearch was founded to help any business access critical market data about their products and services. We're a mission driven organization looking to empower any company of any size, to access the critical market insights they need to make better real time data driven decisions. Our Audience Network connects tens of millions of people with surveys in the apps and games they use everyday. We are growing by more than 100,000 New Users per day & are currently live in 25+ Countries.

Hundreds of companies, like Qualtrics, Zynga and comScore rely on our technology to collect consumer opinions across thousands of mobile apps and games from publishers such as MiniClip, Jam City, Atari, PeopleFun and many others. By connecting marketers & researchers with everyday mobile users, we are building the next generation of research products.

We're motivated by the size of the opportunity and the impact we make on our customers' organizations. Recently named one of Inc.'s Fastest Growing Companies in both 2018 and 2019, we are a fast-moving, rapidly growing, profitable company & would love for you to join us.

OUR VALUES

The TapResearch team culture is both informal and biased towards action. We encourage individuality, curiosity, diverse thinking and innovation. You will be motivated, enterprising and enthusiastic. Company culture is such that you need to be able to ""check your ego at the door"" and possess a high level of integrity. You thrive in a fast moving start-up & hands on environment and have the ability to take on project ownership & Idea generation. We value transparency and are committed to building an inclusive work environment where everybody grows and succeeds together.

THE ROLE

You are a strong data engineer with extensive experience in data architecture, data modeling, and data management. You are passionate about building a strong data foundation for TapResearch. You will be working cross-functionally with product management, business stakeholders, and application engineers to build data pipelines, infrastructure, and models that empower data-driven decisions.

You are both an independent and a good team player, a great problem solver, and have a strong commitment to writing quality, efficient software. You will collaborate with a highly autonomous team of talented individuals and your work will have a high impact on tens of millions of users starting on day one.

WHAT YOU'LL DO

Work with cross-functional teams to produce high performance, scalable data engineering solutions.
Design, build, and monitor pipelines to deliver data with measurable quality.
Partner with business stakeholders to transform raw data into actionable data models and insights.
Lead the overall strategy for data governance, quality, and retention that will satisfy business policies and requirements.

YOUR PROFILE

5+ years experience with data engineering
Proficient with MySQL and relational datastores.
Proficiency in at least one scripting language. Python preferred.
Experience with Cloud-based service and development environment, such as AWS or GCP
Knowledge of data warehousing concepts
Experience with Periscope or other business intelligence tools is a plus
Previous startup experience strongly preferred

TECHNOLOGY

We use a variety of open source languages and frameworks - Ruby on Rails, ReactJS, iOS, Android, Unity
We are also reliant on AWS, MySQL, Redis, Redshift, Kibana, Kinesis, RabbitMQ, Terraform, and Capistrano.

ADDITIONAL BENEFITS

Along with competitive compensation, TapResearch offers first-class benefits to all of its employees including:

Unlimited PTO.
100% Healthcare Premium Reimbursement.
Stock options.
New Macbook, company swag kit.

EQUAL EMPLOYMENT OPPORTUNITY

TapResearch is an Equal Employment Opportunity (EEO) employer, and is committed to equal opportunity in our recruitment and selection process without regard to race, gender, age, color, religion, ancestry, disability, medical condition, national origin, sexual orientation, marital status, veteran status, genetic information, or any other characteristic protected by federal, state or local law.",5.0,"TapResearch
5.0","Menlo Park, CA",-1,1 to 50 Employees,2013,Company - Private,Enterprise Software & Network Solutions,Information Technology,$10 to $25 million (USD),-1
"Data Engineer, Infrastructure",$70k-$129k (Glassdoor Est.),"The Opportunity at Komodo Health: You will be a member of our Infrastructure Team. You will have the opportunity to use the latest and greatest tools to make sure the the data infrastructure is robust and scalable. As one of the members of this business-critical team, you will: Optimize Spark Infrastructure Building out big tooling to interact with Big Data infra Evaluate data technologies, such as: Delta lake (Data Lake) Apache Presto (SQL Query Engine) What You Bring to Komodo: You possess a sense of curiosity and the ability to think thoroughly about problems and develop robust solutions. You're passionate about about big data technologies and architecting scalable infrastructure. You're able to communicate effectively, work cross-functionally and provide technical leadership You have a demonstrated track record of success and engineering excellence You possess a strong DevOps and partnership mindset You're humble, respectful, and appreciate the diversity within our Engineering organization After 3 months, you will: Obtain certifications (AWS, Spark, etc) Get familiar with our tools and infra and contribute (Spark Optimization) Improve logging and monitoring on Spark and Airflow Clusters Responsibilities: Work with senior engineers to help architect and implement infrastructure platforms Develop internal tooling to interact with our infrastructure Automate infrastructure with languages and tools such as Helm, Terraform, Packer, CloudFormation, Python, and Bash Work deeply with AWS compute, storage, and analytics services Build and maintain Docker images used by our Kubernetes and Spark clusters Collaborate with our data scientists and data engineers to understand their use cases, needs, and pain points and translate that into infrastructure that accelerates them Select the key tools and systems that will be used to build out the infrastructure Setup monitoring, logging, and alerting on our big data systems Respond to and troubleshoot issues and incidents with Kubernetes and Spark Willingness to participate in on-call rotation Qualifications: B.S. in Computer Science, Software Engineering, or equivalent experience 1+ year of experience with big data technologies such as Spark 1+ year of Docker, Kubernetes, and container management 1+ years of working with cloud infrastructure, especially AWS You have excellent programming skills in Python, C++, or other programming languages Self-motivation to learn about the big data ecosystem Nice-to-Haves: You have experience working in a regulated environment especially HIPAA and healthcare data You have experience building machine learning production systems at scale You have experience with Elastic Map Reduce and AWS Glue AWS certifications Benefits: Competitive salary Performance bonuses Unlimited PTO Equity 401K Plan Wellness Stipend Your choice of equipment Comprehensive Vision, Medical, and Dental insurance Pre-tax commuter benefits Community involvement through our philanthropy group, Komodo Cares Awesome office locations in San Francisco (SoMa) or New York City (Flatiron) The opportunity to help scale a team and company and work with smart people",3.5,"Komodo Health
3.5","San Francisco, CA",-1,201 to 500 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
"Senior Data Engineer (Match.com LLC, Palo Alto, CA)",$92k-$164k (Glassdoor Est.),"Support existing data pipelines and systems in production. Design and scale data ingestion systems with high availability and reliability. Design, build and launch new data extraction, transformation and loading processes in production. Design, build and launch new data models in production. Write batch and real-time big data jobs using Hadoop and Spark. Maintain and evolve dimensional data models and schema designs. Create automated reports to be consumed by the rest of the organization. Build machine learning models and derive insights from massive amounts of data. Use strong communications skills (oral and written) to interact with engineers and product managers to support Tinder product decisions. Scale one of the fastest growing data warehouse’s in mobile.
Minimum Requirements: Bachelor’s degree or U.S. equivalent in Computer Science, Computer Engineering, Software Engineering, Computational Data Science, or a related field, plus 3 years of professional experience utilizing big data technologies to support data ingestion and extraction to facilitate data driven decision making. Must also have the following: 3 years of professional experience developing new data models and schema designs using programming languages (including Scala, Python, or Java); 1 year of professional experience designing and implanting ETL (Including Extract, Design, and Load) systems; 1 year of professional experience analyzing and utilizing large data sets to identify trend and troubleshoot issues using analytical tools; 1 year of professional experience working with engineers and product managers to support product decisions.
Please send resume to: Lauren Lozano, Match Group LLC, 8750 North Central Expressway, Suite 1400, Dallas, TX 75231. Please specify ad code KVLL. EOE. MFDV.
Why Match Group?

A lot of places say they change lives, but we actually do it. We’ve helped millions of people find love and happiness! Here are a few perks we have in store for you:

$1,500 annual training budget
100% employer match on 401k contributions
Specific COVID-19 allowance for home office set-up
Matched giving to qualified organizations
100% paid Parental Leave
Happy Hours and Company parties (right now they are all virtual, but still a ton of fun!)

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

Learn more about why our employees love working at Match!",3.8,"Match
3.8","Palo Alto, CA",-1,1001 to 5000 Employees,1999,Company - Public,Internet,Information Technology,Unknown / Non-Applicable,-1
Clinical Data Engineer,$101k-$178k (Glassdoor Est.),"Abbott is a global healthcare leader that helps people live more fully at all stages of life. Our portfolio of life-changing technologies spans the spectrum of healthcare, with leading businesses and products in diagnostics, medical devices, nutritionals and branded generic medicines. Our 107,000 colleagues serve people in more than 160 countries

Diabetes Care

At Abbott, we believe people with diabetes should have the freedom to enjoy active lives. That’s why we’re focused on helping people with diabetes manage their health more effectively and comfortably, with life-changing products that provide accurate data to drive better-informed decisions. We’re revolutionizing the way people monitor their glucose levels with our new sensing technology.

Our location in Alameda, CA currently has an opportunity for a Clinical Data Engineer.

WHAT YOU’LL DO

The Data Engineer will be responsible for providing analytics solutions that complement efforts towards clinical and economic evidence generation. The candidate will be responsible for working on complex problems in the field of implantable medical technology. Use statistical modeling, algorithmic, data mining, and visualization techniques. Collaborate effectively with internal stakeholders and cross-functional teams. Apply a breadth of tools, data sources and analytical techniques to answer a wide range of high-impact clinical questions and present the insights in concise and effective manner. Additionally, should be an effective communicator capable of independently driving issues to resolution and communicating insights to a non-technical audience. Will work on team with scientists and physicians to prepare scientific conference presentations and publications.

Responsibilities

Design and develop high-performance data architectures, which support data warehousing, real-time ETL, and batch big-data processing

Solve analytical problems and effectively communicate methodologies and results

Draw inferences and conclusions

Create dashboards and visualizations of processed data, identify trends, anomalies

Translate project needs and goals into a data driven analytical approach

Be accountable for the successful implementation and support of data analysis, data investigation for health economics and outcomes projects

Be accountable for the assessment of technical risks around data

Understand the context of big data and its implications around privacy

Work autonomously in a fast-paced environment

Work with a broad range of technologists and support personnel both within and outside of Abbott

Possess analytical skills with a solid foundation in: programming (R, SAS, Python, SQL, NoSQL) and data/database design

EDUCATION AND EXPERIENCE YOU’LL BRING

Required

Bachelors degree

Minimum of 9 years of related work experience with a complete understanding of specified functional area.

Comprehensive knowledge and application of business concepts, procedures and practices.

Preferred

Masters degree preferred.

Prior experience in R or SAS

Prior experience in data warehousing, data modeling, ETL and SQL
Prior experience with a major BI Tool such as Looker, MS BI Tool, Tableau, QlikView, BIRST, Cognos, SAP BI
Prior experience or working knowledge with one or more of the following tools and technologies: Spark, Clojure, Hive, Spark, Pig, Redshift
Interest in developing predictive analytics and machine learning techniques to solve complex problems.

WHAT WE OFFER

At Abbott, you can have a good job that can grow into a great career. We offer:

Training and career development, with onboarding programs for new employees and tuition assistance

Financial security through competitive compensation, incentives and retirement plans

Health care and well-being programs including medical, dental, vision, wellness and occupational health programs

Paid time off

401(k) retirement savings with a generous company match

The stability of a company with a record of strong financial performance and history of being actively involved in local communities

Learn more about our benefits that add real value to your life to help you live fully: www.abbottbenefits.com

Follow your career aspirations to Abbott for diverse opportunities with a company that provides the growth and strength to build your future. Abbott is an Equal Opportunity Employer, committed to employee diversity.

Connect with us at www.abbott.com, on Facebook at www.facebook.com/Abbott and on Twitter @AbbottNews and @AbbottGlobal.",3.7,"Abbott Laboratories
3.7","Alameda, CA",-1,10000+ Employees,1888,Company - Public,Health Care Services & Hospitals,Health Care,$10+ billion (USD),-1
Senior Data Engineer,-1,"Parkside Securities is simplifying global access to US markets through regulatory innovations and technology. We are a US-based broker-dealer allowing foreign citizens the ability to invest in US securities using their local currency offering low fees and no minimum investment amounts.

We are looking for an experienced Data Engineer that can work across multiple teams to own data processing and programming. The ideal candidate will have knowledge of functional programming languages such as Clojure or Scala and be willing to roll up their sleeves in a fast-paced startup environment.

Responsibilities

Design the architecture to perform financial data analysis and algorithm-based products including, but not limited to, optimizing stock brokerage operations, portfolio management, as well as customer behavior analytics.
Create and maintain optimal data pipeline architecture
Work closely with server-side, Cloud Operations, Infrastructure, and front-end engineers
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.
Work with stakeholders including Executives, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.

Requirements

5+ years of data processing and analytics programming
Knowledge of Python to translate the proof of concepts
Experience developing data processing apps in Clojure would be ideal, but other functional languages such as Scala is acceptable
Knowledge of Spark to design, develop, and maintain
Hands-on experience with AWS Analytics stack such as Redshift, EMR, Athena, Glue, etc.
Experience developing ETL ( Extract, Transform and Load) Data pipelines
Experience with real-time streaming data processing
Experience with implementing clustered/distributed/multi-threaded infrastructure to support Machine Learning processing

Technology Stack used in core application development

AWS
Terraform
Kubernetes
Docker
Clojure
DynamoDB
Apache Kafka
Datomic
GitHub
MacOS/Linux for development",5.0,"Parkside
5.0","San Francisco, CA",-1,1 to 50 Employees,2016,Company - Private,Brokerage Services,Finance,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"Enlitic is on the verge of transforming patient care by bridging human and artificial intelligence to identify medical issues earlier and more accurately. We were founded in San Francisco in 2014 and our team has since expanded to New York, Vancouver, Canada, and Melbourne, Australia. We believe that our ground-breaking work will help save time, money, and most importantly, lives - a belief shared by our investors who have fueled our work with $55 million to date. We have also garnered the attention of CNN, The Wall Street Journal, The New York Times, Inc. and Fortune among other global media outlets, and we were named one of the smartest AI companies by MIT Tech Review. Enlitic is seeking an experienced Data Engineer to join the Engineering team to support next generation AI enhanced medical diagnostics. The engineering team builds large distributed components that run Enlitic's platform. The Enlitic platform strives to redefine performance in terms of scale and latencies. The engineering team does everything from designing scalable data storage, to building out software components and internal infrastructure tools. We are looking for candidates who share a passion for tackling complexity and building platforms that can scale through multiple orders of magnitude. The exciting things you will get to do: Design and manage large scale data systems Define and collaborate on data schema construction and implementation Work closely with Backend Engineering and Modeling to optimize data ingestion, indexes, and data architecture and schema design Analyze and improve efficiency, scalability, and stability of various system resources Support remote data ETL for deployments to partners and clients Ability to collaborate on tackling complex problems on a mission driven product The experience and qualifications we hope you bring to the position: Bachelor's degree in computer science or related field, or equivalent practical experience 5+ years coding experience in Python or another language with a desire to work with Python predominantly 5+ years experience architecting/managing big data stores (Petabytes) Experience managing PostgreSQL databases Proven experience in ETL and data optimization 2+ years' experience with big data processing tools such as Apache Spark, MapReduce, Hive, HDFS, or Hadoop Bonuses Hands-on experience working with containers e.g. Kubernetes and Docker Experience working with Cloud Computing platforms e.g. AWS Working knowledge of data visualization tools such as Jupyter Notebooks, D3.js, or Plotly Experience with Apache Airflow We celebrate diversity at our company and are proud to be an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, gender identity, or disability status. We understand that to hire the best talent, we need to search the globe. We encourage our employees to be themselves and require tolerance and acceptance among our diverse talent.",4.8,"Enlitic
4.8","San Francisco, CA",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Senior Data Engineer,$151k-$267k (Glassdoor Est.),"Mission

The Senior Data Engineer plays a key role in technological decision making for business teams future data, analysis, and reporting needs. The role supports the business’s daily operations inclusive of troubleshooting of the business’s data intelligence warehouse environment and job monitoring. It is also the role of the Senior Data Engineer to guide the business in identifying any new data needs and deliver mechanisms for acquiring and reporting such information as well as addressing the actual needs. The Senior Data Engineer is also tasked with gathering and maintaining best practices that can be adopted in big data stacking and sharing across the business. The Senior Data Engineer provides expertise to the business in the areas of data analysis, reporting, data warehousing, and business intelligence.

Outcomes

Design/Strategy: The Senior Data Engineer designs and supports the business’s database and table schemas for new and existing data sources for the data warehouse. Creates and supports the ETL in order to facilitate the accommodation of data into the warehouse. In this capacity, the Senior Data Engineer designs and develops systems for the maintenance of the business’s data warehouse, ETL processes, and business intelligence.

Collaboration: The role that the Senior Data Engineer plays is highly collaborative and, as such, works closely with data analysts, data scientists, and other data consumers within the business in an attempt to gather and populate data warehouse table structure, which is optimized for reporting. The Senior Data Engineer also works closely with other disciplines/departments and teams across the business in coming up with simple, functional, and elegant solutions that balance data needs across the business

Analytics: The Senior Data Engineer plays an analytical role in quickly and thoroughly analyzing business requirements for reporting and analysis and subsequently translating the emanating results into good technical data designs. In this capacity, the Senior Data Engineer establishes the documentation of reports, develops, and maintains technical specification documentation for all reports and processes.

Competencies

We are looking for a candidate with 8+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. In depth knowledge of Model and Design of DB schemas for read and write performance.
Extensive working knowledge of API or Stream based data extraction processes like Salesforce API and Bulk API is must.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with building data pipeline from various business applications like Salesforce, Marketo, NetSuite, Workday etc.
Experience with big data tools: Hadoop, Spark, Kafka, Spark & Kafka Streaming, Python, Scala, Talend etc.
Working knowledge of BI Tools like Tableau, Looker etc is plus

Benefits

Medical, dental, vision
401k Retirement Plan
Unlimited Paid Time Off
Gym reimbursement
Employee referral bonus program
Maternity and paternity plans

About Databricks

Databricks’ mission is to accelerate innovation for its customers by unifying Data Science, Engineering and Business. Founded by the original creators of Apache Spark™, Databricks provides a Unified Analytics Platform for data science teams to collaborate with data engineering and lines of business to build data products. Users achieve faster time-to-value with Databricks by creating analytic workflows that go from ETL and interactive exploration to production. The company also makes it easier for its users to focus on their data by providing a fully managed, scalable, and secure cloud infrastructure that reduces operational complexity and total cost of ownership. Databricks, venture-backed by Andreessen Horowitz, NEA and Battery Ventures, among others, has a global customer base that includes Viacom, Shell, and HP. .

Apache, Apache Spark and Spark are trademarks of the Apache Software Foundation.",4.7,"Databricks
4.7","San Francisco, CA",-1,1001 to 5000 Employees,2013,Company - Private,Computer Hardware & Software,Information Technology,Unknown / Non-Applicable,-1
Google Cloud Data Engineer,$84k-$156k (Glassdoor Est.),"Job Description Are you ready to step up to the New and take your technology expertise to the next level? Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements and the way we collaborate, operate and deliver value provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career. People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. As part of our Intelligent Software Engineering practice, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a growing network of technology experts who are highly collaborative taking on today’s biggest, most complex business challenges. We will nurture your talent in an inclusive culture that values diversity. Come grow your career in technology at Accenture! Google Cloud Platform (GCP) Data Engineers will be responsible for architecting transformation and modernization of enterprise data solutions on GCP cloud integrating native GCP services and 3rd party data technologies. A solid experience and understanding of considerations for large scale architecting, solutioning and operationalization of data warehouses, data lakes and analytics platforms on GCP is a must. We are looking for candidates who have a broad set of technology skills across these areas and who can demonstrate an ability to design right solutions with appropriate combination of GCP and 3rd party technologies for deploying on GCP cloud. Key responsibilities may include: Work with implementation teams from concept to operations, providing deep technical subject matter expertise for successfully deploying large scale data solutions in the enterprise, using modern data/analytics technologies on premise and cloud Experience in building solution architecture, provision infrastructure, secure and reliable data-centric services and application in GCP Work with data team to efficiently use Hadoop/Cloud infrastructure to analyze data, build models, and generate reports/visualizations Integrate massive datasets from multiple data sources for data modelling Implement methods for automation of all parts of the predictive pipeline to minimize labor in development and production Formulate business problems as technical data problems while ensuring key business drivers are captured in collaboration with product management Knowledge in machine learning algorithms especially in recommender systems Extracting, Loading, Transforming, cleaning, and validating data Designing pipelines and architectures for data processing Creating and maintaining machine learning and statistical models Querying datasets, visualizing query results and creating reports Basic Qualifications: Minimum 3 year of designing, building and operationalizing large-scale enterprise data solutions and applications using one or more of GCP data and analytics services in combination with 3rd parties - Spark, Cloud DataProc, Cloud Dataflow, Apache Beam, BigTable, Cloud BigQuery, Cloud PubSub, Cloud Functions, etc. Minimum 1 year of hands-on experience analyzing, re-architecting and re-platforming on-premise data warehouses to data platforms on GCP cloud using GCP/3rd party services Minimum 1 year of designing and building production data pipelines from ingestion to consumption within a hybrid big data architecture, using Java, Python, Scala etc. Minimum 1 year of architecting and implementing next generation data and analytics platforms on GCP cloud Minimum 1 year of designing and implementing data engineering, ingestion and curation functions on GCP cloud using GCP native or custom programming Minimum 1 year of experience in performing detail assessments of current state data platforms and creating an appropriate transition path to GCP cloud Hands-on GCP experience with a minimum of 1 solution designed and implemented at production scale Bachelor's degree or equivalent (minimum 12 years) work experience. If Associate Degree, must have minimum 6 years work experience Ability to meet travel requirements (100% Monday-Thursday) Preferred Qualifications: Minimum 1 year of experience in architecting large-scale data solutions, performing architectural assessments, crafting architectural options and analysis, finalizing preferred solution alternative working with IT and Business stakeholders 1 year of hands-on experience designing and implementing data ingestion solutions on GCP using GCP native services or with 3rd parties such as Talend, Informatica 1 year of hands-on experience architecting and designing data lakes on GCP cloud serving analytics and BI application integrations Minimum 1 year of experience in designing and optimizing data models on GCP cloud using GCP data stores such as BigQuery, BigTable Minimum 1 year of experience integrating GCP or 3rd party KMS, HSM with GCP data services for building secure data solutions Minimum 1 year of experience introducing and operationalizing self-service data preparation tools (e.g. Trifacta, Paxata) on GCP Minimum 1 year of architecting and operating large production Hadoop/NoSQL clusters on premise or using Cloud services Minimum 1 year of architecting and implementing metadata management on GCP Architecting and implementing data governance and security for data platforms on GCP Designing operations architecture and conducting performance engineering for large scale data lakes a production environment Craft and lead client design workshops and provide tradeoffs and recommendations towards building solutions 2+ years of experience writing complex SQL queries, stored procedures, etc Google Cloud Platform certification is a plus Professional Skill Requirements: Excellent communication (written and oral) and interpersonal skills Proven ability to work creatively and analytically in a problem-solving environment. Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture. Equal Employment Opportunity Statement Accenture is an Equal Opportunity Employer. We believe that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion or sexual orientation. Our rich diversity makes us more innovative, more competitive and more creative, which helps us better serve our clients and our communities. All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law. Accenture is committed to providing veteran employment opportunities to our service men and women. For details, view a copy of the Accenture Equal Opportunity and Affirmative Action Policy Statement Requesting An Accommodation Accenture is committed to providing equal employment opportunities for persons with disabilities or religious observances, including reasonable accommodation when needed. If you are hired by Accenture and require accommodation to perform the essential functions of your role, you will be asked to participate in our reasonable accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodations once hired. If you would like to be considered for employment opportunities with Accenture and have accommodation needs for a disability or religious observance, please call us toll free at 1 (877) 889-9009, send us an email or speak with your recruiter. Other Employment Statements Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration. Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process. The Company will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. Additionally, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company's legal duty to furnish information.",3.9,"Accenture
3.9","San Francisco, CA",-1,10000+ Employees,1989,Company - Public,Consulting,Business Services,$10+ billion (USD),-1
Data Engineer III -- 609 : PRODUCT ENGINEERING - CUSTOMER RELIABILITY : 20,-1,"Primary Skills: Data Engineer, Python, SQL, Linux, ETL Duration: 6+ Months with potential to extend Contract Type: W2 Only Job Description: We are looking for a Senior Data Engineer with advanced knowledge of SQL and intermediate knowledge of Python. Nice to have (but not required) beginner or intermediate level java experience. Your primary focus will be the writing complex SQL queries, optimizing them and development of all server-side backend data processing logic, ensuring high performance using Python and SQL. Responsibilities: Develop and maintain scalable ETL pipelines, build new pipelines and facilitate API integrations to support new requirements. Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging. Writing SQL functions, procedures as required based on the requirements Finetune or optimize queries to support the increasing volume of data. Debug Python code, modify and enhance Python ETL applications based on the requirements on Linux environment. Writing reusable and efficient code in Python and SQL. Write unit, functional, regression tests for enhanced feature, maintain engineering documentation. Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture. Skills: Basics of Computer Science - OOPS, Data Structures and Algorithms. Basic understand of regular Linux commands and usage. 5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries, function and stored procedures. 3+ years of experience with hands on experience in writing, debugging Python code on Linux. Experience writing python applications that interact with ORM (Object Relational Mapper) libraries. Knowledge of XML and JSON parsing with unit test and debugging skills. Willingness and ability to learn new tools/languages as needed. Process oriented with excellent oral and written communication skill with a desire for customer service. An excellent team player and communicator who can work effectively with cross functional teams and ability to navigate ambiguity. To follow up with any questions, please contact Ashutosh @ 408-512-2368 Akraya is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US. We offer comprehensive benefits including Health Insurance (medical, dental, and vision), Cafeteria Plan (HSA, FSA, and dependent care), 401(k) (enrollment subject to eligibility), and Sick Pay (varies based on city and state laws). If this position is not quite what you're looking for, visit akraya.com, and submit a copy of your resume. We will get to work finding you a job that is a better fit at one of our many amazing clients. Akraya is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender, race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law. Akraya is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.6,"Akraya Inc.
4.6","Sunnyvale, CA",-1,51 to 200 Employees,2001,Company - Private,Staffing & Outsourcing,Business Services,$25 to $50 million (USD),-1
"Senior Data Engineer, China",$122k-$215k (Glassdoor Est.),"About Airbnb China As an independent business unit, China is one of the major engines behind Airbnb's global growth. The company has increased its investment in China and experienced rapid growth since officially entering the market at the end of 2015. Airbnb continues to focus on product localization and innovation to better serve local users and enhance the user experience, nurture a trusting local community, and engage Chinese Millennial travelers with inspiring brand moments. As one of the leaders in the home sharing industry, Airbnb works with partners across many fields to actively promote the sustainable development of this emerging industry and achieve win-win results for everyone. Airbnb will continue to build an efficient local team, bringing the end-to-end, authentic travel experience to Chinese travelers, and creating a world where ""anyone can belong anywhere"". Why Data @ Airbnb? Quality data is fundamental to Airbnb's success. As a rapidly growing pre-IPO company, we are preparing for a future of tremendous growth and transformation. We are rebuilding our Data Engineering practice to enable the company's success by building a solid data foundation. We are seeking stunning Senior Data Engineers to help us define and realize our vision for trustworthy data across the company. This is a unique opportunity to join Data Engineering for a strong, but high potential, company early in its lifecycle. Like all teams at Airbnb, we value and promote the diversity of our workforce, our guests, our hosts, our marketplace platform, and the world. Simply put, you belong at Airbnb. What is Data Engineering at Airbnb China? We need to ensure every area of the business has trustworthy data to fuel insight and innovation. Understanding the business need, securing the right data sources, designing usable data models, and building robust & dependable data pipelines are essential skills to meet this goal. This is critical for a fast growing business like Airbnb China. We need to be fast to respond to the fierce competition in the China market but also need to be cautious on every decision we make according to the data. Data Engineers in Airbnb China need to better leverage the resources we have as an international company: being fast in building trustworthy data but also collaborating well with the data engineers in other business units to prevent reinventing the wheel. You will have the chance to learn the dynamics of the fast growing China market and also have tied relationships with other data engineers to redefine what it means to do Data Engineering in Airbnb. Responsibilities: Develop and automate large scale, high-performance data processing systems (batch and/or streaming) to drive Airbnb business growth and improve the product experience. Build scalable Spark data pipelines leveraging Airflow scheduler/executor framework Design our data models for optimal storage and retrieval and to meet critical product and business requirements. Understand and influence logging to support our data flow, architecting logging best practices where needed Contribute to shared Data Engineering tooling & standards to improve the productivity and quality of output for Data Engineers across the company Improve data quality by using & improving internal tools to automatically detect issues Minimum Requirements: 5+ years of relevant industry experience Bachelor's and/or Master's degree, preferably in CS, or equivalent experience Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutions Experience designing and deploying high performance systems with reliable monitoring and logging practices Working knowledge of relational databases and query authoring (SQL). Excellent communication skills, both written and verbal Bilingual in Mandarin and English Benefits: Stock Competitive salaries Quarterly employee travel coupon Paid time off Medical, dental, & vision insurance Life insurance and disability benefits Fitness Discounts 401K Flexible Spending Accounts Apple equipment Commuter Subsidies Community Involvement (4 hours per month to give back to the community) Company sponsored tech talks and happy hours Much more…",4.1,"Airbnb
4.1","San Francisco, CA",-1,5001 to 10000 Employees,2008,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Big Data Engineer,-1,"Developer/Big Data Engineer (Remote due to Covid)
Sunnyvale CA
6 months or longer contract
$85-90/hr on w2

JOB DUTIES:

Building Big Data Platforms that can ingest hundreds of terabytes of data, for Business Analytics, Operational Analytics, Text Analytics, Data Services. Build Back end analytical applications. Work on performance optimizations. Debug complex production scenarios.
Design and development of java, Scala and spark based back end software modules, performance improvement and testing of these modules.
Scripting using python and shell scripts for ETL workflow. Design and development of back end big data frameworks that is built on top of Spark with features like Spark as a service, workflow and pipeline management, handling batch and streaming jobs;
Build comprehensive Big Data platform for data science and engineering that can run batch process and machine learning algorithms reliably
Design and development of data ingestion services that can ingest 10s of TB of data;
Coding for Big Data applications on clickstream, location and demographic data for behavior analysis using Spark / Scala & Java
Optimized resource requirements including number of executors, cores per executors, memory for Spark streaming and batch jobs
Development of AI/ML modules and algorithms for Verizon ML use cases.

MUST HAVE SKILLS (Most Important):
Expert level knowledge and experience in

Scala
Java
Distributed Computing
Apache Spark
PySpark
Python
HBase
Kafka
REST based API
Machine Learning

DESIRED SKILLS:
8 years of experience in each of the following (except where otherwise noted):

At least 8 years of experience building and managing complex products/solutions.
Good problem Solving/analytical skills & an absolute team player.
5 Plus years Experience developing Restful web services in any Java framework.
Minimum 5 years experience on Hadoop Ecosystem (Spark/Scala/Python preferred) & Backend software modules using Scala / Spark & java
Minimum 8 years of experience working in Linux / Unix environment.
Expert level programming in Java, Scala & Python.
Experience in developing ETL modules for the AI/ML use cases, developing algorithms & testing
Minimum 5 Years of experience on performance optimizations on Spark, Hadoop, Any NoSQL
Minimum 5 Years of experience on Testing and Debugging Data pipelines based on Hadoop and Spark
Experience with debugging production issues & performance scenarios

EDUCATION/CERTIFICATIONS:
Bachelor's Degree in Computer Science, Engineering, or a related field

INDIT",3.6,"APR Consulting Inc
3.6","Sunnyvale, CA",-1,501 to 1000 Employees,1980,Unknown,Staffing & Outsourcing,Business Services,$50 to $100 million (USD),-1
Big Data Engineer,-1,"Coalition is the leading provider of cyber insurance and security, combining comprehensive insurance and proactive cybersecurity tools to help businesses manage and mitigate cyber risk. Coalition’s unique product offerings combine best-in-class insurance and proactive cybersecurity tools to help keep businesses safe. Cyber losses cost the global economy upwards of $1.5 trillion each year, and yet the majority of businesses are under-insured and under-prepared to manage and mitigate the risks of an increasingly digital world. Coalition is addressing this gap by providing no-cost cybersecurity tools to prevent losses, security and incident response services to contain them, and comprehensive cyber insurance to help organizations recover. We have over 25,000 customers, ranging from small and mid-sized businesses to Fortune 500 companies.

Founded in 2017, Coalition has raised $125M from a number of top tier global investment firms including Ribbit Capital, Greenoaks Capital, Valor Equity Partners, Felicis Ventures, and Vy Capital. Headquartered in San Francisco, Coalition’s team is distributed across more than 15 locations globally, including Austin, Washington DC, Denver, Canada and Portugal.

About the role:

The Coalition engineering team is growing rapidly, and we are looking for Data Engineers to work with the various data sources utilized in our business. The ideal candidate would have experience working with data engineering, data analysis, and statistical modeling, in the development of a real-time service. They would have experience working with structured and unstructured data systems, optimizing performance across large, disparate data sets, and streaming data systems. They would be a proficient developer who can deliver production-quality implementations.

This is a versatile role that will touch many aspects of data at Coalition, including network security data, actuarial data, and growth analytics data.

Responsibilities
Implement risk models for various insurance products
Evaluate, recommend, and implement data pipelines for a variety of data sources used at Coalition
Deliver production-quality software implementations for ETL and streaming pipelines
Explore new data sources and develop insights into existing data sources that improve business efficiency
Requirements
3+ years working with large disparate data sets
Deep understanding of ETL pipelines, statistical modeling, data analytics, and large scale data streaming
Expert-level knowledge of SQL, Python, R, or similar language used for data engineering
A proven track record of successfully automating business value from data insights
Experience with at least one big data search tool, such as Elastic
Excellent oral and written communications skills at all levels
Bachelor’s degree in Computer Science or a related field preferred
Bonus Points
Prior experience with insurance or network security technologies
In-depth knowledge of AWS or other cloud-hosted platforms relevant to data engineering
Experience with data visualization technologies
Perks
Enjoy a highly fulfilling, mission-driven culture
Health, dental, and vision benefits for you and your family
Life insurance and disability benefits
Paid Parental Leave
401(k) plan
Wellness and commuter benefits
Flexible working hours
Open vacation days
We embrace distributed work; some benefits will vary by location
You are an owner! We offer stock options to each of our employees
More details at https://www.coalitioninc.com/careers
Why Coalition?*
We are all here to build something we believe in and to make a company that will last. We’re also assembling a team of expert incident responders, threat and malware researchers, and security analysts to protect our customers before, during, and after a cyber incident. Our goal is to harness the power of technology with the safety of insurance, to provide the first holistic solution to cyber risk. Coalition's culture is one that strongly values humility, authenticity, and diversity. We want to work with people of different backgrounds and different paths in life, and we trust our team members to take responsibility, share ownership and work for one another. We are always looking for collaborative, inquisitive and dedicated individuals to join our team.

Recent press releases:
https://news.crunchbase.com/news/coalition-secures-90m-series-c-at-890m-valuation-to-grow-cyber-insurance-platform/
https://www.forbes.com/sites/amyfeldman/2020/05/28/next-billion-dollar-startups-2020/

Job Type: Full-time

Pay: $116,673.00 - $147,000.00 per year

Benefits:
401(k)
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Referral program
Vision insurance
Schedule:
Monday to Friday
Company's website:
www.coalition.com
Benefit Conditions:
Only full-time employees eligible",5.0,"Coalition Inc.
5.0","San Francisco, CA",-1,51 to 200 Employees,2017,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,$88k-$160k (Glassdoor Est.),"Data Engineer: Designs, builds and oversees the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs.

BS degree: 8-9 years of hands-on experience
MS degree- 4-5 years of hands-on experience

Following skills sets is must
4 years of hands-on experience working on data using SQL on multiple platforms (SQL server, my SQL, Mongo) (required)
5 years of hands-on experience with Python and C# programming (required)
4 years of hands-on experience with Data analytics (dashboards) and derive insights
3-4 years of hands-on experience with Application Development Skills - web based or service based (required)
3 years of hands-on experience with ETL tools and automation (required)
1 year of experience AWS technology stack understanding (desirable)
Job Types: Full-time, Contract",4.9,"DGN Technologies
4.9","Sunnyvale, CA",-1,201 to 500 Employees,2003,Company - Private,Consulting,Business Services,$25 to $50 million (USD),-1
Data engineer,-1,"Qualification: Master's Degree in Statistical Analytics, Data Science, or Bachelor's Degree in computer science engineering will be considered with at least three - five years of applicable work experience Preferred Proficiency: Web development experience (AngularJS, D3). Experience in a statistical programming language like R or Python; applied machine learning techniques including dimensionality reduction strategies, supervised/unsupervised classification and natural language processing frameworks. Experience in at least one data visualization tools (e.g. Tableau, QlikView) and data warehousing tools (e.g. Informatica) is preferred Huge Advantage: Building and scaling Machine Learning frameworks Hadoop (Hive, Spark, UDF's) Definite Plus: Web development experience (AngularJS, D3). Experience: 5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics. 5+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets. 2+ years of experience in scripting languages like Python etc.",4.6,"Stacklogy
4.6","Fremont, CA",-1,Unknown,-1,Company - Public,-1,-1,Less than $1 million (USD),-1
Data Engineer (AML/KYC/Sanctions/Fraud Detection Process),-1,"Role and Responsibilities: Title: Data Engineers(AML/KYC/Sanctions/Fraud Detection Process) (Immediate Onsite opportunity with a Global leader in web-based technologies and e-commerce) Job Description: As a Data Engineer, you will be working on client specific projects and will beexpected to understand the client data architecture. You will be required to use different technical skills and variety of tools to generate actionable insights from various client data sources. Your focus shall remain on solving business problems while working with structured as well as unstructured data. Dealing with ambiguity yet providing a structured approach to resolve client’s business issues will be a key requirement in this client facing role Client Interaction and Management Understanding the client objectives and managing client expectations on projects Independently work with client to scope and deliver on projects as a part of larger team As part of a larger team, align requirements on different work streams and tasks assigned by the client Project Management Making significant contributions to the design of the analytical approach and work plan Being responsible for smooth project operations – overall project quality and productivity of teams Key Skills/Experience: 2-4 years of relevant experience as a Data Engineer in payments compliance process- AML/ KYC/ Sanctions/ Fraud detection. Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage etc.) and experience with applications. Knowledge of machine learning techniques (clustering, decision tree learning) is a plus Proficiency in SQL, Python, Tableau Prior experience in tuning Sanctions, AML and RR models is highly preferred Experience in developing dashboards and metrics for payments compliance such as model effectiveness Experience with building and tuning predictive models and system implementation Any experience with Customer Risk Scoring, Customer Risk Rating is a plus Educational Background: Graduate with an engineering degree (preferably Computer Science) or a degree in statistics/math with experience in coding or equivalent Post Graduate with analytical pedigree (preferably Operations Research, Econometrics or MBA equivalent with analytical electives) Location(s): San Francisco USA Benefits: This is a full-time position with standard benefits such as healthcare, dental, vision, 401K and onsite amenities like catered lunches, micro kitchens and shuttle service.",3.1,"Grail Insights
3.1","San Francisco, CA",-1,201 to 500 Employees,2008,Company - Private,Research & Development,Business Services,$10 to $25 million (USD),-1
Data Engineer,$66k-$124k (Glassdoor Est.),"EOE STATEMENT We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status or any other characteristic protected by law. DESCRIPTION Position: The Data Software Engineer will be responsible for performing web scraping tasks to support Sciton’s ongoing research. The ideal candidate will have demonstrated professional experience in web scraping and data extraction along with the ability to communicate effectively and adhere to set deadlines. This is a remote position primarily based in the Palo Alto office. This position reports to the Director of Sales Operations. Responsible for: Utilize Python and other related programming software for writing script and code Extract, compile, prepare, and present data Plan organize and participate in project and focus group meetings Apply best practices in predictive analytics, text analysis, coding, data table design, and data visualization Apply statistical methodologies and software for sampling, testing, modeling, and interpretation and designing, implementing and evaluating quality assurance techniques and tools Perform analytics to identify trends and patterns, compare variables, discover correlations, and draw insights Prepare and present data reports summarizing project methods and techniques POSITION REQUIREMENTS Qualification / Skills: 2-3 years of professional experience related to data analytics, web scraping Strong demonstrated knowledge of Python, java and web scraping. Ability to develop scripts to extract and analyze data Ability to gather, manipulate, organize, and combine complex data sets to enable faster, easier, smarter, and more scalable analysis Ability to analyze operational data to identify trends, issues, and opportunities to optimize performance Ability to generate, validate, and maintain analytical reports from a variety of sources Experience summarizing and communicating technical data to a non-technical audience Experience / Education: Experience working with Google cloud and cloudSQL Preferred working knowledge of Tableau Experience in performing all aspects of qualitative and quantitative analysis (data cleaning and model testing) Attention to detail and experience creating high quality work products suitable for executive-level review Strong presentation skills BA/BS in Computer Science, Math/Finance, Physics, Applied Economics, Statistics or other technical field FULL-TIME/PART-TIME -unspecified- LOCATION Palo Alto (HQ) ABOUT THE ORGANIZATION SCITON, a privately held and employee owned company headquartered in Palo Alto, California, is a leading manufacturer of medical and aesthetic lasers and light source technologies. Founded by the laser industry's preeminent engineers and physicists, our mission is to provide best-in-class solutions to improve human conditions. Our culture and company strength stems from our diverse collection of talented and motivated employees. One word that best describes Sciton is 詮amily' because we treat each member of our team like family. We give our employees the support, recognition, and room to grow their careers within Sciton. We empower our people to develop their creative genius. In fact, we incentivize creativity and innovations to all our people. We look for lifelong learners, problem solvers, and individuals who excel in a challenging, team-oriented environment. We hire individuals who want to retire with us. How do you feel about that? THIS POSITION IS CURRENTLY ACCEPTING APPLICATIONS.",4.7,"Sciton
4.7","Palo Alto, CA",-1,201 to 500 Employees,1997,Company - Private,Health Care Services & Hospitals,Health Care,$50 to $100 million (USD),-1
"Data Engineer, CRM",-1,"Responsibilities TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, Mountain View, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo. The goal of the Customer Growth team is to build a stable, flexible and intelligent CRM platform, improve commercialization efficiency and client satisfaction. We are seeking self-motivated software engineers to develop an excellent platform for our clients and sales all over the world. Leveraging your knowledge of CRM system architecture, you'll work hands on in a fast paced environment to engineer solutions and actionable recommendations to develop TikTok's proprietary CRM into an enterprise grade, market leading platform. You'll work alongside a global team of product managers located in TikTok's major markets to build innovative sales, marketing and analytics tools to increase user adoption and satisfaction. Responsibilities: 1. Follow up the excellent solutions in the industry and implement them in our system. 2. Responsible for SAAS CRM system operation, maintenance and problem solving. 3. Understand the user's personalized demands and select reasonable technical solutions to solve problems quickly. Qualifications 1. Bachelor's degree or above in Computer Science or related fields. 2. 3+ years of experience with Data Warehouse. 3. Experience with Spark, Hive, Hadoop, SQL, Kafka, Parquet, HDFS, or HBase. 4. Proficiency in multiple systems languages (Java, Python, Go etc). 5. Knowledge of standard CRM functionality across Lead to Compensation pipeline. 6. Knowledge of Ad Sales automated processes and best practices in a CRM platform. TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or an accommodation, please reach out to us at usrc@tiktok.com.",4.1,"TikTok
4.1","Mountain View, CA",-1,1001 to 5000 Employees,2016,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,$74k-$140k (Glassdoor Est.),"Job Title: Data Engineer
Location: Sunnyvale, CA USA, 94086
Duration- Contract

Responsibilities
Client Call Updates:

·Following skills sets is must

o 4 years of hands-on experience working on data using SQL on multiple platforms (SQL server, my SQL, Mongo) (required)
o 5 years of hands-on experience with Python and C# programming (required)
o 3-4 years of hands-on experience with Application Development Skills - web based or service based (required)
o 1 year of experience AWS technology stack understanding (desirable) – S3, EC2, Lambda, SQS, SNS
o 4 years of hands-on experience with Data analytics (dashboards) and derive insights
o 3 years of hands-on experience with ETL tools and automation (required)

Description:

Data Engineer: Designs, builds and oversees the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs.

BS degree: 8-9 years of hands-on experience
MS degree- 4-5 years of hands-on experience

Following skills sets is must
4 years of hands-on experience working on data using SQL on multiple platforms (SQL server, my SQL, Mongo) (required)
5 years of hands-on experience with Python and C# programming (required)
4 years of hands-on experience with Data analytics (dashboards) and derive insights
3-4 years of hands-on experience with Application Development Skills - web based or service based (required)
3 years of hands-on experience with ETL tools and automation (required)
1 year of experience AWS technology stack understanding (desirable)
Contract length: 4 months

Job Types: Full-time, Contract

Pay: $40.00 - $75.00 per hour",4.0,"Procyon Technostructure
4.0","Sunnyvale, CA",-1,1 to 50 Employees,-1,Company - Private,Accounting,Accounting & Legal,Less than $1 million (USD),-1
Contract Data Engineer,$115k-$192k (Glassdoor Est.),"Contract Data Engineer

This role is with machine learning solutions team that solve a diverse set of real-world problems, and impact millions of users and billions of devices worldwide.

The ideal candidate should have a solid software development background and cloud development experiences.
As a data engineer, your day-to-day tasks will be the following.
• Understand and anticipate the infrastructure needs of the ML service development.
• Automate, deploy and maintain ML services using existing cloud resources.
• Automate end-to-end ETL/ML pipelines with structural understanding of data products.
• Work with team members to assist with data-related technical issues and support their data product needs.
Required Skills
• A background in computer science, engineering, mathematics, or similar quantitative field with a minimum of 2 years professional experiences.
• Experience in cloud application development and testing
Technical skills :
• Strong programming skills and experience in implementing data pipelines using python
• Experience with workflow scheduling / orchestration such as Kubernetes or Airflow
• Experience with NoSQL and SQL databases (e.g. Cassandra, Postgres)
• Experience with microservices architecture
• Experience with Unix-based command line interface and Bash scripts
Nice to have Skills
• Data visualization or web development skills a plus
Education
Bachelors in computer science or engineering; Masters preferred

Job Type: Contract

Pay: $62.00 per hour

Schedule:
Monday to Friday
Contract Renewal:
Possible
Work Remotely:
Temporarily due to COVID-19",3.8,"Deloitte
3.8","Sunnyvale, CA",-1,10000+ Employees,1850,Company - Private,Accounting,Accounting & Legal,$10+ billion (USD),-1
Big Data Engineer,$61k-$117k (Glassdoor Est.),"About NCR NCR Corporation (NYSE: NCR) is a leading software- and services-led enterprise provider in the financial, retail and hospitality industries. NCR is headquartered in Atlanta, Georgia, with 36,000 employees globally. NCR is a trademark of NCR Corporation in the United States and other countries. NCR's Digital Insight™ solutions are a leading Software-as-a-Service (""SaaS"") platform for financial institutions in the United States. We connect over 600 small to mid-size banks and credit unions with over 18 million online banking users and nearly 10 million mobile users. Join us to revolutionize digital banking by building upon our open technology platform developed on a unique service-oriented architecture (SOA), connecting with the wide breadth of NCR offerings across Retail, Hospitality, and Financial Services industries. What we build: The Digital Banking team in Redwood City, California is looking for Data Engineers to develop our next generation Digital Banking Data Platform. You will build and design highly scalable data pipelines using new generation tools and technologies like Google Cloud Platform (Pub Sub, Dataflow, BigQuery, BigTable), Kafka, Flume, to induct data from various systems to provide efficient reporting and analytics capability You will translate complex business requirements into scalable technical solutions, and design dashboards or visualization using BI tools to perform data analysis and to support business Collaborate with multiple cross functional teams such as product management, solution architectures, security, and software engineering Position Summary & Key Areas of Responsibility for the Big Data Engineer: NCR's Digital Banking solutions are a leading Software-as-a-Service (""SaaS"") platform for financial institutions in the United States. The Digital Banking team in Redwood City, California is looking for a Data Engineer to participate in the development of our next generation Digital Banking Data Platform. As a Data Engineer, you will build and design highly scalable data pipelines using new generation tools and technologies like Spark, Kafka, Storm and BigQuery to induct data from various systems to provide efficient reporting and analytics capability. You will translate complex business requirements into scalable technical solutions, and design dashboards or visualization using BI tools to perform data analysis and to support the business. You will collaborate with multiple cross functional teams such as Product Management, Solution Architecture, Security, and Software Engineering. Basic Qualifications for the Big Data Engineer: 2-4 years experience / relevant course work in designing and developing ETL data pipelines. Experience in OOP programming language (Java, Python) Understanding of data modeling, data structures and algorithms Experience with all aspects of software development life cycle (source control, continuous integration, deployments, etc.) Preferred Qualifications for the Big Data Engineer: Any experience with Google Cloud Platform a big plus Any experience developing Big Data/Hadoop applications using java, Spark, Hive, Oozie, Kafka, and Map Reduce is a plus Experience with or advance courses on data science and machine learning is ideal Any experience with BI tools is a plus Offers of employment are conditional upon passage of screening criteria applicable to the job. EEO Statement Integrated into our shared values is NCR's commitment to diversity and equal employment opportunity. All qualified applicants will receive consideration for employment without regard to sex, age, race, color, creed, religion, national origin, disability, sexual orientation, gender identity, veteran status, military service, genetic information, or any other characteristic or conduct protected by law. NCR is committed to being a globally inclusive company where all people are treated fairly, recognized for their individuality, promoted based on performance and encouraged to strive to reach their full potential. We believe in understanding and respecting differences among all people. Every individual at NCR has an ongoing responsibility to respect and support a globally diverse environment. Statement to Third Party Agencies To ALL recruitment agencies: NCR only accepts resumes from agencies on the NCR preferred supplier list. Please do not forward resumes to our applicant tracking system, NCR employees, or any NCR facility. NCR is not responsible for any fees or charges associated with unsolicited resumes.",3.6,"NCR
3.6","Redwood City, CA",-1,10000+ Employees,1884,Company - Public,Computer Hardware & Software,Information Technology,$5 to $10 billion (USD),-1
Biomedical Data Engineer,$77k-$144k (Glassdoor Est.),"The Biomedical Data Engineer performs R&D for biomedical sensor and algorithm development. They will research and design new technologies to objectively measure biomedical signals such as sensors that extract and classify physiological and behavioral data. This role will obtain experience working with clinical data and work closely with clinical experts and internal partner development, data, and engineering teams for biomedical algorithm development using the REAL System. What You'll Work On Perform literature reviews and work with clinical experts to understand biomedical signals and translate them into technical signals Research, design and prototype sensing techniques (both physical and digital) for biomedical signals Benchtop test and analyze sensing configurations against clinical gold standards Work with engineering and data teams to translate learnings and prototypes into production Write white papers and educate teams on methods and findings Support technical aspects of the strategic relationships with developers throughout the product lifecycle of health and wellness applications on Real System Understand partner developer use cases for new features and requests What You Bring Bachelor’s degree in STEM field or equivalent work experience in related field Strong programming skills such as C, C++, Java, Python, SQL, MATLAB Understanding of clinical science Prior experience with designing sensors (digital, physical) for biomedical signals Experience with processing time series data from multimodal inputs and sensors Experience with basic data structures and algorithms Familiarity with data modeling and machine learning preferred Experience with data quality review, pre-processing and algorithm development preferred Knowledge of cloud based services such as Microsoft Azure, Kubernetes, Docker, Node.js., React, PostgreSQL, GraphQL preferred Experience with game engine pipelines and technology stacks, including Unreal and Unity preferred Experience with VR headset technologies (Oculus Rift S, HTC Vive, etc.) preferred What We Offer A collaborative teamwork environment where learning is constant and performance is rewarded. The opportunity to be at the forefront of technology that is revolutionizing the treatment of some of the world's most devastating diseases. A generous benefits package that includes medical, dental, vision, and life insurance; a 401(k) match; and an Employee Stock Purchase Plan. Penumbra, Inc., headquartered in Alameda, California, is a global healthcare company focused on innovative therapies. Penumbra designs, develops, manufactures and markets novel products and has a broad portfolio that addresses challenging medical conditions in markets with significant unmet need. Penumbra sells its products to hospitals and healthcare providers primarily through its direct sales organization in the United States, most of Europe, Canada and Australia, and through distributors in select international markets. The Penumbra logo is a trademark of Penumbra, Inc. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status If you reside in the State of California, please also refer to Penumbra’s Privacy Notice for California Residents.",3.6,"Penumbra US
3.6","Alameda, CA",-1,1001 to 5000 Employees,2004,Company - Public,Health Care Services & Hospitals,Health Care,$100 to $500 million (USD),-1
eCom Data Engineer,$68k-$126k (Glassdoor Est.),"Auto req ID: 214340BR
Job Description


As other sectors have shifted to eCommerce-first business models in recent years, food & beverage has continued to rely predominantly on traditional brick & mortar models, but this is changing rapidly and a period of extraordinary disruption is now underway. New technologies are transforming every aspect of reaching consumers, from the rise of digital marketing and online grocery platforms to the creation of supply chain tools that enable speedy at-home delivery.

To seize this opportunity and lead the food & beverage industry into its remarkable next chapter, PepsiCo – the international food & beverage powerhouse with annual net revenue exceeding $64 billion and beloved brands including Frito-Lay, Gatorade, Pepsi-Cola, Quaker, and Tropicana – is expanding its Global eCommerce Team. As it needs the greatest minds in data & analytics, software development, machine learning optimization, and next-generation supply chain. Although PepsiCo is a large multinational, the PepsiCo Global eCommerce Team prides itself on having the entrepreneurial, action-oriented culture of an exciting startup business. As part of our group, alongside Silicon Valley veterans, founders of successful startup companies, and food & beverage experts to address a wide variety of the fascinating technical challenges facing our industry.

Given PepsiCo’s incredible global reach – our foods and beverages are enjoyed more than one billion times a day in more than 200 countries and territories, and our value chain involves diverse partners ranging from farmers and food scientists to retailers and logistics specialists – the challenges we’re addressing are complex and the solutions will be deeply impactful. The goal of the PepsiCo Global eCommerce Team is to build the technological products and capabilities that will reinvent our industry and make us the #1 food & beverage business in eCommerce for decades to come.

Accountabilities:
Lead problems assessment of eCommerce challenges to lead the development and design of technology solutions across functions involving computer hardware and software.
Lead technology project evaluations as well as proposal feasibility with the different eCommerce businesses
Apply theoretical expertise and innovation to create or apply new technologies to apply to the entire digital landscape.
Act as a consultant to the broader business users, management, vendors, and technicians to determine technology needs and system requirements.
Build new technologies and algorithms to optimize any business process.
Develop data set processes and projects requirements.
Use large data sets to resolve major business and functional issues while improving data reliability, efficiency and quality.
Optimize processes implementing new technology and automation across eCommerce businesses and eCommerce functions.
Qualifications/Requirements
BS or MS degree in Computer Science or a related technical field
6+ years of Python or Java development experience, Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases
6+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or an MPP system on any size/scale
Experience with specific AWS technologies (such as Glue, S3, Redshift, EMR, and Kinesis) a plus
Experience writing production code for Python or JVM-based systems, but you know a few other languages and like the right tool for the job
Knowledge of machine-learning tools and techniques
Experience optimizing larger applications to increase speed, scalability, and extensibility
Proven self-starter who can move projects forward by filling in the gaps on Agile teams, from leading a design session to doing some test automation, to mentoring a teammate struggling with a new technology
Relocation Eligible: Not Applicable
Job Type: Regular

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity

Our Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901 - 4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance.

If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy

Please view our Pay Transparency Statement",3.8,"PepsiCo
3.8","San Francisco, CA",-1,10000+ Employees,1965,Company - Public,Food & Beverage Manufacturing,Manufacturing,$10+ billion (USD),-1
Data Engineer,$106k-$194k (Glassdoor Est.),"Together, we can beat cancer. At Varian, we bring together the worlds’ best talent to realize our vision of a world without fear of cancer. Together, we work passionately to develop and deliver easy-to-use, efficient oncology solutions. If you want to be part of this important mission, we want to hear from you. Key responsibilities and team dynamics You live for efficient, accessible data. Your customers are AI and data scientists within the group. You build any needed data infrastructure and you listen to and service the needs of AI scientists, and they love you for it! You find the right tools for the job, even if you have to learn them. The buck stops with you on all things data, data connectors, data performance, data redundancy, data backups, and errors in data ingestion. As you grow in your role, you will learn machine learning tuning skills to retrain to new data, deploying importance sampling quickly for Reinforcement Learning tools, etc. Ability Requirements Fluency with at least one standard open-source tool in each BigData category and how to architect with the CAP theorem NoSQL Document Store Graph Databases and Queries Batch Map Reduce Service layer Streaming Fluency with common cloud and open-source data lake tools Fluency with common ingest and connectors for ML Ability to connect Pandas row factory to BigData stores Spark and ML flow ingestion and infrastructure Ability to create PyTorch DataLoader compatible data sets Fluency in shared-nothing architecture and lambda architectures Experience maintaining large data repositories Common deduplication tools and strategies Using tools to translate schemas Versioning, scripting, and documenting Creating SQL queries and macros Setting up above infrastructure Ability to connect data pipes, maintain Kubernetes containers Familiarity with common HA and FT data strategies Familiarity with HIPAA-compliant infrastructure and decision making Development platform skills Experience with Python, pandas, and Javascript Experience with GitHub based CI CD development Experience with Jenkins automation Experience with git Experience with AWS S3 and Glacier Experience with GCP storage Minimum requirements: Bachelors in Computer Science or similar. #LI-OSS1 Fighting cancer calls for big ideas. We envision a world without fear of cancer. Achieving this vision takes dedication and commitment from all of us, every single day. That's why we celebrate and value the distinctly beautiful and intersectional identities of each of our employees. We are a mirror of our patient-base, which allows us to innovate. Big ideas come from everywhere, and the best ideas are fostered by our unique individual experiences. At Varian, we encourage you to bring your whole self to work and believe your bold and authentic perspective will help to power more victories over cancer. #TogetherWeFight",3.9,"Varian Medical Systems
3.9","Palo Alto, CA",-1,5001 to 10000 Employees,1950,Company - Public,Biotech & Pharmaceuticals,Biotech & Pharmaceuticals,$2 to $5 billion (USD),-1
Data Engineer,-1,"If you thrive on working with big data in high performance teams then this is the place for you. You would work on data and build some of the tools that are critical to moving & transforming this data into valuable and insightful information. Creating reliable, scalable, and high performance products requires exceptional technical expertise and practical experience working with large-scale distributed systems. Finally, you will tackle challenging issues of scale, reliability and security while delivering a delightful, simple user experience to a global user base.

RESPONSIBILITIES

You will manage data warehouse plans for a product or a group of products. You will interface with engineers, product managers and product analysts to understand data needs. In addition, you will design, build and launch new data extraction, transformation and loading processes in production. You will work with data infrastructure to triage infra issues and drive to resolution. Be prepared to build and launch new data models that provide intuitive analytics to your customers as well as design and extremely efficient & reliable data pipelines to move data to our Data Warehouse. You will use your expert coding skills across a number of languages from Python, Scala, Java and PHP and work across multiple teams in high visibility roles.

REQUIREMENTS

2+ years of Scala and/or Python development experience is necessary
2+ years of SQL (Oracle, Vertica, Hive, etc) experience is required
2+ years of experience in custom or structured (ie. Informatica/Talend/Pentaho) ETL design, implementation and maintenance
2+ years or experience applying statistical data analysis to real-life problems
Experience working with either a Map Reduce or a MPP system on any size/scale
BS or MS degree in Computer Science or a related technical field
Previous experience with Data ingestion and IR (information retrieval) is highly desirable
Industry experience as a Data Engineer or related specialty

Powered by JazzHR

6DOeUlsRZn",4.0,"LotusFlare, Inc.
4.0","Sunnyvale, CA",-1,51 to 200 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Data Engineer,-1,"Our client in Sunnyvale, CA is looking for an experienced Data Engineer to design, build and oversee various projects.

Data Engineer:

The Data Engineer will design, build and oversees the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs.

Additional Skill Set Requirements:
4 years of hands-on experience working on data using SQL on multiple platforms (SQL server, my SQL, Mongo)

5 years of hands-on experience with Python and C# programming
4 years of hands-on experience with Data analytics (dashboards) and derive insights
3-4 years of hands-on experience with Application Development Skills - web based or service based
3 years of hands-on experience with ETL tools and automation
1 year of experience AWS technology stack understanding

Educational Requirements:

BS degree with 8-9 years of hands-on experience or MS degree with 4-5 years of hands-on experience

Location: Sunnyvale, CA

About Advantage Resourcing

Advantage Resourcing is committed to providing equal employment opportunity for all persons regardless of race, color, religion (including religious dress and grooming practices), sex, sexual orientation, gender, gender identity, gender expression, age, marital status, national origin, ancestry, citizenship status, pregnancy, medical condition, genetic information, mental and physical disability, political affiliation, union membership, status as a parent, military or veteran status or other non-merit based factors. We will provide reasonable accommodations throughout the application, interviewing and employment process. If you require a reasonable accommodation, contact us. Advantage Resourcing is an E-Verify employer. This policy is applicable to all phases of the employment relationship, including hiring, transfers, promotions, training, terminations, working conditions, compensation, benefits, and other terms and conditions of employment.

All employees are directed to familiarize themselves with this policy and to act in accordance with it. All decisions with respect to employment matters and other phases of employer-temporary employee relationships will be in keeping with this policy and in accordance with all applicable laws and regulations.",2.7,"Advantage Resourcing
2.7","Sunnyvale, CA",-1,10000+ Employees,1969,Company - Private,Staffing & Outsourcing,Business Services,$1 to $2 billion (USD),-1
Master Data Engineer,$110k-$192k (Glassdoor Est.),"201 Third Street (61049), United States of America, San Francisco, California

Master Data Engineer

Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who love to solve real problems and meet real customer needs.

We are seeking Data Engineers who will be a part of a team thats building new analytical and machine learning tools and frameworks to exploit advantages in the latest developments in cloud computing. As aCapital One Data Engineer, youll have the opportunity to be on the forefront of driving a major transformation within Capital One. Learn more about#lifeatcapitalone and our commitment todiversity & inclusion by jumping to slides 76-91 on our Corporate Social Responsibility Report.

What Youll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelors Degree
At least 5 years of experience in application development
At least 2 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)
Preferred Qualifications:
Master's Degree
7+ years of experience in application development
3+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink
1+ years of experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service
2+ years of experience with Ansible / Terraform
3+ years of experience with Agile engineering practices
3+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)
3+ years of experience with NoSQL implementation (Mongo, Cassandra)
3+ years of experience developing Java based software solutions
4+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)
4+ years of experience developing software solutions to solve complex business problems
3+ years of experience with UNIX/Linux including basic commands and shell scripting
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.",4.0,"Capital One
4.0","San Francisco, CA",-1,10000+ Employees,1994,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Lead Data Engineer, Big Data",$120k-$217k (Glassdoor Est.),"Leading the future of luxury mobility

Lucid’s mission is to inspire the adoption of sustainable energy by creating the most captivating luxury electric vehicles, centered around the human experience. Working at Lucid Motors means having a shared vision to power the future in revolutionary ways. Be part of a once-in-a-lifetime opportunity to transform the automotive industry.

We are looking for a Staff Data Engineer, Big Data who is looking for a challenge, enjoys thinking big and looking to make their mark on an extremely fast growing company. If building large and building fast, working with a young and very talented team of engineers and collaborating with the brightest mind in the Automotive industry is what you like, Lucid is the best to experience it.
The Role
Lead Data Engineer and architect, to design, implement a highly scalable system to ingest and process Petabytes of data per day.
Design and develop applications for data pipeline and data management.
Set processes and policies for data governance and data pipeline
Architect and implement best practices of big data tools such as Spark, Airflow, Kafka, Presto and Timeseries Databases
Lead and mentor junior Data and BI engineers.
Set and define the standards and best practices in data team
Be the point of reference for solving a challenging technical problem.
Architect and implement Machine Learning Pipelines for Data Science team.
Qualifications
Bachelor or Masters in Software Engineering and Computer Science
8+ years of experience in design and development of large-scale data platforms
4+ years of software development experience in Scala or Java.
Experience in hands-on implementing new application with production quality.
Expert in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Proficient in Spark application development with Scala Spark.
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Scala or Python.
Experience with running large-scale distributed computing infrastructure such as load balancing, Zookeeper, Micro service architecture
Experienced in security and access management
Experience with managing distributed databases like Elasticsearch, Cassandra and Timeseries Databases.
Experience with Columnar database such as Redshift, Vertica or Parquet
Great verbal and written communication skills.
LMMP

Be part of something amazing

Come work alongside some of the most accomplished minds in the industry. Beyond providing competitive salaries, we’re providing a community for innovators who want to make an immediate and significant impact. If you are driven to create a better, more sustainable future, then this is the right place for you.

At Lucid, we don’t just welcome diversity - we celebrate it! Lucid Motors is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, national or ethnic origin, age, religion, disability, sexual orientation, gender, gender identity and expression, marital status, and any other characteristic protected under applicable State or Federal laws and regulations.

To all recruitment agencies: Lucid Motors does not accept agency resumes. Please do not forward resumes to our careers alias or other Lucid Motors employees. Lucid Motors is not responsible for any fees related to unsolicited resumes.",3.9,"Lucid Motors
3.9","Newark, CA",-1,1001 to 5000 Employees,2007,Company - Private,Transportation Equipment Manufacturing,Manufacturing,Unknown / Non-Applicable,-1
Senior Data Engineer,$108k-$195k (Glassdoor Est.),"The Opportunity at Komodo Health: We are looking for an experienced Data Engineer on the Healthcare Map / Data Platform team to play a key role in enabling Komodo scalability. In this role, you will have the opportunity to help solve complex challenges and be a part of a great team. Our engineering team is currently about 90 and growing rapidly. You will be able to use one of the biggest, most complete healthcare datasets to solve real world problems that deliver value to clients and positive impact on the world. Some of the tools we use are: Python, Snowflake, Airflow, AWS EMR, AWS Lambda, Spark, AWS EKS, and Docker. You are an experienced technologist who has a proven track record of designing and building large scale solutions using big data technologies, ideally using Spark, Airflow and Python. As one of the members of this business-critical team, you will: Design, develop, and implement distributed data systems using Spark or similar technologies to ingest incoming data at scale. Create automation systems and tools to configure, monitor, and orchestrate the data services and data pipelines. Evaluate new technologies for continuous improvements in Data Engineering. Collaborate with data scientists to implement validation and/or data quality enhancements. What you bring to Komodo Health: BS/MS in Computer Science, related degrees or relevant technical experience. 5-7+ years implementing data engineering and distributed systems. Building and deploying large-scale, complex data services and data pipelines. Python development Pipeline scheduling and monitoring systems, like Airflow or Luigi Data processing platforms such as Spark, Hadoop, etc. Ability to work as part of a collaborative team in a fast-paced environment. Sincere interest in working at a healthcare startup and passion for healthcare data. Nice to have: Basic knowledge (college level or hands on) of machine learning and statistics. After three months, you will: Help architect and implement new data ingestion framework Write code to ingest new data sources at high scale Help operate existing data sources",3.5,"Komodo Health
3.5","San Francisco, CA",-1,201 to 500 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Marketing",$107k-$188k (Glassdoor Est.),"Company Description Square builds common business tools in unconventional ways so more people can start, run, and grow their businesses. When Square started, it was difficult and expensive (or just plain impossible) for some businesses to take credit cards. Square made credit card payments possible for all by turning a mobile phone into a credit card reader. Since then Square has been building an entire business toolkit of both hardware and software products including Square Capital, Square Terminal, Square Payroll, and more. We’re working to find new and better ways to help businesses succeed on their own terms—and we’re looking for people like you to help shape tomorrow at Square. Job Description As a Data Engineer on the Marketing team, you will join an organization whose mandate is to develop foundational data and reporting infrastructure to driving Square's revenue growth and accelerating seller acquisition. You will collaborate and work with teams across Square to build outstanding data pipelines, dashboards and processes that stitch together complex sets of data stores and guide large investment decisions. Your work will have an impact on hundreds of partners at Square. You Will: Develop data foundation and architecture, data pipelines and dashboards to ensure accurate and reliable business reporting Partner with business leads to understand their data and reporting requirements and translate them into Product Requirement Definitions and technical specifications Be the expert on end-to-end data flow for Marketing Bridge the gap between business requirements and technical implementation by troubleshooting data discrepancies and implementing scalable solutions, and communicate with high-level stakeholders in formats Monitor daily execution, diagnose and log issues, and fix business pipelines to ensure SLAs are met with internal stakeholders Make data model and ETL code improvements to improve pipeline efficiency and data quality Mentor Data Engineers and Data Analysts, and promote data engineering best practices Qualifications You Have: 6 years experience in Data Engineering or similar role 6 years experience in writing complex SQL and ETL development with experience processing extremely large datasets within cloud-based data warehouses like Snowflake, Google BigQuery, and Amazon Redshift Expert knowledge in data warehousing architecture and concepts, and dimensional data modeling Experience in Python Experience working with business teams on complex problems and translating them to efficient, scalable and easy to maintain data engineering solutions BS degree in Engineering, Computer Science, Math or a related technical field Technologies we use and teach: SQL and Python Looker, or other data visualizations technologies ETL scheduling technologies with dependency checking such as Airflow Linux/OSX command line, version control software (git) Additional Information At Square, we value diversity and always treat all employees and job applicants based on merit, qualifications, competence, and talent. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance. Applicants in need of special assistance or accommodation during the interview process or in accessing our website may contact us by sending an email to assistance(at)squareup.com. We will treat your request as confidentially as possible. In your email, please include your name and preferred method of contact, and we will respond as soon as possible. Perks At Square, we want you to be well and thrive. Our global benefits package includes: Healthcare coverage Retirement Plans Employee Stock Purchase Program Wellness perks Paid parental leave Paid time off Learning and Development resources",4.0,"Square
4.0","San Francisco, CA",-1,1001 to 5000 Employees,2009,Company - Public,Computer Hardware & Software,Information Technology,$1 to $2 billion (USD),-1
Data Engineer,$69k-$130k (Glassdoor Est.),"POSITION SUMMARY As part of our software engineering team, Data Engineers ( we are considering individuals with different experience levels) work with our internal business partners to provide critical insights into different products using analytics dashboards. We support a variety of different business units such as laboratory operations, R&D, process development, customer support, genetic counseling, reporting, bioinformatics, statistics, and more. Data Engineers need to be able to understand both the business processes of the groups we work with, and the technical details of the software to be an effective liaison for our department. This knowledge is used to discuss potential solutions and help drive decision-making using data. For this opening, we are looking for someone that specifically has experience understanding, designing and developing complex data models and reporting tools to ensure business partners have access to the data they need to make informed decisions. PRIMARY RESPONSIBILITIES Use analytical skills to drive a deep understanding of the various products in our fast changing business. Develop strong hands-on knowledge of our software products and be able to aid in tasks ranging from writing complex SQL queries to documenting and maintaining versions of data models across all of our products. Build effective data views while maintaining data integrity enabling analysts to create dashboards that deliver insightful analytics to business stakeholders. Act as a subject matter expert on the data models used across our software products providing guidance to stakeholders and create a shared understanding among analysts and technical teams. Produce deliverables that facilitate better understanding across all products such as data flow diagrams, data models, ERDs and other illustrations. Build confidence with our business partners by adding value to discussions, being responsive, and following through on action items. Incorporate a focus on quality into data modeling, and work closely with technical product managers, development and software quality assurance teams on data model improvements. Understand both the business processes of the groups we work with, and the technical details of the software to be an effective liaison for our department. Help with troubleshooting of issues by analyzing the underlying data and identifying the business impact for these issues. Support the data warehousing team in the implementation of the data warehouse for the new genomic data platforms. Keep up with industry trends and best practices, advising on new and improved data engineering strategies leading to improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance. Performs other duties as assigned. QUALIFICATIONS BS or higher in related major or equivalent industry experience 2+ years of experience in business intelligence, analytics, or an equivalent data engineer position with experience in writing extensive SQL and analyzing complex data sets. Experience designing and building data pipelines to enable data-driven decisions for the business. Excellent problem solving and analytical skills to easily break down complex problems and deliver simple and effective data modeling solutions. Experience using interactive BI / data visualization platforms such as QlikView / QlikSense, Tableau, Power BI etc. Structured and clear thinking with attention to detail. Effective verbal/written communication skills. KNOWLEDGE, SKILLS, AND ABILITIES Knowledge in following areas of software development: Software database design SDLC process JIRA or similar issue tracking system Desirable technical skills: DB design understanding and strong SQL skills Comfortable with Linux command line usage Ability to read JAVA code is a plus. Cloud/AWS basics. Experience with R or Python scripting is preferred. Comfortable contributing to and leading meetings with business partners in variety of positions and domain areas to receive data requests and guide decision-making Ability to understand and describe technical complexities in a simplified way appropriate to different audiences. Excellent written communication skills to produce clear, concise and correct technical documentation Strong ability to comprehend complex problems and apply knowledge to propose potential solutions Be able to break down large projects into granular milestones and track progress. Performs other duties as assigned PHYSICAL DEMANDS & WORK ENVIRONMENT Duties are typically performed in an office setting This position requires the ability to use a computer keyboard, communicate over the telephone and read printed material Duties may require working outside normal working hours (evenings and weekends) at times OUR OPPORTUNITY Driven by the passion for elevating the science and utility of genetic testing, Natera is committed to helping families identify and manage genetic diseases. Natera is a rapidly-growing diagnostics company with proprietary bioinformatics and molecular technology for analyzing DNA. Our complex technology has been proven clinically and commercially in the prenatal testing space and we are actively researching its applications in the liquid biopsy space for developing products with oncology applications. The Natera team consists of highly dedicated statisticians, geneticists, doctors, laboratory scientists, business professionals, software engineers and many other professionals from world-class institutions, who care deeply for our work and each other. When you join Natera, you’ll work hard and grow quickly. Working alongside the elite of the industry, you’ll be stretched and challenged, and take pride in being part of a company that is changing the landscape of genetic disease management. OUR OPPORTUNITY Driven by the passion for elevating the science and utility of genetic testing, Natera is committed to helping families identify and manage genetic diseases. Natera is a rapidly-growing diagnostics company with proprietary bioinformatics and molecular technology for analyzing DNA. Our complex technology has been proven clinically and commercially in the prenatal testing space and we are actively researching its applications in the liquid biopsy space for developing products with oncology applications. The Natera team consists of highly dedicated statisticians, geneticists, doctors, laboratory scientists, business professionals, software engineers and many other professionals from world-class institutions, who care deeply for our work and each other. When you join Natera, you’ll work hard and grow quickly. Working alongside the elite of the industry, you’ll be stretched and challenged, and take pride in being part of a company that is changing the landscape of genetic disease management. WHAT WE OFFER Competitive Benefits. Healthy catered lunches, Premium snacks and beverages, Onsite gym with cardio and weight-training equipment, Game room with satellite TV, Onsite dry cleaning and alteration service with pick-up and delivery, Employee-organized sport leagues, Happy hours and BBQs, Generous Employee Referral program. For more information, visit www.natera.com. Natera is proud to be an Equal Opportunity Employer. We are committed to ensuring a diverse and inclusive workplace environment, and welcome people of different backgrounds, experiences, abilities and perspectives. Inclusive collaboration benefits our employees, our community and our patients, and is critical to our mission of changing the management of disease worldwide. All qualified applicants are encouraged to apply, and will be considered without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, age, veteran status, disability or any other legally protected status. We also consider qualified applicants regardless of criminal histories, consistent with applicable laws.",4.0,"Natera
4.0","San Carlos, CA",-1,501 to 1000 Employees,2004,Company - Public,Biotech & Pharmaceuticals,Biotech & Pharmaceuticals,$100 to $500 million (USD),-1
Crash Sensing Data Engineer,$84k-$157k (Glassdoor Est.),"The Role

Collate, analyze and review fleet crash data to determine occupant injury metric, pedestrian protection performance and vehicle crashworthiness to further advance safety attributes.

Responsibilities

• Construct and maintain a large database of fleet impact data compiling information from multiple sources and data types into useful outputs

• Build and maintain an array of scripts (Python & MATLAB) for the database

• Facilitate useful output from the database, including automation of simulation (working closely with the CAE team) enabling iterative restraint and interior design based on real world events

• Review CAE simulation and field data using (MATLAB, LS-PrePost and /or METApost)

• Assist in setup and execution of attribute identification from images (tagging)

• Assist in categorization of impacts based on risk of injury

• Analysis of post-crash test data (DIAdem /MATLAB), reviewing results against expectations and predictions

• Design and complete studies of field data to assess restraint system performance

• Assist in the design of impact detection systems for pedestrian and occupant protection

• Assist in co-ordination and oversight of test execution for correlation and calibration purposes

• Support future and research projects related to occupant safety

Requirements

• Bachelor’s degree in Mechanical or Biomechanical Engineering

• Previous knowledge of automotive crashworthiness legislation and attributes of vehicle safety and crashworthiness is preferred

• Engineering fundamentals in problem solving

• Test data analysis tools e.g. (DIAdem, Python, MATLAB, Hyperworks, LS-PrePost and/or METApost)

• Good understanding and experience of vehicle structure development for crashworthiness

• The ability to articulate and present technical issues to audiences of either end of the spectrum regarding familiarity on crash safety and potential occupant injury.

• Hands-on experience with vehicle crash tests, sled tests and anthropomorphic test devices

• Experience with field data analysis for vehicle crashes

• Excellent organization skills and self-motivation

Tesla participates in the E-Verify Program",3.5,"Tesla
3.5","Fremont, CA",-1,10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$2 to $5 billion (USD),-1
Data Engineer,-1,"Job Description : *
\* Strong development skills around Hadoop, Hive, Pig Latin, Spark, Scalding, Map Reduce, Storm, Kafka
\* Knowledge of Spark and NoSQL databases
\* Have working experience in Google Cloud Platform
\* Strong skills in object oriented design, Core Java, Scala, shell scripting/Python
\* Excellent problem solving and analytical skills
\* Proven background in Distributed Computing, Data Warehousing, ETL development, and large scale data processing
\* Strong development skills in Python and SQL
\* At least 3-4 years of experience in Hadoop ecosystem and strong analytical skill

Contract length: 3 months

Job Types: Full-time, Contract

Salary: $50.00 - $60.00 per hour

Schedule:
Monday to Friday
Experience:
Hadoop, Hive, Pig Latin: 3 years (Preferred)
Google Cloud Platform: 2 years (Preferred)
Data Engineer: 4 years (Preferred)
Spark and NoSQL databases: 2 years (Preferred)
Application Question:
Are you comfortable with W2?
Work Remotely:
Temporarily due to COVID-19",-1,eTek IT Service | Savvysol,"Sunnyvale, CA",-1,-1,-1,-1,-1,-1,-1,-1
Database Engineer/Data Engineer with Java and NoSQL,-1,"Hello
Greetings From Purpledrive Technologies
Hope you are doing well
Please find my Job details below. Kindly let me know your Interest
Position: Database Engineer/Data Engineer with Java and NoSQL*
Location: San Francisco, CA*
Duration: Long term*
Required Skills:

· Experience in heterogeneous data stores (SQL/NoSQL/Column oriented/Timeseries)

· Experience in building the data lakes with large volume data. IoT related data volumes are relevant

· Ability to deconstruct and denormalize inbound data

· ETL related technologies (Like Informatica, Glue, Kinesis, Spark)

· AWS experience is must

· Ability to build data stores from the ground up

· Java Language Experience

· Data security and PII Data experience

Ideal Candidate must have:

· Exposure to multiple ETL Technologies (streaming Vs batching Vs Micro batching)

· AWS native technologies like Lambda, Glue, Redshift, Step functions

· CI/CD Pipeline technologies like CloudFormation/Terraform, IAM

· Python experience is plus fro scripting purposes

· Ability to assess proposed options and arrive at an objectively verifiable conclusion

· Experience working with offshore entities

Job Type: Full-time

Salary: $139,898.00 - $219,891.00 per year

Schedule:
8 hour shift",-1,Purple Drive Solution,"San Francisco, CA",-1,-1,-1,-1,-1,-1,-1,-1
Data Specialist-Data Engineer (Local to CA Only),-1,"Role: Data Specialist - Data Engineering At least six (6) years of FTE experience within the last ten (10) years performing at least two (2) of the following tasks: Application design and building solutions to move data from operational and external environments to the analytics environment; Designing, developing, and operating modern data pipelines, including ETL, ELT, event sourcing, stream processing, or other approaches to moving, transforming, and integrating data; Using modern tools for building ETL and ELT processes; Using stream processing tools such as Apache Kafka or Spark or equivalent tool; or Translating data between Kafka topics, Spark data structures and graph data structures. Tools and Technologies: Apache Kafka or Spark or equivalent tool, Apache Spark data structures and graph data structures, Neo4j, Delta lake, python, kafka, JSON, GraphQL",5.0,"xFusion Technologies
5.0","Sacramento, CA",-1,1 to 50 Employees,-1,Company - Private,IT Services,Information Technology,Less than $1 million (USD),-1
Big Data Engineer,$68k-$124k (Glassdoor Est.),"Position: Sr BigData Engineer*
Location: Costa Mesa, CA // San Jose, CA *
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Requirements *
BS degree in computer science, computer engineering or equivalent
Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies
Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive
3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation
Flair for data, schema, data model, how to bring efficiency in big data related life cycle
Must be able to quickly understand technical and business requirements and can translate them into technical implementations
Experience with Agile Development methodologies
Experience with data ingestion and transformation
Solid understanding of secure application development methodologies
Experienced in developing microservices using spring framework is a plus
Understanding of automated QA needs related to Big data
Strong object-oriented design and analysis skills
Excellent written and verbal communication skills
Responsibilities *
Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services
Integrate new data sources and tools
Implement scalable and reliable distributed data replication strategies
Ability to mentor and provide direction in architecture and design to onsite/offshore developers
Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases
Perform analysis of large data sets using components from the Hadoop ecosystem
Own product features from the development, testing through to production deployment
Evaluate big data technologies and prototype solutions to improve our data processing architecture
Automate everything
Job Type: Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
8 hour shift
Monday to Friday
Work Remotely:
Temporarily due to COVID-19",2.7,"IMG SYSTEMS
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Data Engineer,-1,"Position: Data Engineer *
Location: Menlo Park, CA*
Duration: 12+months *
The team needs Data Engineer having strong in SQL and have great experience in building data frameworks.
Understand and execute SQL queries efficiently (understands the tradeoff between different types query)
Ability to ask the right questions ; very good communication skills
Good problem-solving skills (ability to think through problems and walk through the with the stakeholders)
JD-
• Create and maintain optimal data pipeline architecture,
• Assemble large, complex data sets that meet functional / non-functional business requirements,
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources,
• Work with stakeholders including Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs,
• Experience building and optimizing data pipelines, architectures and data sets,
• A successful history of manipulating, processing and extracting value from large disconnected datasets.
• Experience in creating reports and dashboards in Tableau
• SQL (Adv to Expert level) ; Mid-level Python programming and Tableau required
Technologies stack in use: Dataswarm (data pipeline framework in Python), Hive, Presto, Python, Scuba (in-memory database), SQL, Oracle, Tableau

Job Type: Contract

Pay: $65.00 - $80.00 per hour

Schedule:
Monday to Friday
Experience:
Oracle: 1 year (Required)
Hive: 3 years (Required)
Presto: 1 year (Required)
IT professional: 8 years (Required)
Dataswarm: 1 year (Required)
SQL: 3 years (Required)
Python: 3 years (Required)
Scuba: 1 year (Required)
Tableau: 2 years (Required)
Location:
Menlo Park, CA (Required)
Contract Length:
1 year
Company's website:
https://veearprojects.com/
Work Remotely:
Temporarily due to COVID-19",4.3,"Veear Projects
4.3","Menlo Park, CA",-1,51 to 200 Employees,2003,Company - Private,IT Services,Information Technology,Less than $1 million (USD),-1
Data Engineer,$82k-$150k (Glassdoor Est.),"AppLovin is a global leader in mobile entertainment. Its studios create popular, immersive mobile games and its technology brings games to more players around the world. Since 2012, the company’s platform has been instrumental in driving the explosive growth of mobile games, resulting in a richer ecosystem and more games played by millions of people every day. AppLovin is headquartered in Palo Alto, California with several offices globally. Learn more at applovin.com . AppLovin is one of Inc.‘s Best Workplaces and a recipient of the 2019 Glassdoor Top CEO employee’s choice award. The San Francisco Business Times’ awarded AppLovin one of the Bay Area’s Best Places to Work and the Workplace Wellness Award which recognizes businesses that are leaders in improving worker well-being. About You: Have 1-3 years of experience and a minimum of a BS and/or MS in Computer Science Have excellent knowledge of computer science fundamentals including data structures, algorithms, and coding Experience independently creating and maintaining projects Product focused mindset Have experience working with big data systems (Spark, Hadoop, Pig, Impala, Kafka) Experience designing, building, and maintaining data processing systems Experience with a backend language such as Java or Scala About the Role: Collaborate with various engineering teams to meet a wide range of technological challenges Work closely with product and business teams to improve data models that feed business intelligence tools Define company data models using Spark Perks: Free medical, dental, and vision insurance Daily lunches and fully stocked kitchen Free public transit Free laundry service (wash/dry clean) Free gym membership 401k matching Fun company parties and events Autonomy to make decisions in a rapidly growing company Flexible Time Off - work hard and take time when you need it Interested? Send us your resume and let's talk! #LI-TP1 AppLovin is an equal opportunity employer and considers qualified applicants without regard to race, gender, sexual orientation, gender identity or expression, genetic information, national origin, age, disability, medical condition, religion, marital status or veteran status, or any other basis protected by law.",2.9,"MachineZone
2.9","Palo Alto, CA",-1,501 to 1000 Employees,2008,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr. Data Engineer - Digital Experience,$104k-$192k (Glassdoor Est.),"The Role

Tesla aims to accelerate world’s transition to a sustainable energy by ensuring we provide a seamless and frictionless experience to our customers. To meet this goal, Tesla is constantly striving to innovate and provide best in class services through introduction of pioneering new products and services. To that end, Tesla is seeking a hardworking and passionate Data Analyst. This Data Analyst will be part of a startup team that is focused on delivering a highly scalable platform that enables growth of current and future products and services to our customers.

Responsibilities

· Measure impact and performance of various growth initiatives, including incremental lift, revenue, and business metrics

· Perform a deep-dive analysis of referral program and help to develop and maintain a set of KPIs

· Partner with product and work cross-functionally to create and communicate key insights, influence key decisions and offer recommendations to leadership

· Work with the engineering team to make sure that we are collecting all of the data/logs needed to properly analyze the referral program performance

· Create ETL flows to help support the growth team make data-informed decisions

Requirements

· Bachelor’s degree in Engineering or Analytics, or equivalent in experience

· Experience in data analytics, dashboarding, and insight generation

· Strong analytical and problem-solving skills

· Proven history of finding valuable insights from complex sets of data

· Ability to successfully work with cross range of teams including engineering, UX, legal, finance, sales and operations

· Good understanding and working experience in technology development best practices (such as, Agile, etc.)

· Strong SQL, Python, and data-visualization skills

· Excellent written and verbal communications across technical and non-technical stakeholders

Tesla participates in the E-Verify Program",3.5,"Tesla
3.5","Fremont, CA",-1,10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$2 to $5 billion (USD),-1
"Data Engineer, GMS",$106k-$175k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Facebook is looking for exceptionally talented and experienced engineers to join GMS Technology team in Menlo Park, California. Our team provides analytics and workflow tools for Global Marketing Solutions (GMS), partnering with sales, marketing, measurement, support and operations teams.In this role, youll work with some of the brightest minds in the industry, work with one of the richest data sets in the world, use cutting edge technology, and get an opportunity to solve some of the most challenging business and engineering problems, at a scale that few companies can match. You will do so by partnering with stakeholders/teams and building scalable, reliable solutions that provide business critical insights and metrics, while ensuring the best uptime and responsiveness. This is a full-time position based in Menlo Park, CA.
Manage data warehouse plans for a business vertical or a group of business verticals
Build data expertise and own data quality for allocated areas of ownership
Design, build, optimize, launch and support new and existing data models and analytical solutions
Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions
Conduct design and code reviews
Work with data infrastructure to triage infra issues and drive to resolution
Manage the delivery of high impact dashboards, tools and data visualizations
BS/B.Tech./M.Tech in Computer Science, Math or related field
2+ years of experience in the data warehouse space, custom ETL design, implementation and maintenance
2+ years of experience in SQL or similar languages, and development experience in at least one language (Python, PHP etc.)
Experience with data architecture, data modeling, schema design and software development
Experience in leading data driven projects from definition through interpretation and execution
Experience with large data sets, Hadoop, and data visualization tools
Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders
Experience working in support of diverse communities
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Data Engineer - (B3),$107k-$186k (Glassdoor Est.),"Applied Materials, Inc. is the global leader in materials engineering solutions used to produce virtually every new chip and advanced display in the world. Our expertise in modifying materials at atomic levels and on an industrial scale enables customers to transform possibilities into reality. At Applied Materials, our innovations make possible the technology shaping the future. Job Responsibilities Programming skills:- R/Python (with working experience in most of common libraries like Scikit , numpy, pandas, mathplotlib, keras, tensorflow, nltk, genism, spacy etc) Good knowledge in statistics and deep understanding on ML algorithms and their usage Working experience in end to end data science project life cycles from use case framing, data collection, data exploration, model building, deployment Working experience in most of the common Machine Learning techniques related to Time series, Regression, Classification, Clustering, NLP, working with IoT data Working Knowledge in Deep learning with different NN architectures like CNN, RNN, LSTM, GANs Auto encoders etc. Knowledge of visualization tools like Tableau, AngularJ, R-Shiny with libraries to effectively communicate the results/inferences derived out of data science models Good to have working exposure in common cloud environments and understanding of robust on premise data science infrastructure. Nice to have understanding of big data related technologies and DevOps(Dockers, Singularity) Other skills/ expectations Good communication and presentation skill Proven ability be creative and analytical in trouble shooting issues Ability to work in a fast-paced and high-pressure environment to manage competing priorities Qualifications Education Bachelor's Degree in computer sciences or related field 6+ total years of Experience 2+ years of relevant Experience Applied Materials is committed to diversity in its workforce including Equal Employment Opportunity for Minorities, Females, Protected Veterans and Individuals with Disabilities. #LI Qualifications Education: Bachelor's Degree Skills Certifications: Languages: Years of Experience: 2 - 4 Years Work Experience: Additional Information Travel: Yes, 10% of the Time Relocation Eligible: Yes Applied Materials is committed to diversity in its workforce including Equal Employment Opportunity for Minorities, Females, Protected Veterans and Individuals with Disabilities.",3.9,"Applied Materials Inc.
3.9","Santa Clara, CA",-1,10000+ Employees,1967,Company - Public,Electrical & Electronic Manufacturing,Manufacturing,$10+ billion (USD),-1
Big Data Engineer,-1,"Data Engineer*
San Jose, CA*
$60-70/hr*
As the Big Data Engineer, you will be driving Test Strategy and Data Validation for AWS Cloud based networking products in a Fortune 50 environment. The Big Data Engineer is primary responsible for performing data validation, creating automation of test cases using Python and Spark, and running manual queries in a MySQL database to optimize product efficiency and customer satisfaction.
Minimum Qualifications *
Strong scripting experience using Spark, PySpark, or Python
Understanding of big data pipeline and databases including MySQL, MongoDB, Hbase
Hands on experience in AWS cloud environment and understanding of Hadoop framework
Understanding of networking stack or any wireless protocol - preferable 802.11
Experience with Cloud testing tools
Responsibilities*
Perform automation of test cases using pyspark/python
Perform manual query using SQL/Hive/pyspark to collect and analyze large amounts of data
Perform data validation of different parameters from router to cloud for multiple product line
Design and develop appropriate test automation frameworks and data validation techniques to ensure optimized product performance
Job Types: Full-time, Contract

Benefits:
401(k)
401(k) Matching
Dental Insurance
Health Insurance
Paid Time Off
Vision Insurance
Experience:
AWS: 3 years (Required)
Spark: 3 years (Required)
Python: 5 years (Required)
Full Time Opportunity:
Yes
Work Location:
One location
Schedule:
Monday to Friday
No weekends
Day shift
8 hour shift
Benefit Conditions:
Waiting period may apply
Work Remotely:
Temporarily due to COVID-19",4.4,"Brooksource
4.4","San Jose, CA",-1,501 to 1000 Employees,2000,Company - Private,Staffing & Outsourcing,Business Services,$100 to $500 million (USD),-1
Senior Data Engineer,$122k-$210k (Glassdoor Est.),"Have you ever tried to hire a plumber? How about a house cleaner? If you have, chances are it took you way longer than it should. In the era of instant-everything, you shouldn't have to waste an entire afternoon researching, calling and vetting local service professionals whenever you need one. The market for hiring them is huge — $1 trillion in the US alone — but the process is inefficient and largely offline.

Thumbtack is transforming this experience end-to-end, building a marketplace that matches millions of people with local pros for almost any project. In making these connections, not only do our customers get more done every day, our pros are able to grow their businesses and make a living doing what they're great at.

These customers and pros come from all walks of life and every zip code in the country. We want our team to reflect that. If you come from an underrepresented background in tech, we strongly encourage you to apply. We challenge ourselves every day to make this a place where you can thrive just the way you are, so we can build a product that does the same for our customers and pros.

About the Engineering Team

At Thumbtack, engineers at every level build products and systems that directly impact our customers and professionals. Our challenges span a wide variety of areas, ranging from building search and booking experiences to optimizing pricing systems, to building tools to help professionals grow their businesses. We believe in tackling these hard problems together as a team, with strong values around collaboration, ownership, and transparency. To read more about the hard problems that our team is taking on, visit our engineering blog.

About the Role

As our Senior Data Engineer, you will help to grow our Data Services team focused on business intelligence, and play a leading role on our Analytics Infrastructure team. You'll architect, design and develop a foundational, strategic vision for how we can continue to scale our world-class data environment and make our data accessible and digestible for reporting and analysis across the company. You'll collaborate with our Analytics and Infrastructure teams to manage and own our data warehouse used by hundreds across the company to drive reporting and analysis. Over time you will implement a portfolio of tools that empower analysts across the company to analyze and enrich our terabytes of data about our customers and pros. We're looking for a skilled data engineer and leader who can blend deep technical understanding with a bias for action and a strong sense of collaboration.

Responsibilities
Proactively drive the vision for BI and Data Warehousing across the company, and define and execute on a roadmap to achieve that vision
Design, architect, and maintain a data warehouse that supports a rapidly evolving product, in partnership with our stellar analytics and data services teams
Define, document and socialize foundational aspects of our reporting and analytical data model
Build and own the reporting pipelines and infrastructure that organize and structure our terabytes of data into digestible tables that empower analysis and reporting across the company
Drive data quality across key product and business areas
Collaborate closely with analysts to ensure our analytical infrastructure helps meet our company goals and allows product development to move fast
Partner with data services and product engineering teams to ensure consistent, seamless tracking and measurement of key company and product metrics
Must-Have Qualifications

If you don't think you meet all of the criteria below but still are interested in the job, please apply. Nobody checks every box, and we're looking for someone excited to join the team.

6 or more years of experience working in data or backend engineering, where your primary focus was on datastores and business intelligence, serving analysis and reporting functions
Experience building ETL data pipelines in a programming language like Python or Scala
Experience designing, architecting, and maintaining a data warehouse that seamlessly stitches together data from production databases, clickstream event data, and external APIs to serve teams of analysts
Experience being a project manager across a set of diverse projects with a strong track record of delivering against aggressive timelines
Excellent ability to understand the needs of and collaborate with stakeholders in other functions, especially the Analytics team
Nice-to-Have Qualifications
Experience managing or leading a data engineering team
Experience in an online marketplace or similar consumer technology company
Experience with a modern public cloud-based tech stack on AWS/GCP and the Google BigQuery data environment
Experience orchestrating data pipelines that serve hourly or daily metric reporting in Airflow
Experience with streaming or near-real time data pipelines
Experience building data pipelines in Scala
More About Us

Thumbtack is a local services marketplace – one of the largest in the U.S. – that helps millions of people hire local professionals. With hundreds of unique service categories, customers can find a Thumbtack pro for almost anything: landscapers, DJs, personal trainers, even piano teachers. And in making these connections, we empower local pros too. Helping them get new customers and make a living doing what they're great at.

Founded in 2008 and headquartered in San Francisco,Thumbtack is backed by over $400 million in investment from Sequoia Capital, CapitalG, Tiger Global Management, Javelin Investment Partners and Baillie Gifford.

See what it's like to work here
Meet the pros who inspire us
Engineers on a mission
Follow us on LinkedIn

Thumbtack embraces diversity. We are proud to be an equal opportunity workplace and do not discriminate on the basis of sex, race, color, age, sexual orientation, gender identity, religion, national origin, citizenship, marital status, veteran status, or disability status. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

The California Consumer Privacy Policy Act (the ""CCPA"") obligates covered businesses to disclose to consumers (including employees and job applicants), at or before the point of collecting personally identifiable information (""PII""), the categories of PI to be collected and the purposes for which the categories of PI shall be used.

In the course of the job application process, we may collect the following categories of PI for the purposes of evaluating you as a job applicant:

Identifiers such as a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, driver's license number, passport number, social security number, or other similar identifiers;
Professional or employment-related information.

We will not collect any additional categories of personal information or use your personal information collected for any other purpose without providing you with additional notice consistent with the CCPA.",3.6,"Thumbtack
3.6","San Francisco, CA",-1,501 to 1000 Employees,2008,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Data Engineer - Data Service Platform,$89k-$159k (Glassdoor Est.),"Who we are: Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 286 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies. Job Description Summary: You will closely work with the various world-class data driven e-payment related businesses, PayPal's GFC (Global Financial Compliance) Data and Service Platform. You will face the complex big data challenges or problems for fin-tech industry domain, which turns the big data to more valuable business insights. You do not only understand the data deeply but also develop the enterprise-level data algorithm and solutions for huge amount of data, various types of data for more business insights with high ATB, SLA, and short TTM. Job Description: PayPal Data Technology team is looking for a talented, creative, and passionate data engineer to design and develop data solutions on one of the largest data platforms in the world. As a self-motivated and enthusiastic member of our team. In this role you will:  Work with PayPal business units and Product Dev teams to design, develop and deliver data solutions on one of the largest data platforms in the world.  Support PayPal business units by providing data in a ready-to-use form to data analysts and data scientists for Business insights, predictive analytics, machine learning, etc.  Own and is accountable for the design and development of a Data solution feature or a Data Pipeline with high reliability, stability, scalability and performance.  Design or develop data model and enrich the analytical data sources for further usability and availability by advanced & cutting-edge technology.  Collaborate with other teams or groups to identify challenges and design solutions Requirements: BS degree in Computer Science or Engineering 4+ years of experience in software engineering. 3+ years working in data engineer or a similar role in large enterprise data warehouse team preferred. Solid programming/scripting skills, like Java, Scala, Unix/Linux Shell Scripting, etc. Good understanding of database principles and SQL beyond just data access Good understanding of Data Modelling Concepts, experience with modelling data and metadata to support relational & non-relational database implementations; experience with building Logical and Physical data models. Familiar with various big data technologies, open source data processing frameworks. E.g., Spark, Flink, Hadoop, HBase, ElasticSearch, Hive, etc. Good understanding of data processing, data structure optimization and design for high scalability, availability, reliability and performance. Good understanding of REST-oriented APIs, understanding of distributed systems, data streaming, Complex Event Processing, NoSQL solutions for creating and managing data integration pipelines for Batch and Real Time Data needs. Strong analytical and problem-solving skills. Work and collaborate well with executives and customers Excellent oral and written communication skills in English We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom. PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities.",4.0,"PayPal
4.0","San Jose, CA",-1,10000+ Employees,1998,Company - Public,Internet,Information Technology,$10+ billion (USD),-1
"Data Engineer, Analytics Lead (Ad Monetization and Delivery)",$153k-$249k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

In this role, you will work closely with the Facebook App Monetization (FAM) team whose mission is to build people-first experiences that give economic opportunity and access to everyone. You will be responsible for building a strong data foundation, data infrastructure and architecture in the FB App Monetization (FAM) Ecosystems team that will help the analytics, product, engineering and FAM leadership to drive decisions and build products with a holistic view and long term optimization founded in data. You will work on some of our most challenging problems that cut across our products, solve for a number of our teams, and set high level direction for the organization.
Drive vision, data strategy for FAM Ecosystems.
Plan, build roadmap for longer data strategies, data foundation, data assets to drive longer term needs for FAM Ecosystems.
Define goals and technical direction within FAM and participate in defining team goals and technical direction within organization.
Design, build and own the optimal data processing architecture and systems for new data and ETL pipelines, dashboards and experimentation metrics.
Build core datasets as well as scalable and fault-tolerant pipelines.
Build data anomaly detection, data quality checks, and optimize pipelines for ideal compute and storage.
Define and own the data engineering roadmap for long term Ecosystems data foundation.
Collaborate with cross-functional partners including data scientists, product managers and Software Engineers and FAM leadership to design and develop end to end data assets.
Work with data infrastructure teams to suggest improvements and influence their roadmap.
Recommend improvements and modifications to existing data and ETL pipelines.
Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership.
Drive internal process improvements and automating manual processes for data quality and SLA management.
4+ years experience in the data warehouse space.
4+ years experience working with either a MapReduce or an MPP system.
7+ years experience in writing complex SQL and ETL processes.
7+ years experience with schema design and dimensional data modeling.
4+ years experience with object-oriented programming languages.
BS/BA in Technical Field, Computer Science or Mathematics.
Knowledge in Python or Java.
Experience in analyzing data to identify deliverables, gaps, and inconsistencies.
Experience mentoring team members in their careers.
Experience effectively collaborating and communicating complex technical concepts to a broad variety of audiences.
Knowledge of monetization or advertisement world for consumer products.
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Data Engineer (Lead) Public Content Monetization,$153k-$249k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

In this role, you will be working as part of the Public Content Monetization team whose mission is to bring the best content for our users by enabling good, high-quality content creators (partners) to monetize. You will be leading the effort to build a strong data foundation for understanding the intersections of partner, user, and advertiser experiences in order to inform decisions on Facebooks partner monetization product offerings - and to ultimately bring more engaging content for our users around the globe. You will have the opportunity to work on some of the most challenging problems that cut across all of our products, create scalable solutions that benefit multiple teams, and collaborate with leadership to set the data vision and strategy for data engineering.
Drive data vision and strategy for an entire team of Data Engineers
Build data road-maps and setting technical direction for others to follow
Work on big projects across multiple product teams, and with a wide range of stakeholders
Design, build and own the optimal data processing architecture and systems for new data and ETL pipelines, dashboards and experimentation metrics
Build data anomaly detection, data quality checks, and optimize pipelines for ideal compute and storage
Collaborate with cross functional partners including data scientists, product managers and software engineers and the Public Content Monetization leadership to design and develop end to end data assets
Recommend improvements and modifications to existing data and ETL pipelines
Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership
Drive internal process improvements and automating manual processes for data quality and SLA management
5+ years experience in SQL and data processing
7+ years experience in data modeling and how to design scalable data architecture
7+ years experience in building out solutions for huge and complex datasets
5+ years experience working with multiple cross-functional teams
5+ years experience building data road-maps and setting technical direction for others to follow
Proficiency coding in Python, Java, or Hack
Experience in data visualization and dashboard design
Experience with experimentation frameworks (A/B testing, etc.)
Deep knowledge of MPP systems (Presto, Spark, BigQuery, Redshift, etc.)
Some leadership experience (as a tech lead, manager, or mentor)
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Data Engineer for CloudLinux projects (Remote),-1,"As CloudLinux rapidly expands, we are making more and more decisions based on data. We need a qualified data engineer to deliver key metrics for our executive, marketing, sales, finance & product management teams to make decisions. You will be in charge of getting data together from all the different sources, matching, scrubbing & analyzing the data, and producing data feed for other team's consumption. We are the maker of the #1 OS for web hosting providers. We develop our products - CloudLinux OS, KernelCare, and Imunify360 - using the most innovative technologies. Our products are used by thousands of companies around the world, including Dell, GoDaddy, IBM, 1&1, Endurance, and many others. Work is fully remote, with flexible hours, where you can plan your day and work from anywhere. More details about the company on Cloudlinux.com. Join us to make the difference! Responsibilities: Develop automatic solution to ingest data from various sources Develop new tools to match data from various sources Develop new tools and metrics for key decision-makers Requirements: 2+ years experience in Python with a solid grasp of the pydata stack (numpy, pandas) Experience with Spark or similar framework 1+ year of experience in SQL Knowledge of Linux (scripting, simple administrating) 1+ year experience in gathering data from various data sources, experience of work with big databases Experience of work with GIT Excellent communication skills Smart but humble, with a bias for action Knowledge of English language Experience with Tableau would be a plus Experience with data analysis/ data science would be a plus General skills: Ability to learn quickly and use new technologies Strong self-motivation, ability to deliver results under a limited supervision Ability to work in a team and independently Formulates new and alternate ideas, approaches, and designs Open to and values collaboration and feedback from others Ability to make decisions and be accountable for decisions and actions Ability to independently analyze a task and find a right solution Adaptable and able to work in a fast paced, rapidly changing environment We’ll give you: the chance of a lifetime to get onboard a young, dynamic, and fast-growing technology company; access to ground-breaking technology, and the freedom to expand your skills; a full-time, regular monthly contract, with a competitive compensation paid in US dollars; paid vacation, compensation for coworking, English language training, individual coaching, paid sick leave, medical insurance (if based in Russia or Ukraine), additional national holidays for your region; The freedom to choose where to work, anywhere in the world, so long as you’re online. What are you waiting for? Talk to us and make this a win-win for you and us. We are CloudLinux, and we need you to make the next move. Apply now! By applying for this position, you agree with Cloudlinux Privacy Policy and give us your consent to maintain and process your personal data with this respect. Please read our Privacy Policy for more information.",4.4,"CloudLinux
4.4","Palo Alto, CA",-1,51 to 200 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Big Data Engineer,$86k-$155k (Glassdoor Est.),"At SpringML, we are all about empowering the ‘doers’ in companies to make smarter decisions with their data. Our predictive analytics products and solutions apply machine learning to today’s most pressing business problems so customers get insights they can trust to drive business growth. We are a tight knit, friendly team of passionate and driven people who are dedicated to learning, get excited to solve tough problems and like seeing results, fast. Your primary role will be to design and build data pipelines. You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive. In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies. If you believe you have these skills please email your resume to info@springml.com. Required Skills: 4-7 years Python and Java programming 3-5 years knowledge of Java/J2EE 3-5 years Hadoop, Big Data ecosystem experience 3-5 years of Unix experience Bachelors in Computer Science (or equivalent) Duties and Responsibilities: Design and develop applications utilizing the Spark and Hadoop Frameworks or GCP components. Read, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery. Migrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing. Should have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets. Additional Skills that are a plus: C, Perl, Javascript or other programming skills and experience a plus Production support/troubleshooting experience Data cleaning/wrangling Data visualization and reporting Devops, Kubernetes, Docker containers Powered by JazzHR xeLSvfmYt5",4.4,"SpringML
4.4","Pleasanton, CA",-1,1 to 50 Employees,2015,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Data Engineer,$117k-$209k (Glassdoor Est.),"ABOUT VARO

We're on a mission to empower hard-working Americans to achieve greater financial resilience; arming them with the products and support they need to create healthy financial habits. Through our mobile app, we offer customers premium bank accounts that have no minimum balance requirement or monthly account fees, high-interest savings accounts, and solutions to build, repair, and access credit. Our state-of-the-art technology provides tech-first features to help people achieve their financial goals and manage their money more easily.

Varo is distinct from other fintechs: We've made history as the first and only consumer fintech to be granted a national bank charter by the Office of the Comptroller of the Currency (OCC). Our unique team combines the best of tech and banking, and we’re wildly passionate about keeping our customers happy by helping them manage and grow their money. Our teams are based in San Francisco and Salt Lake City. Privately held, we've raised over $419M to date, from leading institutional investors and strategic partners including Warburg Pincus, The Rise Fund / TPG Growth, Gallatin Point Capital, Harbourvest Partners, and Progressive Insurance.

ABOUT THE DATA ENGINEERING ROLE

As a Senior Data Engineer, you will play a critical role in implementing a variety of solutions to ingest, process and provision data to/from Varo’s Data Lake. The Varo Lake enables Varo’s data capabilities in reporting, analytics and allowing scientists to explore data in automated and self-service models.

As a technical contributor, you will take ownership of development for processing and analyzing data across the data platform.
WE'RE LOOKING FOR SOMEONE WHO WILL
Design, build and maintain Varo’s data pipelines to process data into and out of Varo’s Data Lake
Work with AWS big data technologies including EMR, Glue, S3, EKS, Lambda, Athena, RDS
Develop and maintain the data strategy for Varo in terms of capabilities and control mechanisms that support company responsibilities as a regulated national bank
Provide technical leadership in the area of data systems development including data ingestion, data curation, data storage, high-throughput data processing and analytics
Work with business partners on requirements, clarification and results
Participate in developing and enforcing data security & access control policies
Develop effective controls for a resilient data ingestion process
Support application data integration design and build efforts, including real-time capabilities
Conduct code reviews in accordance with team processes and standards
PREVIOUS EXPERIENCES THAT'LL HELP YOU BE GREAT
Bachelor's degree in Computer Science, MIS, Engineering or related field, or relevant work experience
5-7 years of data modeling, data pipelines, data lake, data warehouse experience.
5+ years programming experience in Python (will also consider Java, Kotlin, C, C++)
5+ years’ experience working within the AWS Big Data/Hadoop Ecosystem (EMR, Glue and Athena)
Experience with other AWS components like Cloudwatch, EKS, KMS, Lamdas, S3 is a strong plus
Experience with downstream consumption patterns (reports, dashboards) is a plus
Experience in Hadoop, HDFS, Hive, Presto, REST/SOAP API, Spark2, Airflow is a plus
At Varo, we are committed to living our values. We hope these resonate with you.

Customers First: Understand the problems our customers are trying to solve. Respond with a sense of urgency. Build relationships that result in loyalty. Be data and insights-driven. Test everything. Achieve results through strong execution. Build a product people love. Assess new initiatives with the customers’ interest in mind. Act with empathy.

Take Ownership: Bias towards action. Have high standards. Be accountable for the results of your work, our product, our company. Trust others to own it.

Respect: Treat others how you want to be treated. Listen first before being heard. Speak the truth even when it's not easy. Assume best intentions. Bring your full self to work.

Stay Curious: Ask why. Dare to make things better. Learn something new each day (even from mistakes). Be open to growth. Develop creative solutions.

Make it Better: Think big. Set high goals. Work towards long term value rather than short term wins. Create change. Be resilient.

Varo is an equal opportunity employer. Varo embraces diversity and we are committed to building teams that represent a variety of backgrounds, perspectives, and skills. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.",4.1,"Varo Money
4.1","San Francisco, CA",-1,501 to 1000 Employees,2015,Company - Private,Banks & Credit Unions,Finance,Unknown / Non-Applicable,-1
Data Engineer,-1,"Hagerty Consulting (www.hagertyconsulting.com) is one of the nation’s leading emergency management and homeland security consulting firms. Known for its public spirit, innovative thinking, problem-solving, and exceptional people, Hagerty is sought after to work on some of the largest, most complex, crisis and emergency management issues. Our services are focused on creating more resilient/sustainable jurisdictions, developing whole community operational plans, supporting recovery eligibility after disasters, and obtaining billion-dollar federal loans and grants. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. Responsibilities Include: Create and maintain optimal data pipeline architecture Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies Ability to work in an AWS environment to architect and build automated processes for collecting, prepping, and securely transferring delimited data files into a data hub from a variety of resources and providers Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs Keep our data separated and secure across national boundaries through multiple data centers and AWS regions Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader Work with data and analytics experts to strive for greater functionality in our data systems Qualifications: AWS SysOps certification or other comparable AWS certification. Using NIST Big Data Interoperability Framework as a reference architecture Strong analytic skills related to working with unstructured datasets Build processes supporting data transformation, data structures, metadata, dependency and workload management A successful history of manipulating, processing, and extracting value from large disconnected datasets Experience supporting and working with cross-functional teams in a dynamic environment We are looking for a candidate with 5+ years of experience in a technical field, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. They should also have experience using the following software/tools: OpenLattice: cloud-hosted, PostgreSQL cluster with RESTful API access and management layer. API architecture and coding SQL Familiar with relational and Datawarehouse concepts Security practices handling confidential data ( PII and PHI) (secure file transfer protocols and secure email transfer) GitHub, Python (Anaconda Community), Ansible, AWS CloudFormation, Talend (FOSS and commercial). Tools and platforms: Snowflake data warehouse, DataBricks (delta lake), PostgreSQL, Mulesoft, SFTP, MS SQL Server DevOps, CI/CD, data analytics Equal Opportunity Employer Veterans/Disabled",3.2,"Hagerty Consulting Inc
3.2","Sacramento, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Data Engineer,$98k-$176k (Glassdoor Est.),"Infostretch is seeking a Data Engineer with minimum of 8 years of experience.Please note that to start the role it will be remote for 3 - 6 months but once this Covid19 is behind us - it will be an in-house role. Candidate must have good experience working in development projects along with enterprise systems with many interfaces. Must have hands-on experience designing and developing streaming and IoT data pipelines. Be part of a group who will be building large and building fast, working with a very talented team of engineers and collaborating with the brightest mind in the Automotive industry. Job Qualifications 6 - 8+ years of experience in Data Engineering and Business Intelligence. Proficient in IoT tools such as MQTT, Kafka, Spark Proficient with AWS, S3, Redshift Experience with Presto and Parquet/ORC Proficient with Apache Spark and data frame. Experienced in containerization, including Docker and Kubernetes preferred Expert in tools such as Apache Spark, Apache Airflow, Presto Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines Extensive programming and software engineering experience, especially in Java, Python, Experience with Columnar database such as Redshift, Vertica Great verbal and written communication skills. Bachelor or Masters in Software Engineering or Computer Science Job Responsibilites: Hands-on design and develop streaming and IoT data pipelines. Developing streaming pipeline using MQTT, Kafka, Spark Structure Streaming Orchestrate and monitor pipelines using Prometheus and Kubernetes Deploy and maintain streaming jobs in CI/CD and relevant tools. Python scripting for automation and application development Design and implement Apache Airflow and other dependency enforcement and scheduling tools. Hands-on data modeling and data warehousing Deploy solution using AWS, S3, Redshift and Docker/Kubernetes Develop storage and retrieval system using Presto and Parquet/ORC Scripting with Apache Spark and data frame.",4.2,"Infostretch Corporation
4.2","Fremont, CA",-1,1001 to 5000 Employees,2004,Company - Private,IT Services,Information Technology,Unknown / Non-Applicable,-1
Data Engineer (Remote work),-1,"Title: Data Engineer
Duration: 3 Months
Location: Remote

NOTE - Need to work on W-2 only (No 1099 or Subcontracting)

Description:
Be part of Data Engineering team responsible for Data Analytics Platform servicing the business needs of the broader organization.

Qualifications:
Bachelor's degree in Computer Engineering, or related discipline
3+ years relevant working with Redshift, SQL, Python, Airflow and AWS data technologies
Understanding of data architecture concepts such as data modeling, metadata, workflow management, ETL/ELT, real-time streaming), data quality
3+ years of experience working with SQL
Experience with setting up and operating data pipelines using Python or SQL
1+ years of experience working on AWS
Exposure to open source and proprietary cloud data pipeline tools such as Airflow, and Glue
Experience working with relational databases
Experience with code management tools (e.g. Git, SVN) and DevOps tools (e.g. CICD)
Great written and verbal communication skills
Self-starter with the ability to work independently or as part of a project team
Capability to conduct performance analysis, troubleshooting and remediation
Everest Consultants is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, national origin, age, disability, or any other characteristic protected by applicable local, state or federal civil right laws.",-1,"Everest Consultants, Inc.","Sunnyvale, CA",-1,1 to 50 Employees,-1,Company - Private,Architectural & Engineering Services,Business Services,Less than $1 million (USD),-1
Sr. Data Engineer,$109k-$198k (Glassdoor Est.),"Sr. Data Engineer-20000SCP

Applicants are required to read, write, and speak the following languages: English

Preferred Qualifications

Data Engineer - Bay area

No Visa Sponsorship is available for this position. Applicants are required to read, write, and speak the following languages: English

Preferred Qualifications

We at Oracle have set out on a mission to streamline the clinical trial study startup process. Oracle Health Sciences already offers customers the industry's most advanced cloud solution for clinical trial planning, data collection, trial execution and safety management. goBalto adds the leading industry cloud solution that significantly reduces clinical trial startup time. Together, Oracle and goBalto will provide the most complete end-to-end cloud platform dedicated to unifying action and accelerating results for the Life Sciences industry

goBalto is the industry leader in cloud-based study startup software for the global Life Sciences industry, offering the only complete end-to-end platform for starting clinical trials, from site identification, feasibility assessment and selection through to activation, with comprehensive metrics to track adherence to timelines and budget.

Our Opportunity

The Principal/Senior Data Engineer will be responsible for developing data pipeline, ETL solutions and productizing machine learning features in the SaaS platform and applications in Oracle's Health Sciences Global Business Unit. We work with data scientists to develop machine learning models and incorporate them into our products. This person will work independently and perform at a senior level to help build out our technology platform using technologies like SQL and Python.

You’ll work with a rock star engineering team that’s passionate about building the first study startup platform of its kind for our pharmaceutical clients. You must be willing to collaborate and help others on the team as necessary, and be a solid team player. This position is an individual contributor (no direct reports). We’re based in the financial district near the Embarcadero.

What you'll do...

Develop SQL based ETL solutions
Build data pipelines and productize machine learning capabilities into the product
Work with other top-notch engineers, PMs and QA
Operate as a member of the goBalto agile development team.
Skills you have…

Minimum of 5 years experience
Strong data modeling and SQL skills.
Experience with Python, scikit-learn.
Experience with PostgreSQL and Oracle DB.
Proven experience with unit testing and test frameworks.
Git or similar distributed SCM tools.
Experience with bash/shell scripting.
Strong interpersonal skills.
Demonstrable expertise and recent experience with the following is a plus:
Enterprise SaaS architectures
Service-oriented architectures
Familiarity with cloud-based deployments
Queuing and real-time streaming systems
Reporting and analytics
Team attributes…

Not just Another Social App – we’re building a meaningful product with potential speed the delivery of new medicines to market
Agile Bones - we promote an agile, test-driven engineering practice that emphasizes team collaboration through pair programming (where appropriate), daily standups, retrospectives, etc.
Technology You’re Already Passionate About, and Want to Do More With - we’re using technologies such as Node.js, Ruby on Rails, Angular.js, AWS, Cucumber, Capybara, Rspec, and more to build solid, well-tested, and beautiful web applications, and challenge our engineers to learn and do more with them.

Detailed Description and Job Requirements

Design, develop, troubleshoot and debug software programs for databases, applications, tools, networks etc.

As a member of the software engineering division, you will assist in defining and developing software for tasks associated with the developing, debugging or designing of software applications or operating systems. Provide technical leadership to other software developers. Specify, design and implement modest changes to existing software architecture to meet changing needs.

Duties and tasks are varied and complex needing independent judgment. Fully competent in own area of expertise. May have project lead role and or supervise lower level personnel. BS or MS degree or equivalent experience relevant to functional area. 4 years of software engineering or related experience.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Oracle is an Affirmative Action-Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veterans status, age, or any other characteristic protected by law.

Job: Product Development
Location: US-CA,California-San Francisco
Other Locations: US-CA,California-Dublin, US-CA,California-Oakland, US-CA,California-Palo Alto, US-CA,California-Sunnyvale, US-CA,California-Pleasanton, US-CA,California-Redwood City, US-CA,California-San Jose
Job Type: Regular Employee Hire
Organization: Oracle",3.6,"Oracle
3.6","San Francisco, CA",-1,10000+ Employees,1977,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD),-1
"Data Engineer, Machine Learning",$110k-$188k (Glassdoor Est.),"At Cadence, we hire and develop leaders and innovators who want to make an impact on the world of technology. At Cadence, we hire and develop leaders and innovators who want to make an impact on the world of technology. If you are an engineer who enjoys an exciting, fast-moving environment and daily technical challenges as much as we do, the newly founded Cadence Data Platform team will be a great fit for you. We are looking for a Data Engineer who will be a key contributor in building a scalable and resilient data analytics platform. Key Qualifications · Experience working with big data, such as Hadoop ecosystem, Spark, Kafka, etc. · In-depth understanding of data structures, algorithms, and distributed systems · Development experience in Java, Scala, or Python · Solid understanding of database fundamentals, with proficiency in SQL · Experience building and optimizing ETL data pipelines · Enthusiastic, highly motivated, and able to work collaboratively · Broad understanding of various technologies and frameworks with ability to combine into practical solutions · Familiarity with provisioning, installing, configuring, and maintaining data platform solutions Education & Experience · BS degree in Computer Science or related field, 4+ years of industry experience · MS degree in Computer Science or related field, 2+ years of industry experience We’re doing work that matters. Help us solve what others can’t.",4.3,"Cadence Design Systems
4.3","San Jose, CA",-1,5001 to 10000 Employees,1988,Company - Public,Computer Hardware & Software,Information Technology,$1 to $2 billion (USD),-1
Senior Data Engineer,-1,"Join SADA as a Sr. Data Engineer! Your Mission As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses. You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects. Pathway to Success #BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs. Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions. As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks. Expectations Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted. Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives. Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared. Job Requirements Required Credentials: Google Professional Data Engineer Certified or able to complete within the first 45 days of employment Required Qualifications: Mastery in at least one of the following domain areas: Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive). Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions (relational and NoSQL) Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or customer-facing role Useful Qualifications: Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc) Experience with IoT architectures and building real-time data streaming pipelines Experience operationalizing machine learning models on large datasets Demonstrated leadership and self-direction - a willingness to teach others and learn new techniques Demonstrated skills in selecting the right statistical tools given a data analysis problem About SADA Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer. Make them rave Be data driven Be one step ahead Be a change agent Do the right thing Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal! Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs. Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.",3.8,"SADA
3.8","San Francisco, CA",-1,51 to 200 Employees,-1,Company - Private,Logistics & Supply Chain,Transportation & Logistics,Less than $1 million (USD),-1
Data Engineer,$110k-$201k (Glassdoor Est.),"Date: Aug 28, 2020 As the tech firm that created the mobile world, and with more than 54,000 patents to our name, we’ve made it our business to make a mark. When joining our team at Ericsson you are empowered to learn, lead and perform at your best, shaping the future of technology. This is a place where you're welcomed as your own perfectly unique self, and celebrated for the skills, talent, and perspective you bring to the team. Are you in? Come, and be where it begins. Our Exciting Opportunity! — You get to work with an outstanding local and international team, while interacting closely with our customers — You have the ability to influence and drive exciting, complicated projects — You can remove barriers and address sophisticated situations with regards to technical challenges in applications development You will —Define how we instrument, prioritize, and store data that powers AI/ML solutions —Design and implement scalable and durable data models for our data —Evolve and optimize our data and data pipeline architecture, as well as, optimizing data flow and collection for multi-functional teams. You will also be responsible for integrating them with the architecture used across the company. — Support our Data Science teams, partners on data initiatives and ensure efficient data and model architecture is designed and implemented successfully and efficiently —Develop the end-to-end automation of data pipelines, making datasets readily-consumable by the data science teams or downstream AI/ML applications — Empower junior team members and lead by example To be successful in the role you must have — Have Advanced degree in Computer Science, Math, Statistics or a related discipline — Have 6-8 years hands on experiences on building data pipeline, data models, data solutions from scratch, industry experience preferred — Have expertise in Data Engineering and relevant technologies (e.g. Spark, Python,Kubernetes) — Comfortable with complex SQL — Balance the practical considerations on resources, efficiency, footprint requirement when designing a solution or architecture — Have experience in batch and streaming data architectures like Kafka, Kinesis, Flink, Storm, Beam — Have expertise in various types of data base structures (object storage, document or key-value stores, graph databases, column-family databases) — Have excellent coding skills (python, Java or Scala) — Detail oriented — Understand the Data Lifecycle and concepts such as lineage, governance, privacy, retention, anonymity, etc — Expertise in production support and troubleshooting — Be self-directed and comfortable supporting the data engineering needs of multiple teams, systems and products. — Excel at taking vague requirements and crystallizing them into scalable data solutions You might also have: — Working knowledge of AI/ML — Experience of providing technical leadership and mentor other engineers for the standard methodologies on the data engineering space — Telecommunication domain knowledge — Basic proven understanding of UI frameworks desired **LI-SP1 Do you believe that an organization fostering an environment of cooperation and collaboration to execute with speed creates better business value? Do you value a culture of humanness, where fact based decisions are important and our people are encouraged to speak up? Do you believe that diverse, inclusive teams drive performance and innovation? At Ericsson, we do. DISCLAIMER: The above statements are intended to describe the general nature and level of work being performed by employees assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties and skills required of employees assigned to this position. Therefore employees assigned may be required to perform additional job tasks required by the manager. We are proud to be an EEO/AA employer M/F/Disabled/Veterans. We maintain a drug-free workplace and perform pre-employment substance abuse testing. Ericsson provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, pregnancy, parental status, national origin, ethnic background, age, disability, political opinion, social status, protected veteran status, union membership or genetics information. Ericsson complies with applicable country, state and all local laws governing nondiscrimination in employment in every location across the world in which the company has facilities. In addition, Ericsson supports the UN Guiding Principles for Business and Human Rights and the United Nations Global Compact. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, training and development. Ericsson expressly prohibits any form of workplace harassment based on race, color, religion, sex, sexual orientation, gender identity, marital status, pregnancy, parental status, national origin, ethnic background, age, disability, political opinion, social status, protected veteran status, union membership or genetic information. Ericsson will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by Ericsson or (c) consistent with Ericsson’s legal duty to furnish information. Employee Polygraph Protection Act Notice - Employers are generally prohibited from requiring or requesting any employee or job applicant to take a lie detector test, and from discharging, disciplining, or discriminating against an employee or prospective employee for refusing to take a test or for exercising other rights under the Act. For more information, visit https://www.dol.gov/whd/regs/compliance/posters/eppac.pdf. Ericsson is an equal opportunity employer and is committed to providing reasonable accommodation for qualified disabled individuals during the application and hiring process. Ericsson will make modifications or adjustments to the job application or interview process that will enable a qualified applicant to be considered for a position. If you require an accommodation due to a disability, please contact Ericsson at hr.direct.dallas@ericsson.com or (866) 374-2272 (US) or (877) 338-9966 (Canada) for further assistance. Primary country and city: United States (US) || || Santa Clara || ProdMgt",3.6,"Ericsson
3.6","Santa Clara, CA",-1,10000+ Employees,1876,Company - Public,Telecommunications Services,Telecommunications,$10+ billion (USD),-1
Senior Data Engineer,$130k-$236k (Glassdoor Est.),"Description At First Republic, we care about our people. Founded in 1985, we offer extraordinary client service in private banking, private business banking and private wealth management. We believe that personal connections are everything and our success is driven by the relationships we form with our colleagues and clients. You'll always feel empowered and valued here. Incredible teams doing exceptional work, every day In Data & Client Insights, we are responsible for delivering accurate data reporting to help promote data fluency, improve client service, and inform business decisions across the enterprise. We aim to maximize the value of the Bank's data to capitalize on competitive opportunities in increasingly sophisticated ways. Responsibilities The Senior Enterprise Data and Client Insights (EDCI) Engineer will work as part of the EDCI team helping with data architecture modernization initiative including cloud adoption, real-time processing, data virtualization, rules driven transformation, data lineage, metadata management, data governance, control, distributed processing for scalability/fault tolerance, rapid development support, Pyriting (test data generation), tagging/profiling, etc. As part of the job duties, the Data Engineer will partner with stakeholders to understand current pain-points, gaps, business drivers, and common needs and come up with target architecture, design and execution path from current state to target state. The Data Engineer will also be responsible for creating detailed design and overseeing delivery. What you'll do as a Senior Data Engineer: - Cloud adoption/migration - Snowflake/AWS - Work with partners to: - Design and implement strategy to move from on-prem to cloud infrastructure - Establish guidelines, best practices, processes, and governance - Design and build services/utilities/accelerators to help with migration including: - Solution to move data from on-prem Sqlserver to cloud Snowflake - Accelerators to rewrite SQLs, help with migration, etc. - Sandbox management for quants/adhoc data analysis - Seamless integration with on-prem components - EDCI common service fabric including Data curation using rules engine, data access layer, real time data processing, etc.: - Work with stakeholders/user groups to understand current pain-points, gaps, business drivers, and common needs - Conceptualize, design and develop common framework/utilities leveraging new advancements in technology with an emphasis on performance and scalability - Evaluate new tools and technologies required - Oversee implementation - Metadata Management - Conceptualize comprehensive metadata layer to enable necessary fine-grained controls, audits, lineage, status, etc. - Work with partners to develop strategy and oversee implementation - SPOT User Interface - Work with stakeholders/user groups to define a vision for Single portal for all data needs - Prepare wireframes, define services/interfaces - Design, integrate and oversee implementation - General Consultation - Provide consultation in Architecture and Design related matters to partner teams - Help evaluate new products - Support IT and business partners as needed on day to day issues and problems Responsibilities include: 1) adheringto and complying with the applicable, federal and state laws, regulations and guidance, includingthose related to Anti-money laundering (i.e. Bank Secrecy Act, US PATRIOT Act, etc.) 2) adhering to Bank policies and procedures, 3) completingrequired training, 4) identifying and reporting suspicious activity to the AML Officer, and 5) knowing and verifying the identity of any customer(s) that enters into a relationship with the Bank. Qualifications You could be a great fit if you have: - Expert level technical skills (required in at least some of the following): - Programming Languages/Frameworks: Java/JEE, Scala, Python, Javascript, SQL, Spark - Databases/Cache: Snowflake, MSSqlserver, Oracle, Mongodb, Redis, PostgreSQL, Elasticsearch, Denodo - Cloud/SAAS/Middleware: AWS, Snowflake, Nodejs, Kafka - Web Stack: Nodejs, jQuery, AngularJS, React, Vue, HTML5 - 12+ years of IT experience is a must - 5+ years of experience working as an engineer in strategic initiatives - Bachelor of Science/Engineering in computer science/electronics or related field - Knowledge of or experience with machine learning is a plus - - Strong problem-solving skills, including trouble shooting systems under time pressure - Focus on designing reusable and scalable solutions - Ability to quickly learn and adopt new technologies - Drive to innovate with latest tools and strategies - Knowledge of industry best practices, proven design patterns - Consistently demonstrated and follow high standards of integrity in decision-making - Ability to look toward the broadest possible view of an issue or challenge; can easily pose future scenarios; can think globally about all aspects of the Bank; can discuss multiple considerations of an issue and forecast them into the future; understands how the Bank works, competes, serves clients, and generates shareholder value - Excellent communication and interpersonal skills; ability to communicate clearly and concisely in a variety of settings and styles; effective in a variety of formal presentation and meeting settings - Ability to gain support for change by providing context and responding with sensitivity to concerns; takes initiative to recommend/develop innovative approaches to getting things done - Ability to quickly find common ground and solve problems for the good of all; a team player and encourages collaboration - An attitude of getting things done and delivering results while keeping long terms goals and objectives in mind is a must; to supplement these sophisticated analytical capabilities, candidates need to have solid business acumen and executive-level communication skills - Ability to work effectively in both a team environment and independently, with internal and external (partner) resources Job Demands: - Must be able to review and analyze data reports and manuals; must be computer proficient. - Must be able to communicate effectively via telephone and in person. Own your work and your career - apply now Are you willing to take initiative and make decisions? Are you willing to go the extra mile because you love what you do and how you can contribute as a team? Do you want the freedom to grow and the opportunity to take charge of your own career? If so, then come join us. We want hard working team players. You'll have the independence to learn, lead and drive change. A culture of extraordinary service, empowerment and stability - that's the First Republic way. This job description is not intended to be all-inclusive. Employee may perform other related duties as assigned to meet the ongoing needs of the organization. The Company is an equal opportunity employer. In this regard, the Company makes reasonable accommodations for qualified applicants and employees with disabilities in order to enable them to perform all essential job functions, unless doing so creates an undue hardship. First Republic is subject to federal laws that restrict the employment of individuals with certain types of criminal histories, including FDIA Section 19 and FINRA. To the extent not inconsistent with our obligations under those federal laws and regulations, First Republic will consider qualified candidates with criminal histories in a manner consistent with the Los Angeles and San Francisco ban-the-box laws.",3.8,"First Republic Bank
3.8","San Francisco, CA",-1,1001 to 5000 Employees,1985,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
Senior Data Engineer,-1,"About us: At Ginger, we believe that everyone deserves access to incredible mental healthcare. Our on-demand system brings together behavioral health coaches, therapists, and psychiatrists, who work as a team to deliver personalized care, right through your smartphone. The Ginger app provides members with access to the support they need within seconds, 24/7, 365 days a year. Millions of people have access to Ginger through leading employers, health plans, and our network of partners. Ginger has been recognized by The World Economic Forum as a Technology Pioneer and by Fast Company as one of the Most Innovative Companies in Healthcare. About the Role: At Ginger, we aim to provide better mental health care to humanity at a scale larger than has ever been possible before. This is no small task and as an expanding team we are working on a number of initiatives to achieve this, including aggressively building tools to simultaneously grow our reach and improve quality of care. What You'll Do: Standing at the center of multiple teams (data science, engineering) and core systems, you'll.. Open up our data to uncover important patterns at the level of individuals and sub-populations. Surface, serve and persist key actionable insights in mental health, healthy habit formation, goal pursuit, and care efficacy. Help us scale our services using modern distributed processing tools and GPUs in the cloud (AWS) Collaborate with product to ideate and unlock features which derive as much actionable information from our data (text, media, activity etc) as possible. Help architect systems for near-real-time delivery of recommendations, care insights and other time-critical information to coaches and members. Design lightweight data schemas appropriate for storing, organizing and joining processed communication and care analytics. Devise the tooling that takes us from algorithm prototype to production and can track data/model lineage and statistical drift through time. Develop pipelines that efficiently and reliably route output of machine learning algorithms to consumer processes and persistence mechanisms. Own operational scalability of our algorithms, systems and data models. Stand up infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Python and AWS tools. Work with a variety of stakeholders including the Data, Product, Engineering, Security and Executive teams to support their data accessibility needs. Necessary Skills: Databases SQL/NoSQL 4+ years Cloud platform experience 3+ years SQL 4+ years Schema design 2+ years Amazon Web Services (AWS) 2+ years Deployment pipelines 2+ years Python 2+ years Deploying to production systems with active customers 2+ years Distributed computing (e.g Spark, Hadoop etc.) 3+ years Infrastructure monitoring 1+ years Wide variety of data warehouse, data lake (s3) etc familiarity Analytics experience working with structured and unstructured data Project lead (self-managing) 1+ years Bachelors in technical field or experiential equivalent Ideal Skills: Amazon Web Services (AWS) 3+ years AWS Lambda, Sagemaker Docker / Kubernetes DB performance engineering Machine Learning (ML) 1+ years Running ML on GPUs 1+ years Python 3+ years Strong analytics intuition grounded in significant experience Experience in the healthcare space Masters in technical field or experiential equivalent",3.0,"Ginger
3.0","San Francisco, CA",-1,1 to 50 Employees,-1,Unknown,Consumer Products Manufacturing,Manufacturing,Less than $1 million (USD),-1
Data Engineer - Threat Analysis,-1,"Job Title: Integrity Science Engineer I
Location: Menlo Park, CA
Duration: 1 Year

Summary:
We are looking for engineers with extensive experience investigating and analyzing online abuse, and actively implementing countermeasures.
Our focus on data analysis and machine learning as well as our knowledge of the robust infrastructure of our back-end systems allows us to collaborate seamlessly with engineering, data science, and operations teams to build tools and techniques necessary to enforce content quality at scale.
Integrity Science Engineers have the opportunity to work on many of the most challenging, complicated, and high-visibility integrity risks the company faces.
The potential for impact with this work is substantial, as outcomes could affect the billions of people who use our products.
Successful candidates for this team have a bias toward action and enjoy finding patterns amid chaos, making quick decisions, and aren't afraid of being wrong.
The perfect candidate will have a background in a quantitative or technical field, will have experience working with large data sets, and will have in-depth experience in data-driven decision making.
You are scrappy, focused on results, a self-starter, and have demonstrated success in using analytics to reduce abuse and negative user experiences across the platform.

Responsibilities
Identify and investigate online bad actors
Work closely with others to identify sources of abusive behavior and take countermeasures to stop abuse
Lead technical investigations from start-to-finish and effectively communicate actionable results to different audience types
Manage multiple projects at once while prioritizing time based on team priorities
Minimum Qualifications
2+ years experience doing quantitative analysis
BA/BS in Computer Science, Math/Finance, Physics, Applied Economics, Statistics or other technical field.
Experience investigating and acting on high-impact threats and online threat actors
Experience with relational databases and SQL
Experience thinking critically and qualifying assessments with solid communications skills
Experience coding in at least one of the following: PHP, Python, Haskell, or Java.
Advanced degrees in Computer Science, Math/Finance, Physics, Applied Economics, Statistics or other technical field.
Experience prioritizing and executing with minimal direction or oversight
Experience working across the broader integrity/security/Trust & Safety community
Experience in adversarial space like fraud and spam.
Preferred Qualifications
Proven track record of managing and executing on short term and long term projects
Development experience in Haskell.
Familiarity with cross-platform integrity threats
Demonstrated understanding of security and integrity risks

Skills:
DATA SCIENCE
HASKELL
JAVA
OPERATIONS
PHP
Additional Skills:
PYTHON
QUANTITATIVE
QUANTITATIVE ANALYSIS
CODING
DATA ANALYSIS
DATABASES
ENGINEER
FINANCE
MACHINE LEARNING
MARKETING ANALYSIS
SELF-STARTER
SQL
STATISTICS

IND123",4.6,"Ursus
4.6","Menlo Park, CA",-1,1 to 50 Employees,2015,Company - Private,IT Services,Information Technology,Unknown / Non-Applicable,-1
Staff Data Engineer,$103k-$184k (Glassdoor Est.),"Staff Data Engineer More than 7 billion people depend on farms for healthy and affordable food. By 2050, the global population will reach 9.4 billion and farmers will need new ways to deliver on this demand. They'll be seeking options that are economically viable and that will help them remain environmentally sustainable. Granular's software meets this need, making farmers more efficient. We're looking for a Staff Data Engineer. This role is integral to Granular's success and is a unique opportunity to join a mission focused, fast-paced, smart and fun team. What You'll Get To Do: Design, develop and deploy data pipelines and machine learning models at broad geographic scale, including crop yield models covering millions of acres of agricultural land in the United States. Be a mentor within the data science team, especially when it comes to systems design and architecture. Contribute to Granular's success by building machine learning systems for yield prediction, disease identification and other powerful data science features that help farmers run better businesses. In addition to building new pipelines and features, you will be a mentor for the rest of the team, particularly regarding data pipeline architecture, code quality, maintainability, and automation. You will drive data pipeline design decisions, and help us build systems that are well-tested, scalable, and automatable. Who You Will Do It With: We are a team of over a dozen data scientists with backgrounds ranging across ecology, economics, history, earth systems, and engineering. What we have in common is a passion for data, for solving real-world problems, and for building tools that amaze our customers. We're a team that loves to learn. Who You Are: You are an experienced Python programmer with 3+ years of professional data engineering experience. You are an experienced data engineer. In previous roles you have designed and developed data pipelines for ML, and have deployed and maintained them at scale. (Experience with either deep learning or gradient boosting is desireable but not required: you have either developed ML models yourself, or have worked alongside data scientists in your previous roles.) Experience with spatial data (GIS) is a big plus but is not required. Experience with AWS and/or Google Cloud is preferred. Experience with terraform, AWS Lambda, and Postgres/PostGIS are all a huge plus. You are a phenomenal teammate! You have real-world experience with code review and mentoring other team members, and have worked closely with data scientists, data engineers and product managers in past roles. You design and write maintainable code and data pipelines. You have experience with automation and testing, as well as with monitoring ML models that have been deployed to production. Perks and Benefits: Upward mobility. We are growing and need talent who can take on increasingly ambitious and rewarding roles Generous Vacation 401k Matching Program Employer sponsored Medical, Dental, and Vision Vision, Dental and Dependent Care FSA Family, Maternity and Paternity Leave Learning Programs and Individual Development Support Open floor plan and dog-friendly offices Who We Are: Granular is Farm Management Software (FMS) that is helping thousands of farmers to build stronger farms today and steward their lands for generations to come. Granular uniquely combines an industry-leading support team with the most recommended suite of easy-to-use powerful software to help farmers and their teams run all aspects of their farm business. From financials to agronomy to operations, farmers are now able to make decisions with greater confidence in an increasingly challenging environment. Granular was founded in 2014 with the mission of helping farmers run more profitable and efficient business instead of maximizing bushels. Built as a collaboration between several of the most advanced family farms and top technology investors including Google Ventures, Granular is widely credited for launching the first modern, built in the cloud and mobile-centric FMS in 2014. In 2017, Corteva Agriscience acquired Granular to bring together the best technologists, scientists, and entrepreneurs from across the world with its global presence and deep resources to serve the needs of farmers through the transformative effects of digital technologies. Today, our over 400 passionate employees are on a mission helping our customers solve their problems today and for food demands in the future by shaping agriculture through continuous innovations in farm productivity, profitability, and environmental sustainability. Granular is San Francisco-based, with offices worldwide. Granular is an independent, wholly-owned subsidiary of Corteva Agriscience (NYSE:CTVA), a spin off of DowDuPont™. Learn more about our company and people at http://granular.ag/, Twitter, @GranularAg, and Instagram, @Granular_Ag, or facebook.com/GranularAgriculture/ We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation.",4.4,"Granular
4.4","San Francisco, CA",-1,201 to 500 Employees,2014,Company - Public,IT Services,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Energy Asset Management",$74k-$139k (Glassdoor Est.),"The Role
Tesla is committed to having industry-leading uptime and service in our deployed fleet of residential, C&I and utility scale solar, storage and charging systems. The Tesla Energy Asset Management team is looking to hire a Data engineer to help build out and maintain a world-class Network Operations Center (NOC) that our global asset management and engineering teams will use to proactively monitor and operate our energy fleet, with the goal of maximizing availability and minimizing operating costs.

Responsibilities
• Design, build and deploy efficient, scalable & reliable data pipelines to move and transform data (ETL)
• Optimize existing pipelines and maintain all domain-related data pipelines
• Create, consolidate and maintain a robust data infrastructure that will be the foundation for the real-time data visualizations in the NOC

Must have requirements
• BS or higher in computer science/engineering, software engineering or proof of exceptional skills in related fields, with practical engineering experience
• 2+ years of hands-on experience using SQL, python and Hive to build and deploy production-quality ETL pipelines
• 2+ years hands-on experience using Hadoop, Hive or another MPP database system like AWS Redshift
• Experience with Airflow scheduler/executor framework
• 1+ year hands-on experience using SPARK (Pyspark) in production environment
• Strong aptitude to collaborate with different internal stakeholders to understand requirements and developing systems and tools to meet them
• Excellent oral and written communication skills
• Superior knowledge with at least 2 of the following: Presto, MapReduce, Cassandra, Kafka
• Familiarity with database performance concepts like indices, segmentation, projections, and partitions.

Tesla participates in the E-Verify Program",3.5,"Tesla
3.5","Fremont, CA",-1,10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$2 to $5 billion (USD),-1
Data Engineer,-1,"Location: San Jose CA Duration: 12 Months Job Description : Profile should have more than 8years of experience Hands on experience in designing and executing projects on Google Cloud Platform features like App Engine, Compute, storage, Big Query, Data Proc, Data Flow. Strong Programming Skills in R, Python or Spark. Strong Knowledge on Data Engineering, Simulation and Modelling concepts Proficiency in handling the billions of structured or unstructured transactional data. Proficiency in modeling techniques such linear regression, logistic regression, GLM Knowledge on machine learning techniques such as Decision Trees, xgboost, random forest, PCA etc. Knowledge on unsupervised Machine learning techniques such as Clustering, Segmentation Strong knowledge on Data Manipulation and transformation Knowledge on data loading to GCP services like big query, cloud storage. Knowledge in Hadoop, HIVE and Pig languages. Good communication skills.",-1,Kodeva,"San Jose, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer,-1,"Senior Data Engineer

SAN FRANCISCO

$140,000- 170,000 + BENEFITS & EQUITY

We improve the employee healthcare system by creating a single point of view that allows employers to simplify the healthcare coverage their employees get. Our easy to use integrations make it so employers don't have to worry about managing different vendors, saving your team and valued employees the time and headache. On top of that, we provide predictive analytics which enables companies to adjust and improve their healthcare offerings to their employees.

THE ROLE

You will be working on efforts from requirement to production and solving problems of data modeling, data flow and quality of data
Integrating analytics tools with the company's data platform
Write code that is well-tested and production level
Develop and build new data pipelines with issues pf data sources.

Your Skills & Experience

Proven cloud experience, AWS preferred
Commercial experience coding with Python
Experience building web applications from scratch through to production
Previous experience implementing microservices

How To Apply

Please register your interest by sending your resume to Kyle Wong via the apply link on this page",4.1,"Harnham
4.1","San Francisco, CA",-1,51 to 200 Employees,2006,Company - Private,Staffing & Outsourcing,Business Services,$25 to $50 million (USD),-1
Data Engineer,-1,"Function: Engineering

Position Type: Full Time

Position Level: Mid Level

Travel: Minimum travel may be required

THE OPPORTUNITY

You’ll work with the data team to work on our DBT transformations, and own analytics APIs that serve data to our product. You’ll implement workflows with AWS Lambda, Airflow and DBT while interfacing with the product and marketing team to optimize our targeting and core conversion flows. While this work will be largely done in Python and SQL, you’ll work closely with Mobile and Web engineers to pull in data from our Analytics APIs and advise on how to implement analytics on the client. You will have the opportunity to push data culture at the company and get data in the hands of the executive team needed to drive the business forward.

THE TEAM

NurseFly is small and rapidly growing. You’ll be working directly under our Head of Data as well as working cross functionally with all departments, including engineering, marketing, design, and business development.

THE SKILL SET

Expert Level SQL
Python
Experience with DBT
Experience building APIs
Basic Knowledge of Javascript ( Nice to Have )
Experience with Airflow ( Nice to Have )
Experience with AWS Lambda ( Nice to Have )

WITHIN ONE MONTH, YOU’LL

Have spun up our DBT project and Lambda environment locally
Have Implemented More than one Model in DBT to power our Business Intelligence efforts
Have implemented Unit Tests on our Analytics APIs and DBT Project

WITHIN THREE MONTHS, YOU’LL

Have rolled out Airflow to synchronize our Data Engineering Efforts
Have committed code to our analytics API that powers data in the product experience for both clients and Health Care Professionals
Implemented data flows that have had measurable impact on our marketing

WITHIN SIX MONTHS, YOU’LL

Helping to steer the direction of the data team
Helping to recruit and mentor mid-level data engineers and analysts
Own and maintain a complex set of pipelines from a number of sources that provide our team with industry leading intelligence
Optimized the performance of our pipeline to reduce the lag time of our analytics products

WITHIN TWELVE MONTHS, YOU’LL

Be a subject matter expert in both the industry and our data
Continue to contribute to the rapid growth and increased robustness of our data engineering and analytics practice
Power data to predictive models that provide a personalized experience for our users

BENEFITS & PERKS

Comprehensive Health Insurance Plans including HSA and FSA Options
Pet Insurance
Flexible Time Off
401K Retirement Savings Plan with Generous Employer Match
Access to Corporate Discount Program
Pre-tax Commuter Benefit Plan
Budget for your dream battle station (computer, monitors, keyboard, mouse, headphones, IDE and whatever else you need to do your best work)

NurseFly is proud to be an Equal Opportunity Employer. We provide equal employment opportunities to all employees and applicants without regard to race, color, religion, sex, age, national origin, disability, veteran status, pregnancy, sexual orientation, or any other characteristic protected by law.

We do not accept resumes from agencies, headhunters, or other suppliers who have not signed a formal agreement with us.",4.6,"NurseFly
4.6","San Francisco, CA",-1,1 to 50 Employees,-1,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Principal/Architect Data Engineer,-1,"ABOUT SIDE At Side, we believe everyone should own their path. Side is the only real estate brokerage platform exclusively for the nation’s best agents. Based on its belief that homeownership is a fundamental human right, we provide top-performing real estate agents, teams, and independent brokerages with the best system, support, and service, in order to elevate the experience and results of buyers and sellers. Led by experienced industry professionals and world-class engineers who develop technology designed to improve agent productivity and enhance the client experience, Side is backed by $70M in funding from top-tier venture capital firms including Sapphire Ventures, Trinity Ventures, Matrix Partners, MetaProp, and other real estate investors. Headquartered in San Francisco, Side currently has operations in California, Texas, and Florida. At Side, you’ll have the opportunity to collaborate and innovate your way to success. Becoming an inSider means that you’ll be empowering business leaders to become business owners, all while forging your own path with like-minded entrepreneurs. Join us at Side and own your career, your impact, and your life. ABOUT THE TEAM The Engineering team comprises 20 folks total, on four teams: Core Engineering, Marketing Engineering, Internal Tech Tools Engineering, and QA. We appreciate diversity of thought and backgrounds. Platform Engineering is responsible for building and architecting the foundational tech elements that support other engineers and the organization. What's also really cool is that we share office space with our real estate agents, giving us unparalleled access to feature ideas and feedback on product development. ABOUT THE ROLE As a member of the Platform Engineering team, you will work cross-functionally with the larger engineering organization, product team, operations team, brokerage, finance - almost every team in the company. You architect solutions to problems, help to facilitate adoption and communicate with leadership the needs of the platform. You will be bringing data to the forefront of how the company operates. WHAT YOU'LL BE DOING Architect and build a data warehouse solution from ground up to provide a single consistent view across the company Defining, developing, and optimizing datasets and schemas Develop, drive, and execute the long term data vision and strategy for the data platforms by working with multiple teams and stakeholders across the company. Standardize data quality by establishing metrics, tools, and processes Evangelize and establish data driven decision making strategy Serve as an internal consultant for MIS and engineering on data modeling WHAT WOULD MAKE YOU A STRONG FIT FOR THE ROLE? Extensive hands on experience in data engineering and architecture Extensive knowledge in data modeling concepts and implementation Advanced experience with SQL, ETLs and data analysis Advanced knowledge of cloud based data warehouses (eg. BigQuery, Redshift, Snowflake) Experience with data streaming and monitoring approaches 8+ years working experience in data engineering and architecture 8+ years of engineering experience building clean, maintainable, and well-tested code. Effective communicator who can translate complex technical concepts into simple audience based responses. COMPENSATION/PERKS Competitive salary Stock options Best-in-class benefits, including 100% healthcare coverage (medical, vision and dental) Flexible PTO Learning & Development credit Pet friendly environment Side is dedicated to working with the highest skilled people from the most inclusive talent pool feasible. We maintain that diversity in all aspects leads to positive change, solutions and innovation for our customers and career fulfillment for our employees. All qualified individuals are encouraged to apply. Side uses the E-Verify employment verification program. Our stewardship of the data of many of our customers means that a background and DRE license check is required to join Side. We will, nonetheless, consider qualified applicants with arrest and conviction records in accord with applicable law, including the San Francisco Fair Chance Ordinance.",4.4,"Side, Inc.
4.4","San Francisco, CA",-1,51 to 200 Employees,-1,Company - Private,Real Estate,Real Estate,Unknown / Non-Applicable,-1
Data Engineer,-1,"**Please Read** Local candidates only. This opportunity does not provide Visa sponsorship. No corp to corp applicants please. Candidate must be available to work on our W2. Data Engineer Data applications are critical to our success, powering many aspects of our marketplace and supporting products. We are looking for data engineers who will build, migrate and maintain data pipelines. In this role, you’ll expand and refactor the data sets that generate and transform data into applications, insights, and experiences for our users. The work includes: ? Refactoring existing and build new data pipelines ? Migrating existing data sets into next-gen reporting frameworks and tools ? Using existing data tools and frameworks to configure reports and metrics ? Developing and automating large scale, high-performance data processing systems to drive our business growth and improve the product experience ? Building and refactoring scalable data pipelines on top of Hive and Spark leveraging Airflow scheduler/executor framework We are looking for engineers with: ? Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and drive effective product solutions ? Experience designing and deploying high performance systems with robust monitoring and logging practices ? Experience building high performance data pipelines ? Nice to have: proven ability to think critically about team direction and use analysis to inform that ? Experience using machine learning is a plus, but not required. ? Excellent communication skills, both written and verbal",-1,FutureSoft IT,"Sunnyvale, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer (Cohort Creation API),$108k-$195k (Glassdoor Est.),"The Opportunity at Komodo Health Generation of a medically-related cohort is the gateway to all the work we do at Komodo Health. As a member of the team responsible for Komodo's standardized approach to healthcare cohort generation (Prism), you are responsible for the backbone data retrieval and transformations required by our internal service layer used to serve cohort generation along a variety of axes - patients, payers, providers, etc. You are responsible for ensuring that complex cohort generation queries are supported through the cohort generation API, adhere to SLAs, and for the underlying caching necessary to support multiple cohort generation access patterns. Looking back on your first 12 months at Komodo Health, you will have… Expanded or enhanced the Prism internal data model to ensure timely performance for all types of cohort generation requests Recommended and implemented additional pre-calculation and caching strategies as needed to ensure that SLAs are met Promoted API versioning best practices and open API standards in development Leveraged relevant AWS services (API gateway, lambdas, among others), updating as needed to take advantage of new beneficial features in support of the API Partnered with product managers to understand the variety of internal and external cohort specification needs across a diverse set of use cases Ensured non-functional requirements are met, such as around developer experience and maintainability Managed technical dependencies between different microservices to ensure smooth operations Set a high technical standard overall and be a mentoring resource for others on the team and in the larger organization What you bring to Komodo: Significant experience designing APIs for use in web application development (synchronous vs asynchronous response patterns, caching strategies, etc) Significant experience optimizing data retrieval processes supporting API output, as above, ideally within a low query volume / high data volume environment Demonstrably deep experience with Python Demonstrably deep experience with relevant 'big data' processing either via Spark or through a modern MPP database like Snowflake, ideally with experience in both Experience with separate caching/cache invalidation strategies Understand and design for non-functional concerns such as performance, cost optimization, maintainability and developer experience Strong communication with engineers, product managers, and salespeople",3.5,"Komodo Health
3.5","San Francisco, CA",-1,201 to 500 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Data Engineer - SAP Callidus,$70k-$131k (Glassdoor Est.),"Overview: We are the Gen-Next technology solution provider, solving complex business problems. We assist our happy customers across all spectrum to reimagine their business processes and navigate their digital journey. Our out-of-the-box solutions continue to bridge the gap between ideas and reality, delivering impact through connected data, architecture and experience. WE ARE ENQUERO - DIGITALLY TRANSFORMING businesses since 2014! Excellence with Enquero We are a vibrant and a ridiculously norm-challenging bunch with increasingly diverse teams across the globe. Our open culture enables easy ideation, innovation and delivery. Our voracious appetite for knowledge makes growth organic to us. If you have a passion for problem solving and quirkiness doesn’t scare you, come be a part of our rapidly growing company. Oh! we do have a life outside work as well. Responsibilities: As a Data Engineer in Enquero’s Data & Analytics unit you will be part of a fast-paced team designing, developing, testing, integrating, and supporting technically innovative solutions for our Fortune 500 customers. You will leverage your wide range of experiences, developed professional concepts as well as understanding of the industry, customer, and company objectives to resolve complex issues in creative ways. We expect your passion for technologies and your ability to work on issues where analysis of situation or data requires review or relevant factors. You will be responsible for leading the task assigned, which may include end-to-end ownership of the software stack. Your demonstrated ability to consistently achieve this while building quality software and diversifying your own knowledge in the areas broader than your programs/assignments will define success in this role. Work in an agile development environment. Follow standard practices and procedures in analyzing situations or data from which answers can readily be obtained. Embrace new technologies and document the process and train team members. Work with various tools and technologies to achieve desired functionality. Communicate with leaders, business analysts, project managers, IT architects, technical leads and other developers, along with internal customers, to understand requirements and develop needs according to business requirements Contributing to your BU/Practice by Documenting your learnings from the current work and engaging in the external tech community by writing blogs, contributing in Github, Stack overflow, meet-ups/conferences etc. Keep updated on the latest technologies with technology trainings and certifications Actively participate in organization level activities and events related to learning, formal training, interviewing, special projects etc. Qualifications: REQUIRED At least 5+ years of experience with hands-on implementation knowledge of Callidus Commissions component / SAP Commissions. Leverage features of the CallidusCloud to develop specific rules that automate the variable compensation processes around sales activities for Sales team members. Extensive knowledge of hi-tech sales compensation best practices in quota based compensation and variable compensation. Understanding of internal data models of CallidusCloud. Ability to extend the architecture with customizations, extensions and plug-ins. Experience in handling very high volume of transaction processing systems, Integration (in-flow and out-flow) with CRM/ERP and Datalake Analyze and review internal sales comp plans, processes and policies, Recommend solutions for sales credit, compensation, reporting/analytics, for calculating payments on CallidusCloud. Perform technical duties including: requirements analysis, Callidus-specific compensation plan design, compensation plan implementation and testing, end user sign off, and solution deployment. Maintain data quality in CallidusCloud via constant data analysis, cleanup, reporting, and updates. Excellent communication skills and work very closely with Compensation COE, Business Units and HR comp team to gather new requirements, understand Functional/Technical/Performance issues and drive more value leading the solution and communication from front. In depth knowledge of CallidusCloud Commission components such as: Salary, Salary + bonus, Salary + commission, Only Commission, Variable Commission, Quota based commission, Calls based commissions, Lead Generation commission. Experience in migrating complex compensation environment involving 10,000+ sales agents from homegrown to Callidus or non-callidus to Calldus platform. Hands-on experience in migrating at least 4+ complex Sales comp use cases. Experience in developing and customizing objects in CallidusCloud Commission to provide seamless experience post integration is a must. Understanding of workflows, plan logic, Modeling, Gamification, multiple orgs configuration, Personas, and guided experiences to create and manage commissions. Experience in building complex plans & workflow based solutions for Bonuses, Accelerators, Thresholds, Draws, Recovery, Overachievers, Currency Conversions, prorations, cascaded goalsheets, ragged hierarchies and more. Knowledge of compensation plans and bonuses for direct , indirect compensation and partner compensation models. Capable of configuring various reporting options for – Sales Reps, Sales Managers, Finance etc. Ability to roll-up on a sales hierarchy but manage reporting and security on HR hierarchy. Excellent analytical and problem-solving skills related to CallidusCloud Commissions. Knowledge of Data quality, reconciliation and troubleshooting. Extensive knowledge of Comp plan modeling and design in Sales Strategy & Compensation Management for strategically align to organization’s Go-To-Market strategy. Knowledge of at least one additional compensation product other than Callidus. Disclaimer: This job description is intended to describe the general nature of the work performed by employees in this job. It is not an exhaustive list of all the job's responsibilities. At Enquero, all of our jobs include broad responsibilities for continually improving the processes we use to develop our products. Enquero is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law.",3.9,"Enquero Inc
3.9","Milpitas, CA",-1,501 to 1000 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,$50 to $100 million (USD),-1
Senior Data Engineer,-1,"Palo Alto, CA

Step is a next generation financial services company building the best banking experience to help teens and young adults achieve financial independence and knowledge at an earlier age. Step’s mission is to create a bank that gives you the tools to become smarter with your money and reward you along your financial journey. Step is a well-funded Series A company focused on disrupting banking through a differentiated mobile-first consumer experience. The founding team has extensive background in FinTech and building successful consumer products and brands. If you’re looking to join a fast-growing company with a strong mission and vision that puts people first, we want to meet you.

Role

As a Senior Data Engineer, you will implement and manage Data Models and ETLs that will enable fraud analysis and business analytics. You are a self-starter and you are comfortable working cross-functionally with other teams across Step.

Responsibilities
Partner with various stakeholders within the business, understand their data and reporting requirements, and translate them into definitions and technical specifications
Be responsible for defining, developing and optimizing curated datasets and schemas with standardized metrics and definitions across the company
Develop, deploy and maintain ETL jobs and visualizations
Troubleshoot technical issues with platforms, data discrepancies, alerts etc
Perform ad hoc analysis, insight requests, and data extractions to resolve critical business issues
Must haves
5+ years working experience in a successful data engineering or business intelligence team
Expert knowledge in data modeling concepts and implementation
Strong technical accomplishments in SQL, ETLs and data analysis
Hands on experience in processing large data sets
Strong business intuition and ability to understand complex business systems
Core technologies we use
PostgreSQL
BigQuery
AirStream
Apache Beam/Spark
Working at Step
Competitive salary based on experience, with full medical and dental benefits
Stock in an early stage startup that is growing
Flexible WFH and vacation policy
Free daily lunches
Free snacks & drinks
Monthly team events outside the office
Monthly stipend for commuter benefits and card testing
Office right next to Caltrain",3.8,"Step Mobile
3.8","Palo Alto, CA",-1,1 to 50 Employees,2018,Company - Private,-1,-1,Less than $1 million (USD),-1
Big Data Engineer,$80k-$148k (Glassdoor Est.),"We continuously seek exceptional associates when recruiting
new employees. We pride ourselves on having extensive experience working with
clients in all major markets. Cognizant's delivery model is infused with a
distinct culture of high customer satisfaction. We consistently deliver
positive relationships, cost reductions and business results. At Cognizant, we
believe those who challenge the way they work today will own the way tomorrow.

Big Data Engineer

Primary Skills: Big Data, Hive, Apache Hadoop

Secondary skills: Azure
Shell scripting Python Kubernetes etc.

Must have at least 5 years
of experience working on Big Data technologies

Good experience in Hive
Sqoop Spark UNIX Batch Scripting Shell Scripting Cloudera Hue

Hands on experience in Java
Hadoop HBase HDFS YARN Spark HIVE Impala

Strong in ETL and should
scale up in Big Data and Azure Data Factory Data Factory ADF

Must have deep
understanding of distributed systems

Should have good
communication skills

Cognizant
will only consider applicants for this position whom are legally authorized to

Work in the United States without
company sponsorship (H-1B, L-1B, L-1A, etc.)

Employee Status : Full Time Employee
Shift : Day Job
Travel : No
Job Posting : Oct 01 2020

About Cognizant Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 194 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.

Cognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service.
Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.",3.7,"Cognizant Technology Solutions
3.7","Pleasanton, CA",-1,10000+ Employees,1994,Company - Public,-1,-1,$10+ billion (USD),-1
"Data Engineer, Supply Chain",$115k-$187k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

How would Facebook scale to the next billion users? The Infrastructure Supply Chain group is responsible for the strategic analysis to support and enable the continued growth critical to Facebooks infrastructure organization. We are looking for a Data Engineer to not only build data pipelines but also extend our data tools. As a Data Engineer, you will develop a clear sense of connection with our organization and leadership - as Data Engineering is critical investment to scale analytics capabilities and drive strategy. This is a partnership-heavy role. As a member of Infrastructure Supply Chain Analytics team, you will belong to a centralized Analytics and Data Engineering team who partners closely with teams in Facebooks Infrastructure organization. Through the consulting-nature of our team, you will contribute to a variety of projects and technologies, depending on partner needs. Projects include analytics, ML modeling, tooling, services and more.
Partner with leadership, engineers, program managers and data analyst to understand data needs.
Design, build and launch efficient and reliable data pipelines transforming data into useful report ready datasets.
Communicate at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.
Use your data and analytics experience to see whats missing identifying and address data gaps, build monitor to detect data quality issues and partner to establish a self-serve environment.
Broad range of partners equates to a broad range of projects and deliverables including ML Models, datasets, measurements, services, tools and process.
Leverage data and business principles to automate data flow, detect business exceptions, build diagnostic capabilities, improve both business and data knowledge base.
Build data expertise and own data quality for your areas.
5+ years of SQL experience.
4+ years of Python development experience.
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
3+ years experience with Data Modeling.
Experience analyzing data to discover opportunities and address gaps.
5+ years experience in custom ETL design, implementation and maintenance.
Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
Experience with more than one coding language.
Experience in designing and implementing real-time pipelines.
Experience with data quality and validation.
Experience with SQL performance tuning and e2e process optimization.
Experience with anomaly/outlier detection.
Experience with notebook-based Data Science workflow.
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.
Experience working with Supply Chain or Data Center Operation team.
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Fremont, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Principal Data Engineer,-1,"The most successful companies understand that data and analysis are the foundation of any effective business strategy. Modern enterprises live and breathe data that they leverage through all sorts of dedicated analytics platforms: customer, product, supply chain, finance, etc. But until now, there's never been an effective software solution to understand, measure, and optimize companies' most important asset: their brand. Upwave is a venture-funded startup building the Brand Intelligence Platform. We offer a comprehensive set of tools that help our Fortune 100 clients understand and improve how they're perceived in the marketplace, how aware consumers are of the value they provide, and how much consumers trust them to provide the value they promise. Everything we do rests on a foundation of data, and as a data engineer you'll help us keep extending that foundation and keep it robust and strong. We deal with complex, fast-moving data at massive scale and you'll need every trick in the distributed systems book to make sure our clients always have the data they need, when they need it, how they need to see it in order to successfully allocate tens of millions of dollars marketing budget. Upwave's business is growing rapidly and our data systems have started to become victims of their own success. We're looking for a deeply experienced, deeply knowledgeable technical leader who can help us architect and build a new generation of big data systems to carry our business into the next phase of growth. Our systems deal with tens of billions of data points each month, and you'll be the primary person responsible for making sure that our capacity goes up even as our costs go down. Although this isn't a management role (though it might grow into one, if and only if you're interested), we do expect you to play a key role in training the rest of our data team and in setting and maintaining the highest technical standards while also serving as a role-model for healthy, collaborative communication. Your responsibilities will include: Creating and maintaining data ingestion, transformation, storage, and analysis systems that are critical to our ability to meet our obligations to our customers and to deliver market-leading new products and features. Introducing and applying cutting-edge technologies and techniques around big data, distributed systems, analytics, microservices, data pipelines, and observability. Owning projects through their entire lifecycle, from conception to architecture to implementation to maintenance. Growing our team both through helping shape our future hiring and through inspiring, mentoring, and upskilling our existing engineers. Embodying and modeling the discipline of practical, professional engineering in your day to day development process and practices. What we're looking for: You get excited about billions of events and terabytes of data. You know how to design large scale systems that are reliable, scalable, efficient, maintainable, extensible, and elegant. You deeply understand the power and promise of distributed systems, and you have enough experience building them to know where the pitfalls are and how to avoid them. Idempotence excites you. You love the feeling of seeing a powerful computing cluster fully and efficiently utilized. You are deeply familiar with the Apache big data ecosystem. No one has used every tool, but you should have used several of them extensively in production (and you should know the basics of most of the major players). You should be able to intelligently discuss the tradeoffs that different tools make, and e.g. when to use Hadoop vs. Spark or Hive vs. Presto or Hbase vs. Cassandra. You've used a lot of the services in a major public cloud (preferably AWS). You love some, are frustrated by others, and can't wait to try the next one. You love to learn. You understand that nothing stands still in technology, and you're excited when newly available tools unlock new and radically better ways of solving problems. But you also know when to apply tried and tested tools and techniques. You have strong hands-on experience writing enterprise-grade applications and services in Java or another language on the JVM. People tend to look to you as a leader and respect your expertise, even in roles where you don't have formal authority. You understand that your software runs on computers (but you architect things so you spend the minimum amount of time possible thinking about that). You are a strong believer in the best software development principles (e.g. SOLID OO-design, design patterns) and processes (e.g. TDD, peer reviews, automation) You have experience mentoring junior developers, and understand that healthy human systems are essential to developing and maintaining healthy technical systems. You thrive on the energy of operating in a fast-paced, ever-changing startup atmosphere. You are a self-starter and you love working self-driven in a dynamic team. The typical candidate who's reached the necessary level for this role has more than 8 years of professional experience and more than 4 working specifically on distributed systems and/or with the Apache big data ecosystem. But we care more about what you've accomplished than about how many years you've spent doing it. Bonus Points: Experience with Groovy, Grails, Spring Boot, DynamoDB, Athena, Cassandra, EMR, Kubernetes, Hive, Presto. Expertise in designing and developing scalable, robust and high performing backend services including efficient and optimized API and protocol design. You know when and how to develop utilize microservices but you also understand the trade-offs when applying SOA Solid DevOps experience. You understand that operational systems are everyone's responsibility, and perhaps you've deployed and maintained your own cluster. Experience with marketing or advertising technology. Upwave is the Brand Intelligence Platform. We make brand marketing more impactful. Upwave plans, measures, and optimizes brand marketing. With our Brand Campaign Measurement and Instant Insights products, we provide a software & data platform to the world's largest brand marketers. Our customers are Fortune 500 companies across multiple verticals - including CPG, food & beverage, consumer technology & telecom, and financial services - as well as the world's largest advertising agencies and media platforms. We are unapologetically supportive of brand advertising, and work hard every day to prove its value; we know if companies can measure the value of those dollars, they'll spend more. Brand advertising pays for not only the movies we watch and music we hear, but the journalism we read and the information we access. In short, brand advertising supports the free flow of information through society. So, we're proud to be the first company dedicated to using data science to show enterprises the true effectiveness of their brand spend. We are backed by leading venture investors (Y Combinator, Uncork Capital, Bloomberg Beta, Initialized Capital, PivotNorth, Ridge Ventures, Industry Ventures, Conductive Ventures,) and leading MarTech founders & CEOs. We're a humble but ambitious team that takes its work seriously but never ourselves. Come join us.",4.0,"Survata
4.0","San Francisco, CA",-1,51 to 200 Employees,2012,Company - Private,Internet,Information Technology,$5 to $10 million (USD),-1
Senior Data Engineer,-1,"At Afresh, our mission is to make the fresh food supply chain more efficient, thus dramatically reducing food waste and making fresh, nutritious food available and accessible to everyone. Our A.I.-powered solutions optimize ordering, forecasting, and store operations for fresh food departments for brick-and-mortar grocers. The results are powerful: grocers using Afresh reduce food waste by 25%+ and increase fresh sales by 3%, massively increasing their profitability.

We are a passionate team united by our mission to make an impact - we hope you’ll join us!
What will you be doing?
Building fast, reliable, scalable data pipelines that process over billions of historical data points collected from tens of thousands of retail stores across the US which power our machine learning models.
Integrating, analyzing, finding issues with, and solving shortcomings in the critical lifeblood of our system–the data our customers send us–that powers every aspect of our system.
Maintaining, augmenting, and helping architect our data platform and data warehouses to ensure that our data and ML teams can efficiently process and access the data they need.
Collaborating with an interdisciplinary team of experts in machine learning, data scientists, design, software engineering, and business process optimization
What skills and experience do you need?
Bachelors or Masters in Computer Science or equivalent.
3+ years of work experience.
Strong programming and problem-solving skills.
Expert-level knowledge of using SQL and data warehouses, data lakes, or databases.
Experience with Apache Spark or other Big Data frameworks; working with cloud infrastructure; data visualization tools; and applied statistics is a big plus.
About Afresh

Founded in 2017, Afresh is the first A.I.-powered fresh food optimization platform for grocery chains. We have partnered with several large grocers representing hundreds of stores and >$10B in revenue. Our cutting-edge AI research has been published in top journals (e.g., ICML). We’re backed by some of the top investors in grocery and tech including Innovation Endeavors (former Google CEO Eric Schmidt’s firm), Baseline Ventures (first money in Stitch Fix, SoFi, Heroku, Instagram), Food Retail Ventures, Maersk Growth, and Impact Engine.

We are building a vibrant, diverse, and inclusive team that embodies our company’s values: proactivity, kindness, candor, and humility. We aspire to continually grow as individuals and as an organization to live these values and realize our mission. We earnestly believe that Afresh represents a one-of-a-kind opportunity to have massive social impact at scale by employing novel technology—and to have a ton of fun along the way.

Afresh provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, sexual orientation, gender identity/expression, marital status, pregnancy or related condition, or any other basis protected by law.

Interested? Email us at Careers@afreshtechnologies.com",3.0,"Afresh
3.0","San Francisco, CA",-1,1 to 50 Employees,2009,Company - Private,IT Services,Information Technology,Less than $1 million (USD),-1
Data Engineer,-1,"Java experience 1) Hands-on backend engineer on Java/JEE experience on Java 8+ (preferably Java 11), 2) SQL/Postgres experience 3) Any experience with MongoDB is a plus DevOps experience 1) Kubernetes 2) Jenkins 3) CI/CD 4) Excellent SQL Development skills 5) Excellent Java services skills 6) Excellent communication and soft skills",3.9,"LTTS
3.9","Cupertino, CA",-1,51 to 200 Employees,1998,Company - Private,Preschool & Child Care,Education,$10 to $25 million (USD),-1
Senior Data Engineer (Life Science Products),$108k-$195k (Glassdoor Est.),"The Opportunity at Komodo Health The Life Sciences Products team is looking for Senior Data Engineers to help us build next generation data pipelines, data infrastructure and databases, and to develop data processing that is scalable, reliable and automated. The ideal candidate has demonstrated experience going from understanding business use cases and whiteboard designs all the way through to the nuances of implementation and rollout. The ideal candidate would have experience with product development and has gone through different phases of product growth and maturity. This is an opportunity to help solve complex challenges, and be a part of a team of folks accomplished in diverse Engineering disciplines; focused on using the best of what lies at the forefront of technology and skills to address complex, real-world problems in the Healthcare and Life Science space. Some of the tools we use are: Spark, Snowflake, Airflow, Python, AWS EMR, Kubernetes, and Docker. Looking back on your first 12 months at Komodo Health, you will have… Designed, developed, and implemented data infrastructure, data pipelines and data processing code for product features. Analyzed, reviewed and translated product management artifacts like product requirements documents and product roadmaps, into actionable engineering requirements documents. Created automation systems and tools to configure, monitor, and orchestrate our data infrastructure and our data pipelines. Evaluated new technologies for continuous improvements in Data Engineering. Collaborated closely with the Product and Customer Success teams to build out new data features. Collaborated with Data Scientists to implement descriptive, forecasting, and predictive algorithms and models using latest technologies. What you bring to Komodo: Experience with... Building and deploying large-scale, complex data processing pipelines. Python or Scala development, proficiency with at least one of them is required Pipeline scheduling and monitoring systems, like Airflow or Luigi Data processing platforms such as Spark, Amazon EMR, Google DataProc, Hadoop/MapReduce, etc. Data warehouse, modeling, and SQL experience such as Snowflake, Redshift, or BigQuery You've led planning, launching, optimizing, and refactoring phases of data pipeline platforms. Ability to work as part of a collaborative team in a fast-paced environment. Sincere interest in working at a healthcare startup and passion for healthcare data.",3.5,"Komodo Health
3.5","San Francisco, CA",-1,201 to 500 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"WW is looking for candidates to help change people’s lives. We are a global wellness technology company inspiring millions of people to adopt healthy habits for real life. We do this through engaging digital experiences, face-to-face workshops and sustainable programs that encourage people to move more, shift their mindset and eat healthier while enjoying the foods they love. By drawing on over five decades of experience and expertise in behavioral science, we build communities in order to deliver wellness for all. To learn more about WW and jobs with a purpose, visit ww.com. Position: Senior Data Engineer (Remote) Location:New York City Experience: 6-10 years Reporting to: Director, Data Engineering Job Description At WW, we are re-imagining engineering in and around our data infrastructure, architecture, pipeline, flow and security. We are building a modern data lake fed by batch and streaming pipelines creating incremental value from our data. We are implementing a data catalog, glossary and dictionary solution to make our data search and discoverable propelling data democratization, enhancing data quality and data security. We are reimagining and replacing old data pipelines with modern architecture and technologies. It’s an exciting time for WW Data Org and Data Engineering is driving the excitement. If you would like to be part of this exciting journey, help building our modern data engineering solutions and set yourself up for an impressive learning and career growth potential, we have an opportunity for you. If you feel passionate and excited to design and build data engineering solutions on Google Cloud Platform (GCP), we shall talk. Roles and Responsibilities Lead design, development, implementation and support of end-to-end data pipeline for centralized data lake on cloud Design and implement streaming data analytics platform using moder cloud based and open source technologies and Python/JavaScript/Scala programing language Build data catalog, data dictionary and make data searchable for our stakeholders and users Work with various Google Cloud Platform (GCP) technologies, Kafka, Airflow, Metadata Management tool, Data Quality (DQ) tool, Cloud-Native Microservice architecture, CI/CD, Dev/Ops and much more Design and implement data security, access control (including fine-grained) and compliance solutions to safeguard our data Work with high volume (100s of TB), high velocity (real-time stream), high complexity (heterogeneous) data from hundreds of sources e.g. social media, click stream, e-commerce etc. Mentor and guide junior team members, if hired at a senior position Effectively communicate data governance initiatives and values to stakeholders Experience & Skill Set If you have done similar works as above for 4 years or more, have some of the skillsets from below list and feel passionate and excited doing it again at WW, you have the right experience and skill sets. Let us talk Experienced of architecting and implementing modern data platform (data lake or data warehouse) with batch and streaming data ingestion and processing on GCP or AWS cloud Solid understanding of fundamental architecture and working principle of relational database, NoSQL database, massively parallel processing (MPP) database, distributed processing framework (MapReduce/Spark etc.), pub/sub messaging/streaming platform (e.g. Kafka, Google Pub/Sub, Kinesis) Experienced in SQL query, data manipulation language (DML) and data definition language (DDL) including hands-on experience of managing database tables and views Experienced of building Data App or Data Pipeline using Java/Python/Scala/Go programing language(s) Experienced and willing to lead and manage work for junior data engineers on the team Possess excellent work ethic, positive attitude to work and good verbal communication Can effectively document, present and pursue solution design and idea to stakeholders / teams Excellent work ethic, positive attitude to work and good verbal communication We hire only the best people. Here are the benefits to being top-notch: Competitive compensation and profit-sharing plan A 401K plan to help you plan for your future, plus company match Health care coverage starting on your first day Tuition reimbursement and online courses to help you reach your career aspirations Commuter benefits Yearly well-being allowance for your physical, financial, social and emotional well-being Free WW membership for you plus 3 free WW memberships for your friends and 3 for your family Free fruit, snacks and coffee to get you through your day Summer Fridays, happy hours, and company outings Robust employee referral bonuses Developmental opportunities and assignments to grow your career",-1,WW International,"San Francisco, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer,$148k-$260k (Glassdoor Est.),"Requisition ID # 55671 Job Category : Information Technology Job Level : Individual Contributor Business Unit: Information Technology Job Location : San Francisco Department Overview PG&E’s Information Technology (IT) organization is comprised of various unified departments which collaborate effectively in order to deliver high quality technology solutions. The Data & Analytics Team within IT is responsible for working collaboratively with the lines of business (e.g., Electric Operations, Gas Operations, etc.) to implement consumer-grade data platforms and products across various user groups (e.g., asset strategists, inspectors, emergency operations, etc.). This includes, but is not limited to: Deploying best-in-class / rapid delivery capabilities for data solutions and self-service enablement for highly skilled analysts and data scientists Simplifying, improving, and standardizing processes to promote data-driven decisions and drive the digital transformation of the utility. Rapid delivery of back-end solutions for a multitude of applications Position Summary We are looking for a Senior Data Engineer to be part of our exciting and growing team. Your primary focus is to build data products to support risk reduction and digital transformation of the company. As a productive member of a motivated group of engineers, product teams, and technical analysts, you will be producing ground-breaking, forward thinking, and revolutionary projects that are industry changing. What you will get: The opportunity to contribute to a best-in-class digital organization that ships real products to real users that have a direct impact to company operations and improvements to the safety of California Extreme leadership support for your development and your day to day success Autonomy to make decisions in a rapidly growing team Best in class perks and benefits Responsibilities: Build awesome big data products within a fun team where culture is the foundation for success Provide input on technical feasibility while collaborating with teams to guide features and provide technical know-how Provide subject matter expertise to design data products and workflows that are robust and fault tolerant Evolve technical solutions to incorporate new technologies Debug and optimize complex applications that have real impacts Build best-in-class cloud-native data products in AWS Effectively communicate the solution to non-technical stakeholders Maintain a productive working environment while fostering an open and honest culture of trust, accountability, and humbleness Be a strong advocate for a culture of quality Follow an agile development methodology Qualifications: Minimum: BA/BS in Computer Science, Management Information Systems, or equivalent experience and/or field of study 5 years of programming analysis experience Desired: 10 years of IT/Software Engineering experience 7 years of experience working in application development and architecture 5 years of experience developing solution architecture for complex technical problems Hands on experience working with multiple technologies and platforms Exposure to enterprise applications and ERP systems (e.g. SAP) Strong Data Engineering experience specifically including AWS Glue, Python, PySpark and SQL Experience in building and deploying through CI/CD (e.g. Jenkins) Working knowledge of Docker and Kubernetes Experience working in an Agile environment Experience with developing solutions for data driven applications in the cloud Experience in developing fault-tolerant applications Experience with instrumenting applications with health checks and proactive monitoring Excellent communication and collaboration skills and a strong work ethic Very strong analytical and problem-solving skills Experience with GIS Knowledge of SAP Able to build a sense of trust and rapport that creates a comfortable & effective workplace Able to further community & elevate team performance Brings a high-energy and passionate outlook to the role and has strong sense of ownership",3.6,"Pacific Gas And Electric Company
3.6","San Francisco, CA",-1,10000+ Employees,1905,Company - Public,Utilities,"Oil, Gas, Energy & Utilities",$10+ billion (USD),-1
Sr Data Engineer,$107k-$193k (Glassdoor Est.),"Current Employees of LendingClub: Please apply via your internal Workday Account

LendingClub (NYSE: LC) was founded in 2007 under the belief that a technology and data-driven marketplace can improve the way people access and invest in credit, creating value for both sides. Since then, we've helped millions of Americans take control of their debt, pursue their dreams, and invest in their future – all in a fair, transparent, and affordable way. Today we’re America’s largest online credit marketplace, facilitating billions of dollars in loans annually, and we’re leading the governance of a new industry by developing ethical, responsible ways to bring greater value and better opportunities to our members. Everyone deserves a better financial future and our team is committed to making that a reality.

About the Role

Here at LendingClub, as a Sr. Data Engineer, you’ll be part of the Data Technology organization that help drive business decisions using data. LendingClub’s business is data driven and you will build solutions to help the company make decisions about marketing, pricing, credit, funding, and many other business aspects, which is transforming the banking industry. We’re looking for talented Data Engineers passionate about building new data driven solutions with the latest Big Data technology.

What You'll Do

Develops and maintains scalable data pipelines and builds out new integrations and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using HQL and 'Big Data' technologies
Assemble large, complex data sets that meet functional / non-functional business requirements and fostering data-driven decision making across the organization
Implements processes and systems to validate data, monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Writes unit/integration tests, contributes to engineering wiki, and documents work.
Performs root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement

About You

A seasoned professional with 5+ years of relevant experience who is excited to apply their current skills and to grow their knowledge base.
Passionate with good understanding of with a focus on having fun while delivering incredible business results
BS or MS degree in Computer Science or a related technical field
Experience with data pipeline, data analytics, data warehousing and big data
Experience with SQL/No-SQL, schema design and dimensional data modeling
Experience with BigData technologies stack such as HBase, Hadoop, Hive, Oozie, MapReduce
Experience in AWS/Spark/Java/Python development is a plus
Familiar with Agile methodology, test-driven development, source control management and automated testing.

LendingClub is an equal opportunity employer and dedicated to diversity and inclusion in the workplace. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender identity, sexual orientation, age, marital status, pregnancy status, veteran status, or disability status. We believe that a variety of perspectives will make our teams and business stronger as we work together to transform the traditional banking system.",3.5,"LendingClub
3.5","San Francisco, CA",-1,1001 to 5000 Employees,2006,Company - Public,Banks & Credit Unions,Finance,$100 to $500 million (USD),-1
Big data Engineer,-1,"Job Description: Role:Big Data Engineer Exp: 10+ Years Location: Newport Beach, CA / Silicon Valley, CA / San Francisco, CA Visa: Other than Opt’s MOI: Phone + Skype Skills Required Strong ability create and maintain data pipelines and advanced analytic that capture data for reporting purposes Strong experience with scripting languages (Python) for building out the data pipelines Strong SQL experience Experience with Enterprise Data Management & Data Integration Experience with enterprise ETL tools such as Informatica PowerCenter, Informatica Cloud, or equivalent Experience with AWS: S3, Data Lakes, Redshift, RDS, Athena Java OR Scala development experience Hadoop, spark Regards Sundeep.B 732-790-5650 sundeep.b (at) primesoftinc (dotcom)",-1,primesoftinc,"San Francisco, CA",-1,-1,-1,-1,-1,-1,-1,-1
Data Engineer : 20,-1,"Primary Skills: Big Data Technologies, NoSQL, Spark, Java/Scala/Shell/Python Duration: 6 Months (Possible Extension) Contract Type: W2 Job Description Strong development skills around Hadoop, Hive, Pig Latin, Spark, Scalding, Map Reduce, Storm, Kafka. Knowledge of Spark and NoSQL databases. Have working experience in Google Cloud Platform. Strong skills in object oriented design, Core Java, Scala, shell scripting/Python Excellent problem solving and analytical skills Proven background in Distributed Computing, Data Warehousing, ETL development, and large scale data processing. Strong development skills in Python and SQL. At least 3-4 years of experience in Hadoop ecosystem and strong analytical skills. To follow up with any questions, please contact Khushbu at 408-512-2361 Akraya is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US. We offer comprehensive benefits including Health Insurance (medical, dental, and vision), Cafeteria Plan (HSA, FSA, and dependent care), 401(k) (enrollment subject to eligibility), and Sick Pay (varies based on city and state laws). If this position is not quite what you're looking for, visit akraya.com and submit a copy of your resume. We will get to work finding you a job that is a better fit at one of our many amazing clients. Akraya is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender, race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law. Akraya is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.6,"Akraya Inc.
4.6","Sunnyvale, CA",-1,51 to 200 Employees,2001,Company - Private,Staffing & Outsourcing,Business Services,$25 to $50 million (USD),-1
Principal Level Data Engineer,$110k-$194k (Glassdoor Est.),"Description

WHO WE ARE:

Freedom Financial Network is a family of companies that takes a people-first approach to financial services, using technology to empower consumers to overcome debt and create a brighter financial future. The company was founded in 2002 by Brad Stroh and Andrew Housser on the belief that by staying committed to helping people, you can ensure better financial outcomes for both the customer and the business. This Heart + $ philosophy still guides the vision of our growing company, which has helped millions of people find solutions for their financial needs.

What began with 2 people in a spare bedroom has now rapidly expanded to a vibrant business that employs over 2300 employees (known internally as The Freedom Family) in two locations: San Mateo, CA and Tempe, AZ. When you visit either of our offices, you’ll understand why our employees have voted us the Best Place to Work for the last several years. It’s a place where the Heart + $ philosophy continues to thrive, where we believe that success is only achieved by doing what’s right for our customers, our employees, and our communities.

In order to create brighter futures for our clients, employees, and businesses, Freedom Financial Network holds itself to four core values that have grown out of our Heart + $ philosophy: to care for everyone around us, act with integrity every time, collaborate with everybody we work with, and get better at what we do every day.

THE OPPORTUNITY:

As a Principal Level Data Engineer with Freedom Financial Network in its Tempe, AZ or San Mateo, CA office, you will contribute directly to the success of the business and have a meaningful impact on our customers’ lives.

We are looking for seasoned engineers to bring new and innovative ideas and tackle challenges across the full-stack. We believe in building teams that are passionate about solving complex problems and ready to grow and evolve alongside our expanding business and technology footprint.

THE ROLE:
Lead product requirements and advanced analytics requirements gathering efforts, interfacing with Data Engineering, Data Scientists, Analytics, and stakeholder teams
Own, evolve and help architect and design large part of the Data Platform
Use technology to solve business problems and drive positive outcomes
Build a scalable technology platform to support a growing business
Deliver high-quality code to production, consistently
Play a key role in building out a large scale distributed and event based Data Platform that serves as an underpinning for all of Freedom’s businesses and products
Recommend ways to improve data reliability, efficiency, and quality. • Contribute to data platform architecture and represent data team in various cross – team technical initiative •
Work with data infrastructure team to triage issues and support issue resolution. • Drive data team to adopt industry best practices. •
Mentor data engineers/interns
REQUIREMENTS/CHARACTERISTICS:
10+ years’ experience building large scale, real-time distributed systems with solid coding, problem-solving, and design skills
Experience architecting and design large scale distributed data systems
Ability to demonstrate a team-first attitude towards software development
Hands on Experience with Java, SQL and one data programming language (Python, R, etc)
Knowledge and experience building applied ML based applications highly preferred
Experience working with various data stores: SQL, NoSQL, and/or key-value store
Mature engineering practices (CI/CD, testing, secure coding, etc)
Experience with, or willingness to learn:
Event-driven architecture and messaging frameworks (Pub/Sub, Kafka, and/or RabbitMQ)
Cloud infrastructure (Google Cloud Platform, AWS, and/or Azure)
Excellent communication skills in written and verbal forms, and an ability to communicate complex issues to a range of audience (management, peers, clients). •
Strong attention to detail while excellent time management and prioritization in multitasking
EDUCATION:
Bachelor’s/Masters in Computer Science or relevant field, or relevant experience in software development
CULTURAL FIT (Our Core Values):
Care (for everyone): We show compassion and contribute to the well-being and growth of those around us. We only pursue products that improve the financial lives of our clients.
Act with Integrity (every time): We take the right action even when it is hard and even when no one is watching. We treat our employees, clients, and communities the way they wish to be treated.
Get Better (every day): We innovate, iterate, and improve each day. We are creative, take thoughtful risks, and ultimately learn and recover from failures.
COLLABORATE (with everybody): We strive to work together toward a common purpose by proactively sharing information and inviting participation. We recognize the perspective of various groups and embrace a healthy, constructive debate.
WHY JOIN THE FREEDOM FAMILY?
Fast, continued growth – there’s a lot of opportunity for advancement
Voted the Best Place to Work multiple times by our employees, most recently #1 in Phoenix for the 2nd year in a row!
Benefits start within 30 days
401k with employer match
3 weeks’ paid vacation (increased with tenure)
9 paid holidays & 5 sick days
Paid time off for volunteer work and on your birthday
Attention Agencies & Search Firms: We do not accept unsolicited candidate resumes or profiles. Please do not reach out to anyone within Freedom Financial Network (FFN) to market your services or candidates. All inquiries should be directed to Talent Acquisition only. We reserve the right to hire any candidates sent unsolicited and will not pay any fees without a contract signed by FFN’s Talent Acquisition leader.",3.4,"Freedom Financial Network
3.4","San Mateo, CA",-1,1001 to 5000 Employees,2002,Company - Private,Investment Banking & Asset Management,Finance,$500 million to $1 billion (USD),-1
Big Data Engineer,-1,"Squadex is a fast growing technology consulting and engineering company headquartered in Silicon Valley with the large R&D center in Ukraine. We offer DevOps & Data Science expertise to emerging startups and well-established enterprises at the US market.

The Role

We are looking for a seasoned data/algorithm engineer who has experience building and maintaining large scale ETL’s and data warehousing to build the next generation mission critical data platform. If you are comfortable with swimming in Petabytes of data to define quality metrics, explore trending and to write scalable code for data pipelines, come talk to us.

In this role, you will
Responsible for designing, implementing, and maintaining LeadGenius’s big data pipeline infrastructure — including data ingestions, stream processing, data warehousing, data pipelines, visualization, quality metrics and exposing data to applications
Data modeling and metadata management
Ensure scalable, highly available, and robust big data platform architecture to meet service level agreements
End-to-end data processing, troubleshooting, problem diagnosis, performance benchmark, load balance, and code reviews
Your KPIs would be
Building large-scale infrastructural data systems using open-source technologies
Designing high-performance RESTful web services to expose data from SQL and NoSql
Cross team collaboration and effective communication
Skills & Experience
7+ years of experience with object oriented programming language such as Java, and scripting languages such as Python
5+ years of experience with big data and distributed NoSql technologies such as Spark, Hbase or Map-reduce
3+ years of experience designing and implementing large, scalable distributed systems
3+ years of production experience designing and implementing mission critical and metrics-driven ETL’s
Good to have
Experience with AKKA-based webservice is a plus
Strong in algorithms and CS fundamentals.",3.0,"Squadex
3.0","Palo Alto, CA",-1,1 to 50 Employees,2016,Company - Private,IT Services,Information Technology,Unknown / Non-Applicable,-1
AWS Data Engineer,$107k-$190k (Glassdoor Est.),"This is what you’ll do: · Design, implement and support an analytical data infrastructure providing access to large datasets and computing power. · Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using API, SQLS, Change Data Capture Tools and AWS big data technologies. · Continuous research of the latest big data and visualization technologies to provide new capabilities and increase efficiency · Creation and support of real-time data pipelines built on AWS technologies including EMR, Glue, Kinesis, Redshift/Spectrum and Athena · Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business. · Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering and machine learning this is what you’ll need: · Self-starter with excellent communication skills · 7-10 years of industry experience in software development, data architecture, data engineering, business intelligence, data science with a track record of manipulating, processing, and extracting value from large datasets · Strong understanding of all Big Data and data warehousing services offered by AWS. · Hands-on experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets using AWS S3, Glue, Kinesis, Kafka, SQS, Change data capture tools, Spark · Experience in a query framework for business users and data scientists using Athena, APIs and spinning data science clusters. · Strong data base experience in both Relational, Columnar, NOSQL & Timeseries database like Redshift, DynamoDB, MongoDB, SQL Server, Druid etc. · Strong hands-on experience in one or more programming languages like Java, Python, Scala · Demonstrated strength in data modeling, ETL development, and data warehousing · Good knowledge of statistical models and data mining algorithms · Experience using analytics & reporting tools like Tableau, Power BI, Qlikview etc. · Understanding of business domains like Finance, Supply Chain, Manufacturing is a plus · Degree/Diploma in computer science, engineering, mathematics, or a related technical discipline preferred This is where you’ll work: Department: IT Infrastructure and Enterprise Applications Location: Palo Alto, CA Rivian description: Rivian is on a mission to keep the world adventurous forever. This goes for the emissions-free Electric Adventure Vehicles we build, and the curious, courageous souls we seek to attract. As a company, we constantly challenge what’s possible, never simply accepting what has always been done. We reframe old problems, seek new solutions and operate comfortably in areas that are unknown. Our backgrounds are diverse, but our team shares a love of the outdoors and a desire to protect it for future generations. We operate development centers in Plymouth, MI, Irvine CA and San Jose, CA, and Surrey, England, as well as a manufacturing facility in Normal, Illinois. Rivian is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: Rivian is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at Rivian are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. Rivian will not tolerate discrimination or harassment based on any of these characteristics. Rivian encourages applicants of all ages.",4.0,"Rivian Automotive
4.0","Palo Alto, CA",-1,1001 to 5000 Employees,2009,Company - Private,Transportation Equipment Manufacturing,Manufacturing,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"5+ years of experience in Data Engineering and Business Intelligence.
Proficient in IoT tools such as MQTT, Kafka, Spark
Proficient with AWS, S3, Redshift
Experience with Presto and Parquet/ORC
Proficient with Apache Spark and data frame.
Experienced in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Python,
xperience with Columnar database such as Redshift, Vertica

Job Types: Full-time, Contract

Pay: $115,603.00 - $191,059.00 per year

Schedule:
8 hour shift
Monday to Friday
Experience:
Data science: 5 years (Preferred)",-1,SNAPIDEA SYSTEMS,"San Francisco, CA",-1,-1,-1,-1,-1,-1,-1,-1
Principal Full Stack Big Data Engineer,$180k-$325k (Glassdoor Est.),"The Role: The ForgeRock Autonomous Engineering team is building the next generation of identity and access management powered by AI/ML approaches. The team is rapidly evolving the Autonomous Identity solution to address key use cases in cybersecurity, consumer privacy and regulatory compliance. Design and develop software capabilities related to storage, processing and query of large data sets. What you'll be doing: Design data and prediction pipelines for Autonomous Identity, Autonomous Access and Identity Governance products. Lead data persistence and query design for numerous identity use cases including traceability, history, context, authorization etc. Develop persistent and query scaling that accounts for single-data-center and multi-data-center clusters. Collaborate with security engineers, data scientists and other product groups to evaluate data pipeline requirements and boundaries. Design clean abstractions and API specifications to implement customer use cases Write efficient unit and integration tests to automate quality and the release pipeline Build modular code emphasizing readability, security, and clean dependency management Create data pipelines that efficient transform raw data into meaningful insights and predictions Leverage continuous delivery tools to securely deploy microservices to various environments and ensure SLAs for uptime, latency and throughput across multiple data centers. If you find yourself checking off most of the below, this could be the position for you! Expertise in designing and supporting large production clusters of Cassandra/MongoDB Firm grasp of approaches to data partitioning and query design. Solid understanding of tradeoffs between relational and noSQL models. Proficient in search and querying using ElasticSearch Experience with data security both at-rest and in-transit. Experience with index lifecycle management. Strong understanding of data pipeline and scalable cloud-native ETL technologies (Spark/Dataflow/Beam/Nifi) Deep knowledge of production concerns for both single-data-center and multi-data-center storage and index clusters. Skilled in utilizing Java and/or Javascript/Node for backend development. Experience with Test Driven Development covering Unit, Integration and API tests. Willingness, when needed, to do UI, Dev-Ops, CI/CD, Documentation any development tasks need to get still done. Multi-tenant SaaS experience is a big plus. Knowledge of cyber security, AI/ML modeling, Identity and Access management solutions a plus.",4.2,"ForgeRock
4.2","San Francisco, CA",-1,501 to 1000 Employees,2010,Company - Private,Enterprise Software & Network Solutions,Information Technology,$50 to $100 million (USD),-1
Data Engineer - Business Growth,$116k-$203k (Glassdoor Est.),"Data Engineer - Business Growth: JD.com

JD.com is China's largest online retailer and its biggest overall retailer, as well as the country's biggest Internet company by revenue. JD.com sets the standard for online shopping through its commitment to quality, authenticity, and its vast product offering covering everything from fresh food and apparel to electronics and cosmetics. Its unrivaled nationwide fulfillment network provides standard same- and next-day delivery covering a population of more than 1 billion - a level of service and speed that is unmatched globally. In 2017, JD had 292.5M annual active consumers and net revenue of US$55.7B. JD.com is currently on pace to become China's largest B2C e-commerce platform by 2021. JD.com is listed on the NASDAQ under the ticker ""JD"". In 2014, the year of its listing, this was the largest IPO on the NASDAQ. JD.com was the first Chinese Internet company to be included on the Fortune Global 500 List.

JD plans to strengthen its leadership position as a technology-driven company over the next 10+ years, with a focus on big data, AI technology and smart logistics, and to become the dominant e-commerce platform in China. To achieve these goals, we aim to more deeply understand consumer behavior in China, better understand what drives purchases, how to forecast demand, and how to target promotions, advertising and other efforts across the whole range of technologies available to sophisticated Chinese consumers today. As global brands enter China and as China in turn becomes more globalized, we will provide innovative ways for these brands to deliver new products, help drive new monetization strategies and provide solutions for brand safety and product authenticity. In addition, we are constantly evolving our already state-of-the-art ad-serving platform to serve digital ads across all of China. For this platform, we are developing sophisticated analytics around attribution, return and exposure. To drive this exciting agenda forward, we are looking for people with strong social science, quantitative, statistical and machine learning skills who can partner with our team in Mountain View and in Beijing to leverage the vast data and computational assets of JD (which now involves collaboration and data sharing with Baidu, NetEase, Quihoo, Sogou, Toutiao and WeChat. This powerful consortium combines the largest search, gaming, mobile-security, news-aggregation and social network companies in China).

The ideal candidate for this position will be quantitatively trained (advanced master's or PhD) with expertise in data science. Individuals who are interested in using computing and data to more deeply understanding consumer behavior, marketplace behavior, competition and who have a passion for solving complex business problems through the combined use of data, technology and strategy will thrive in our environment. They will also be comfortable working cross-functionally and thrive in a fast-paced organization. Interest in e-commerce and in economics/quantitative marketing/business-analytics is a plus.

Responsibilities

Design, build and launch new data extraction, transformation and loading pipelines to develop data-based tools for analyzing consumer behaviors.
Produce high quality code with reliability and scalability; efficiently collaborate across different teams to ensure smooth transition of POC stage code stacks to productization.
Analyze large-scale structured and unstructured data; develop new machine learning, statistical models and visualization tools to help answer JD advertising and marketing related business questions.

Minimum Qualifications

Master's or PhD degree in Computer Science, Statistics, Mathematics or related fields.
Proficiency in SQL and a Unix/Linux environment for automating processes with shell scripting.
Deep understanding of big data technologies (some subset of MapReduce, Hadoop, Pig, Spark, Hive, Kafka, etc.).
Proficiency in at least one of the following programming languages: Java, C++, Scala, Python, R.
Solid foundation in data structures and algorithms, with excellent debugging and troubleshooting skills.
Excellent communication skills and a strong team player.
Interest in consumer behavior, e-commerce and related business questions.
Training and experience in ML and building statistical models a plus.

About JD-Business Growth

The Business Growth Business Unit manages JD's ad-business. JD's ad-tech comprises a set of large scale publishing, programmatic ad-exchange, ad-network and data management platforms for serving digital ads on JD-owned and affiliated properties across China. JD Business Group also works on JD's marketing technology platform efforts aimed at helping brands leverage JD and its partners' large-scale data and computing assets to improve their marketing in China.

JD Business Growth research group in Silicon Valley is focused on using science to suggest, support and shape new and existing data-driven advertising, e-commerce, and marketing products. We combine large-scale experimentation, statistical-econometric, machine learning and artificial intelligence tools, with social-science, economics and computer science to find innovative ways to drive growth and monetization. Some of the problems we work on include advertising attribution, recommendation systems, advertising targeting, using applied economics, causal inference, deep learning, and reinforcement learning methods.

JD.com is an Equal Opportunity Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.",3.6,"JD.com
3.6","Mountain View, CA",-1,5001 to 10000 Employees,-1,Company - Public,"Department, Clothing, & Shoe Stores",Retail,Less than $1 million (USD),-1
eCommerce Data Engineer,$74k-$131k (Glassdoor Est.),"Auto req ID: 216200BR Job Description PepsiCo operates in an environment undergoing immense and rapid change, driven by eCommerce and emergent retail technologies. To ensure continued success in the food and beverage space, PepsiCo has assembled a dedicated eCommerce team – tasked with optimizing eCommerce operations and developing innovations that will give PepsiCo a sustainable competitive advantage. While tied closely to broader PepsiCo, the eCommerce group more closely resembles a start-up environment; embracing the core values of having a bias for action, being results-oriented, maintaining a community-focus, and prioritizing people. PepsiCo’s Data Science and Analytics group is a team of data scientists, technology specialists, and business innovators who operate within eCommerce to build industry-leading systems and solutions. By focusing on machine learning and automation, the Data Science & Analytics group is pushing the bounds of possibility for PepsiCo and its strategic partners. What PepsiCo Data Science & Analytics does: Build machine learning systems to understand the cross-channel grocery ecosystem Perform statistical analysis across diverse datasets to drive and measure performance Work with PepsiCo’s strategic partners to expand their technical capabilities, thereby creating a more robust data environment Utilize natural language understanding techniques to uncover insights from contextual data Develop scalable tools to drive automation and optimize business operations As an eCommerce Data Engineer, you will be a primary executor of technical solutions which leverage both Enterprise and Big Data as it relates to B2B and B2C eCommerce. You will focus on the collection, storage, processing, and analysis of disparate data sets. You will engage frequently with business partners and cross-functional technical teams to ensure long term supportability, as well as be responsible for the hand-off into the company’s larger Enterprise Architecture. Responsibilities: Create and Manage ETL processes and data pipelines Leverage SQL for data querying across enterprise platforms Build out the necessary data structures optimized for eCommerce analytics, leveraging the on-premise data warehouse and other structured and unstructured data warehousing/storage platforms Build and maintain data cubes within SQL Server/Azure Analysis Services Qualifications/Requirements Bachelor’s Degree in Computer Science, Data Science, Information Technology, Information Systems, Statistics or other STEM related area General knowledge of data warehousing models (Kimball, Corporate Information Factory, etc.) Experience designing complex ETL solutions Strong experience designing and implementing data processing pipelines, data cleansing, standardization, data profiling, and enrichment processes to populate the data warehouse General knowledge of cloud data pipelines (AWS, Azure, Google cloud) Fluent in TSQL and Python Strong SSIS development skills using SQL Server Data Tools and Visual Studio Experience with SQL Server/Azure Analysis Services Experience with Apache Airflow a plus Experience with MDX queries a plus Relocation Eligible: Not Applicable Job Type: Pipeline",3.8,"PepsiCo
3.8","Fairfield, CA",-1,10000+ Employees,1965,Company - Public,Food & Beverage Manufacturing,Manufacturing,$10+ billion (USD),-1
"Data Engineer, Analytics (Payments Ecosystem)",$118k-$192k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Our more experienced data engineers are clearly characterized by in-depth technical experience and proven progression in leadership responsibility. If you have an interest in being responsible for the dynamics of a fast-paced environment, this is the right role for you. You will be working on many projects at a time, but also focused on the details while finding creative ways to pursue big picture challenges.As part of the payments ecosystem team, you would be responsible for building the data foundations and metrics that are conformed across the Facebook Family of Apps and enable insights to the acquisition, revenue, compliance and risk , engagement/retention across payment products, payouts and provider performance ensure reliability and SLAs. The main consumers of these data artifacts would be the product teams and the executive team.Payments is a top priority for Facebook in 2020 and beyond recently has been made into an independent product organization called Facebook Financials with a focus to platformize the payments services. In this role, you will have the opportunity to define technical specifications for metric definition, logging, define and influence the right metrics and build the core datasets that will be used by our Data Scientists, Machine Learning engineers and product managers.A few examples of the story your dataset will tell:Compliance policies and effectiveness of implementation (KYC, KYB). Understanding the relation between acquisition, reliability, engagement and retention. Provide understanding how payment enables lift across different products. How payments perform on different platforms, products and operating systems.
Ensure conformance of metrics and detailed understanding of the metric definitions from business and technical implementation
Craft and own the optimal data processing architecture and systems for new data and ETL pipelines
Build core datasets as well as scalable and fault-tolerant pipelines
Build data anomaly detection, data quality checks, and enable easy root cause analysis
Define and own the data engineering roadmap for payments ecosystem and other areas to ensure seamless integration
Collaborate with Software Engineers and Data Scientists to design technical specification for logging and add logging to production code to generate metrics both online as well as offline
Work with different cross-functional partners - WhatsApp Payments Team, Compliance, Tax and Finance
Build visualizations to provide insights into the data & metrics generated on the Payments Platform
Work with data infrastructure teams to suggest improvements and influence their roadmap
Able to immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions
Recommend improvements and modifications to existing data and ETL pipelines
Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership
Drive internal process improvements and automating manual processes for data quality and SLA management
Provide ongoing proactive communication and collaboration throughout the organization
Actively mentor team members in their careers
4+ years experience in the data warehouse space
4+ years' experience in payments analytics
4+ years experience working with either a MapReduce or an MPP system
7+ years experience in writing complex SQL and ETL processes
4+ years experience with object-oriented programming languages
7+ years experience with schema design and dimensional data modeling
BS/BA in Technical Field, Computer Science or Mathematics
Knowledge in Python or Java
Experience analyzing data to identify deliverables, gaps, and inconsistencies
Experience effectively collaborating and communicating complex technical concepts to a broad variety of audiences
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Senior Data Engineer,$105k-$184k (Glassdoor Est.),"At Shutterfly, we’re all about people — bringing them together, making them feel welcome, and connecting them to experiences. We make our customers’ memories last a lifetime by capturing, preserving, and sharing them through photography and personalized products. Through our family of brands, trend setting products, cutting edge technology, and best in class customer service, we help our customers, and each other, share life’s joy.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.

What You'll Do Here

Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines and improve the operation and performance of the Data Warehouse

The Skills You’ll Bring

Bachelor’s degree in Computer Science or equivalent
Expert knowledge Python, Spark and SQL scripting, working knowledge of R or similar statistical computing packages
7+ years of hands on experience in building data & feature engineering applications, including design, implementation, debugging, and support
Deep understanding of data integration to support analytics & feature engineering for Machine learning algorithms
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure

It is helpful, but not required to have..

Strong at applying data structures, algorithms, and object oriented design, to solve challenging data integration problems
Expertise with the AWS platform & Databricks preferred
If this aligns to your career goals, skills and experience, we want to work with you!

#LI-KM1",3.1,"Shutterfly
3.1","Redwood City, CA",-1,1001 to 5000 Employees,1999,Company - Private,Internet,Information Technology,$1 to $2 billion (USD),-1
"Data Engineer, Analytics (Marketing Analytics)",$118k-$192k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

In this role, you will work closely with the Marketing Decision science team to measure and help optimize the marketing campaigns that we run in service of one of the 4 top priorities of the company.This team owns making sure that Facebook is heard. Specifically, you will measure the execution, effectiveness and efficiency of the dollars that we spend on achieving that objective. We strive for nothing less than to be the worlds best marketing team.You will be responsible for building a strong data foundation and architecture that will allow us to understand campaign performance, media mix modeling, and brand attribution. In this role, you will have the opportunity to define create the data model and data collection approach, define and influence the right metrics and build the core datasets that will be used by our Data Scientists, Solutions Engineers and Communications Planners.A few examples of the questions you will help answer would be on the campaign pacing and delivery on target, or the most efficient channels to target for a given budget and brand sentiment. Additionally, you will be helping with holistic campaign measurements to bring disparate sources such as TV, digital and Out of Home (OOH) together as well as new bleeding edge approaches to attribution for non-digital channels using resources that only a company the scale of Facebook can bring.
Work with agencies and advertising partners to collect and enhance data around campaign performance.
Work with Marketing Decision scientists on data representation and statistical approaches to measurement.
Define and own the data engineering roadmap for measurement of campaigns effectiveness and efficiency.
Able to immerse yourself in all aspects of marketing, understand the problems, and tie them back to data engineering solutions.
Recommend improvements and modifications to existing data and ETL pipelines.
Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership.
Provide ongoing proactive communication and collaboration throughout the organization.
Able to actively mentor team members in their careers.
4+ years experience in the data warehouse space.
4+ years experience working with either a MapReduce or an MPP system.
7+ years experience in writing complex SQL and ETL processes.
4+ years experience with object-oriented programming languages.
7+ years experience with schema design and dimensional data modeling.
BS/BA in Technical Field, Computer Science or Mathematics.
Knowledge of Python or Java.
Experience analyzing data to identify deliverables, gaps, and inconsistencies.
Effectively collaborate and communicate complex technical concepts to a broad variety of audiences.
Knowledge of Omnichannel marketing, measurement and optimization.
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Avail - Staff Data Engineer,$102k-$178k (Glassdoor Est.),"The world isn’t standing still, and neither is Allstate. We’re moving quickly, looking across our businesses and brands and taking bold steps to better serve customers’ evolving needs. That’s why now is an exciting time to join our team. As a leader in a corporation with 83,000 employees and agency force members, you’ll have a hand in transforming not only Allstate but a dynamic industry. You’ll have opportunities to take risks, challenge the status quo and shape the future for the greater good. You’ll do all this in an environment of excellence and the highest ethical standards – a place where values such as integrity, inclusive diversity and accountability are paramount. We empower every employee to lead, drive change and give back where they work and live. Our people are our greatest strength, and we work as one team in service of our customers and communities. Everything we do at Allstate is driven by a shared purpose: to protect people from life’s uncertainties so they can realize their hopes and dreams. For 89 years we’ve thrived by staying a step ahead of whatever’s coming next – to give customers peace of mind no matter what changes they face. We acted with conviction to advocate for seat belts, air bags and graduated driving laws. We help give survivors of domestic violence a voice through financial empowerment. We’ve been an industry leader in pricing sophistication, telematics, digital photo claims and, more recently, device and identity protection. We are the Good Hands. We don’t follow the trends. We set them. Job Description Avail (affiliate company of Allstate) is a new car sharing platform focused on improving mobility and reducing the cost of car ownership. We give car owners a way to earn extra income from their idle cars and connect drivers with a convenient, affordable way to drive a car when they want. As part of the Avail Data Engineering team, you will be working to manage our ever-growing collection of vehicle sharing and usage data. We support our business analytics and marketing teams by performing data integration and ETL between an increasingly diverse set of data sources and our data warehouse. We will also build big-data applications leveraging techniques from operations research and machine learning to directly enrich, personalize and optimize the Avail car-sharing product. This is a green field opportunity to contribute to the design and implementation of a flexible, scalable data framework for an exciting new sector of the sharing economy. This position can be based in either of two Avail HQ offices at San Francisco or Chicago, or fully remote with semi-regular travel to SF HQ. Key Responsibilities Design and implement scalable data workflows and pipelines, and integrate diverse data sources and sinks Design appropriate database schemas and optimize database deployment architectures for analytics query loads Implement data transforms and organization for various data stores (data lakes and warehouses) Design and implement new platform architectures for building and serving machine learning models Work with the platform operations team to monitor and maintain live production systems Provide tooling and automation for infrastructure, continuous testing, and continuous deploy of data systems Job Qualifications Various experience levels considered. Junior candidates must have a strong background of coursework or academic projects around data engineering or machine learning at scale or have appropriate industry experience contributing to such projects. Senior candidates must demonstrate a track record of successful technical leadership in the execution of large-scale data projects. Software Engineering – Level-appropriate experience in software engineering and SDLC (our stack may include Python, Golang and Scala). Must consider code readability, reuse, and extensibility a priority when developing solutions. Big Data Engineering – Experience in building scalable data pipelines involving machine learning, optimization or prediction Big Data Devops – Experience in performing operations and automation of various big-data ecosystems in production environments on AWS or a related cloud service Ability to thrive in a fast paced, cross regional, diverse, and dynamic work environment Nice to have: Experience with AWS data stack – Redshift, Athena, EMR, Kinesis, DocumentDB, DynamoDB Experience with establishing well-organized data lakes Experience setting up and optimizing data warehouses Background in data modeling and performance tuning in relational and no-SQL databases Experience with data practices (security, data management and governance) Experience in operations research, machine learning or optimization The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen. Good Work. Good Life. Good Hands®. As a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy. Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video. Allstate generally does not sponsor individuals for employment-based visas for this position. Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component. For jobs in San Francisco, please click ""here"" for information regarding the San Francisco Fair Chance Ordinance. For jobs in Los Angeles, please click ""here"" for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance. It is the policy of Allstate to employ the best qualified individuals available for all jobs without regard to race, color, religion, sex, age, national origin, sexual orientation, gender identity/gender expression, disability, and citizenship status as a veteran with a disability or veteran of the Vietnam Era.",3.5,"Allstate
3.5","San Francisco, CA",-1,10000+ Employees,1931,Company - Public,Insurance Agencies & Brokerages,Insurance,$10+ billion (USD),-1
Senior Data Engineer,$105k-$184k (Glassdoor Est.),"At Shutterfly, we’re all about people — bringing them together, making them feel welcome, and connecting them to experiences. We make our customers’ memories last a lifetime by capturing, preserving, and sharing them through photography and personalized products. Through our family of brands, trend setting products, cutting edge technology, and best in class customer service, we help our customers, and each other, share life’s joy.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.

What You'll Do Here

Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines and improve the operation and performance of the Data Warehouse

The Skills You’ll Bring

Bachelor’s degree in Computer Science or equivalent
Expert knowledge Python, Spark and SQL scripting, working knowledge of R or similar statistical computing packages
7+ years of hands on experience in building data & feature engineering applications, including design, implementation, debugging, and support
Deep understanding of data integration to support analytics & feature engineering for Machine learning algorithms
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure

It is helpful, but not required to have..

Strong at applying data structures, algorithms, and object oriented design, to solve challenging data integration problems
Expertise with the AWS platform & Databricks preferred
If this aligns to your career goals, skills and experience, we want to work with you!

#LI-KM1",3.1,"Shutterfly
3.1","Redwood City, CA",-1,1001 to 5000 Employees,1999,Company - Private,Internet,Information Technology,$1 to $2 billion (USD),-1
Senior Data Engineer - Digital Media,$125k-$214k (Glassdoor Est.),"Goodway Group has been fully remote for 12+ years, has a fantastic virtual culture, and this position can be located anywhere in the US working from a home office. Goodway is the leading independent programmatic media services company on the planet. As a 3rd generation, family-owned business with no investors or debt, Goodway is fighting international conglomerates 30x its size and winning! We're looking for a software engineer who wants to build and support a top-tier programmatic media buying and analytics platform, loves to build cool stuff and work hard, can communicate well, and has a personality that fits in with ours. Our development philosophy is to work as small, focused, nimble groups, building everything from front-end to the back-end. We divide up work on a product basis and you'd be responsible for making sure your features have a clear user-interface, perform and scale, and have legible and easy to maintain code. You may not be familiar with all of the specific technologies we use, but you should be excited about the opportunities to pick them up. The right person will feel at home at Goodway because: We prefer open-source, being scrappy/nimble, and family values over proprietary technologies, all-day meetings, and corporate bureaucracy. You can ditch your commute since we’re 100% virtual – everyone works at home but we get together a few times each year. We make being physically together a celebration filled with good times and opportunities for new experiences. We encourage personal growth and professional development through technical seminars/cyber-training, mentorships, and leadership opportunities. We have a long history of promoting from within and a career ladder that provides opportunities in both technical and managerial tracks. We focus on making your career fit into your life, not your life fitting into your career. We have a solid 5-year vision which we’d enjoy sharing with you during the interview process. Things we’re looking for in our engineers: Experience writing software in a professional setting. Strong listening and communication skills (written and verbal), including the ability to communicate effectively with non-technical customers. Willingness and demonstrated ability to learn new technologies and skills. Ability to collaborate well with both internal groups and external partners and thrive in an agile environment with limited oversight. Comfortable and committed to multi-tasking work in a small but rapidly growing environment. We prefer people who are passionate about their craft, and ideally have an interest in internet advertising, algorithmic ad trading, workflow automation, a/b testing, data warehousing, and solving business problems in collaboration with customers. We’re looking for engineers who are strong hands-on coders but experience in every technology we use is not critical, as long as you’re committed to learning what you’re not familiar with already. Solid understanding of data warehousing, relational databases, and data modeling/design techniques and tuning (non-relational database experience a plus). Advanced SQL skills including query optimization. Experience working in ""big data"" platforms and related tools & scripting languages including Amazon Redshift & EMR, Presto, Hive, Python, Spark, Scala, BASH & K-shell scripting, etc. Experience building data pipelines with open source tools such as Airflow and Argo. Modern web tech including: GraphQL, Node JS, REST APIs HTML-5, CSS, XML, JSON, Javascript and JS Libraries, etc. Strong design and coding skills in modern object-oriented or functional languages (Javascript, Java, C#, Python, Scala, etc). While these are the technologies we currently use, we want to find people who are committed, like we are, to using the right tools for the job and want to be part of the evolution of our technical architecture and systems. What you’ll be doing at Goodway: Designing and building software with an emphasis on usability, quality, performance, and maintainability. Work with product owners in an agile environment to analyze requirements and translate them into functioning software. Collaborating with other team members and members of the business unit. Provide exceptional production support to internal departments, including keeping customer-facing teams in-the-loop regarding outstanding issues. Learning new technologies, practices, and techniques and becoming a better engineer, leader, and team member. Raise our collective IQ and acting as a strong contributor to our growing team. Having fun!",4.1,"Goodway Group
4.1","San Francisco, CA",-1,201 to 500 Employees,1929,Company - Private,Advertising & Marketing,Business Services,Unknown / Non-Applicable,-1
Avail - Staff Data Engineer,$102k-$178k (Glassdoor Est.),"The world isn’t standing still, and neither is Allstate. We’re moving quickly, looking across our businesses and brands and taking bold steps to better serve customers’ evolving needs. That’s why now is an exciting time to join our team. As a leader in a corporation with 83,000 employees and agency force members, you’ll have a hand in transforming not only Allstate but a dynamic industry. You’ll have opportunities to take risks, challenge the status quo and shape the future for the greater good. You’ll do all this in an environment of excellence and the highest ethical standards – a place where values such as integrity, inclusive diversity and accountability are paramount. We empower every employee to lead, drive change and give back where they work and live. Our people are our greatest strength, and we work as one team in service of our customers and communities. Everything we do at Allstate is driven by a shared purpose: to protect people from life’s uncertainties so they can realize their hopes and dreams. For 89 years we’ve thrived by staying a step ahead of whatever’s coming next – to give customers peace of mind no matter what changes they face. We acted with conviction to advocate for seat belts, air bags and graduated driving laws. We help give survivors of domestic violence a voice through financial empowerment. We’ve been an industry leader in pricing sophistication, telematics, digital photo claims and, more recently, device and identity protection. We are the Good Hands. We don’t follow the trends. We set them. Job Description Avail (affiliate company of Allstate) is a new car sharing platform focused on improving mobility and reducing the cost of car ownership. We give car owners a way to earn extra income from their idle cars and connect drivers with a convenient, affordable way to drive a car when they want. As part of the Avail Data Engineering team, you will be working to manage our ever-growing collection of vehicle sharing and usage data. We support our business analytics and marketing teams by performing data integration and ETL between an increasingly diverse set of data sources and our data warehouse. We will also build big-data applications leveraging techniques from operations research and machine learning to directly enrich, personalize and optimize the Avail car-sharing product. This is a green field opportunity to contribute to the design and implementation of a flexible, scalable data framework for an exciting new sector of the sharing economy. This position can be based in either of two Avail HQ offices at San Francisco or Chicago, or fully remote with semi-regular travel to SF HQ. Key Responsibilities Design and implement scalable data workflows and pipelines, and integrate diverse data sources and sinks Design appropriate database schemas and optimize database deployment architectures for analytics query loads Implement data transforms and organization for various data stores (data lakes and warehouses) Design and implement new platform architectures for building and serving machine learning models Work with the platform operations team to monitor and maintain live production systems Provide tooling and automation for infrastructure, continuous testing, and continuous deploy of data systems Job Qualifications Various experience levels considered. Junior candidates must have a strong background of coursework or academic projects around data engineering or machine learning at scale or have appropriate industry experience contributing to such projects. Senior candidates must demonstrate a track record of successful technical leadership in the execution of large-scale data projects. Software Engineering – Level-appropriate experience in software engineering and SDLC (our stack may include Python, Golang and Scala). Must consider code readability, reuse, and extensibility a priority when developing solutions. Big Data Engineering – Experience in building scalable data pipelines involving machine learning, optimization or prediction Big Data Devops – Experience in performing operations and automation of various big-data ecosystems in production environments on AWS or a related cloud service Ability to thrive in a fast paced, cross regional, diverse, and dynamic work environment Nice to have: Experience with AWS data stack – Redshift, Athena, EMR, Kinesis, DocumentDB, DynamoDB Experience with establishing well-organized data lakes Experience setting up and optimizing data warehouses Background in data modeling and performance tuning in relational and no-SQL databases Experience with data practices (security, data management and governance) Experience in operations research, machine learning or optimization The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen. Good Work. Good Life. Good Hands®. As a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy. Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video. Allstate generally does not sponsor individuals for employment-based visas for this position. Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component. For jobs in San Francisco, please click ""here"" for information regarding the San Francisco Fair Chance Ordinance. For jobs in Los Angeles, please click ""here"" for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance. It is the policy of Allstate to employ the best qualified individuals available for all jobs without regard to race, color, religion, sex, age, national origin, sexual orientation, gender identity/gender expression, disability, and citizenship status as a veteran with a disability or veteran of the Vietnam Era.",3.5,"Allstate
3.5","San Francisco, CA",-1,10000+ Employees,1931,Company - Public,Insurance Agencies & Brokerages,Insurance,$10+ billion (USD),-1
Data Engineer - Video Ranking & Distribution,$118k-$192k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Our analytics team works very closely with Product Leaders of these apps to determine ways to acquire new users, retain existing users, and optimize user experience - all of this using massive amounts of data. As part of the analytics team, you will see a direct link between your work, company growth, and user happiness.

Our Data Engineers are clearly characterized by in-depth technical expertise and proven progression in leadership responsibility. If you have an interest in being responsible for the dynamics of a fast-paced environment, this is the right role for you. You will be working on many projects at a time, but also focused on the details while finding creative ways to pursue big picture challenges.

In this role, you will work closely with the Video Ranking and Distribution teams that are responsible for helping users discover interesting video content through different surfaces (New Feed, Watch, Pages etc) and create a planned viewing habit for Facebook users.

The Video Ranking & Distribution team is responsible for effective distribution of videos across our family of apps through recommendations and ranking. This is the largest video ranking group, responsible for serving videos to over one billion users daily. The Video Ranking team builds the best video recommendations technology and algorithms, which provides a delightful discovery and consumption experience to users.

You will be responsible for building a strong data foundation and architecture that will allow us to understand/measure: video distribution, ranking data for algorithms (including candidate generation), feature selection, relatedness, popularity bias and its impact on consumption and engagement, compute metrics for video ranking events, and reliability on different devices. In this role, you will have the opportunity to define technical specifications for logging, define and influence the right metrics, and build the core datasets that will be used by our Data Scientists, Machine Learning engineers and product managers.

The core datasets will allow us to:
Measure how effective video ranking and distribution is on our platforms.
Understand Session Metrics like time spent watching a video, relatedness of videos.
Understand relation between recommendation and consumption.
Provide understanding into the video interactions, devices, markets, media types.
Monitor generator algorithm health, relevance of recommendation and reliability.
Reducing popularity bias and optimizing evergreen content vs recent content.
Craft and own the optimal data processing architecture and systems for new data and ETL pipelines
Build core datasets as well as scalable and fault-tolerant pipelines
Build data anomaly detection, data quality checks, and optimize pipelines for ideal compute and storage
Work with video machine learning teams and prepare datasets to provide a 360 degree understanding of the video and help in feature engineering
Define and own the data engineering roadmap for Video Ranking and Distribution Data Engineering
Collaborate with Software Engineers and Data Scientists to design technical specification for logging and add logging to production code to generate metrics both online as well as offline
Work with different cross-functional partners - Creator and Publisher experiences team, Video Understanding, Video Integrity, Computer Vision and Ranking Science
Build visualizations to provide insights into the data & metrics
Work with data infrastructure teams to suggest improvements and influence their roadmap
Able to immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions
Recommend improvements and modifications to existing data and ETL pipelines
Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership
Drive internal process improvements and automating manual processes for data quality and SLA management
Provide ongoing proactive communication and collaboration throughout the organization
4+ years experience in the data warehouse space
4+ years experience working with either a MapReduce or an MPP system
7+ years experience in writing complex SQL and ETL processes
4+ years experience with object-oriented programming languages
7+ years experience with schema design and dimensional data modeling
BS/BA in Technical Field, Computer Science or Mathematics
Knowledge in Python or Java
Experience analyzing data to identify deliverables, gaps, and inconsistencies
Actively mentored team members in their careers
Ability to effectively collaborate and communicate complex technical concepts to a broad variety of audiences
Knowledge of machine learning recommendation and ranking
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Senior Data Engineer,$107k-$192k (Glassdoor Est.),"Overview


Intuit is a leading software provider of business and financial management solutions for small and mid-sized businesses, consumers and accounting professionals. You probably know us by our flagship products, QuickBooks®, TurboTax® and Mint®, but that is just the start. We are currently going through a fundamental transformation to a global financial solutions and services company.

Come join the Intuit Data & Cloud (DAC) Team as a ""Senior Data Engineer"". Intuit DACs team mission is to enable Intuit enterprise data for business insights, decisions and delightful experiences for our customers, employees and business leaders. We are looking for an experienced data engineer to take our Sales and Customer Success analytics platform to the next level.
What you'll bring
BS/MS in Computer Science or equivalent work experience
Strong CS fundamentals including data structures, algorithms and distributed systems.
5+ years of experience in a software development role with a focus on data systems
Hands-on experience with AWS technologies like S3, Redshift, EMR/EC2, Lambda functions
Hands-on experience building scalable and reliable data pipelines based on Big Data processing technologies like Hadoop, MapReduce, Spark, Python.
Hands-on experience in ETL tools and working with large data sets in the cloud
Operational mindset with ability to do Problem, SLA and Incident Management.
Proven capability of building data marts and data solutions
History of contributing to open source projects is a plus.
Experience with Informatica is a plus
Imagine a career where your creative inspiration can fuel BIG innovation. Year-over-year, Intuit has been recognized as a best employer and is consistently ranked on Fortune's 100 Best Companies To Work For and Fortune Worlds Most Admired Software Companies lists. Immerse yourself in our award winning culture while creating breakthrough solutions that simplify the lives of consumers and small businesses and their customers worldwide.
Intuit is expanding its social, mobile, and global footprint with a full suite of products and services that are revolutionizing the industry. Utilizing design for delight and lean startup methodologies, our entrepreneurial employees have brought more than 250 innovations to market -- from QuickBooks®, and TurboTax®, to GoPayment, Mint.com, big data, cloud (SaaS, PaaS) and mobile apps. The breadth and depth of these customer-driven innovations mean limitless opportunities for you to turn your ingenious ideas into reality at Intuit.

Discover what it is like to be part of a team that rewards taking risks and trying new things. Its time to love what you do! Check out all of our career opportunities at: http://jobs.intuit.com. Intuit is an Equal Opportunity Employer.
How you will lead
In this role, you will be driving projects in the Sales and Customer success analytics areas. You will be building data solutions in the areas of speech/text analytics, contact center technologies, workforce scheduling, sales forecasting and reporting etc.
While you will be building these solutions, customer empathy and delight will be the main drivers for your success
Design and develop ETL (extract-transform-load) processes to validate and transform data, calculate metrics and populate data models etc., using Spark, Python, SQL, and other technologies in the AWS cloud.
As an engineer, you will be designing and building scalable, best-in-class data products in close coordination with our engineering teams in other geographies
You will be helping bring best practices in data engineering to accelerate our move to a devops model
You will be working cross-functionally with CRM and Contact center engineering teams, Sales BI teams and Customer Success BI teams, Analysts, etc
You will also focus on operational excellence through root cause analysis and continuous improvement. Ability to pro-actively follow issues through to resolution will be important.
You will be expected to contribute to innovations that fuel the growth of Intuit as a whole",4.3,"Intuit - Data
4.3","Mountain View, CA",-1,5001 to 10000 Employees,1983,Company - Public,Computer Hardware & Software,Information Technology,$5 to $10 billion (USD),-1
"Data Engineer, Legal Central Services",$115k-$187k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

How would Facebook scale to the next billion users? The Legal Central Services data engineering team is responsible for building solutions to solve complex problems related to compliance, security & investigations, intellectual property, and more. We are looking for a Data Engineer to not only build data pipelines but also extend our data tools. As a Data Engineer, you will develop a clear sense of connection with our organization and leadership - as Data Engineering is a critical investment to scale analytics capabilities and drive strategy. This is a partnership-heavy role. As a member of LCS Data Engineering team, you will belong to a centralized Analytics and Data Engineering team who partners closely with teams in Facebooks legal department. Through the consulting-nature of our team, you will contribute to a variety of projects and technologies, depending on partner needs. Projects include analytics, data pipeline frameworks, tooling, services and more.
Partner with leadership, engineers, program managers and data analyst to understand data needs.
Design, build and launch efficient and reliable data pipelines transforming data into useful report ready datasets.
Broad range of partners equates to a broad range of projects and deliverables including data pipeline frameworks, datasets, measurements, services, tools and process.
Use your data and analytics experience to see whats missing identifying and address data gaps, build monitor to detect data quality issues and partner to establish a self-serve environment.
Leverage data and business principles to automate data flow, detect business exceptions, build diagnostic capabilities, improve both business and data knowledge base.
Build data expertise and own data quality for your areas.
5+ years of SQL experience
4+ years of Python development experience
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience analyzing data to discover opportunities and address gaps
5+ years experience in custom ETL design, implementation and maintenance
Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
Experience with more than one coding language
Experience in designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and e2e process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.
Experience working with Supply Chain or Data Center Operation team
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","San Francisco, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
"Data Engineer, Cell Qualification",$88k-$163k (Glassdoor Est.),"Description

As a member of the Cell Data Engineering team, your software will help contribute to accelerating the pace of battery innovation. The data pipelines that you build will enable Tesla to better characterize and predict battery lifetime. We are seeking someone who has built scalable software before that can be adapted to a new need with a minimum of modification.

Responsibilities
Design data pipelines and infrastructure for a multitude of cell characterization techniques.
Apply the same software framework to new test methods as they come online
Implement near real time monitoring and visualization of the data systems and their associated data
Improve the robustness of our experimental pipeline from the time we receive cells, to test, to results
Help construct machine learning infrastructure to help increase our learnings and experimental throughput
Build tables with the optimum indices for fast querying. These tables will also be constrained to reduce the chance of bad data leaking into our system
Requirements
BS/MS in Computer Science or or a similar background with strong software engineering experience
Experience building data scalable data pipelines complete with monitoring
Experience with Python and relational databases
Experience with a production task scheduler like celery or airflow
Experience with front end technology like React is a plus

Tesla participates in the E-Verify Program",3.5,"Tesla
3.5","Palo Alto, CA",-1,10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$2 to $5 billion (USD),-1
Big Data Engineer,-1,"Experience you will need:
3+ years in data engineering and processing of complex data pipelines.
Practical experience with object-oriented in Scala / Java
Strong knowledge of some of big data tools: Spark, Spark streaming, Hadoop/HDFS, Kafka, Akka, Flink
Experience with relational databases, including Postgres, MySQL and ETL processing.
Proven expertise with AWS cloud services: EC2, EMR, RDS, S3,.
Knowledge of automating testing, deployment of distributed systems & performance tuning.
Basic understanding in statistics
Experience building robust REST API (Json).
Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Experience that is nice to have:
Experience with machine learning libraries: MLlib, Python scikit-learn
NoSQL databases: HBase, Cassandra
Experience with data pipeline and workflow management tools: Airflow
NoSQL or Graph database: Neo4j
Job Type: Full-time

Pay: $100,000.00 - $115,000.00 per year

Schedule:
8 hour shift
Experience:
Scala: 2 years (Required)
Relational Databases: 2 years (Required)
Spark: 2 years (Required)
Java: 2 years (Required)
Work Location:
Fully Remote",2.6,"Expert Hiring
2.6","Pleasanton, CA",-1,201 to 500 Employees,2009,Company - Private,Staffing & Outsourcing,Business Services,$50 to $100 million (USD),-1
"Staff Data Engineer, Tech",$122k-$212k (Glassdoor Est.),"Position Summary...

What you'll do...

Problem Formulation: Translates business problems within one's discipline to data related or mathematical solutions. Identifies what methods (for
example, analytics, big data analytics, automation) would provide a solution for the problem. Shares use cases and gives examples to demonstrate
how the method would solve the business problem.
Applied Business Acumen: Provides recommendations to business stakeholders to solve complex business issues. Develops business cases for
projects with a projected return on investment or cost savings. Translates business requirements into projects, activities, and tasks and aligns to
overall business strategy. Serves as an interpreter and conduit to connect business needs with tangible solutions and results. Recommends new
processes and ways of working.
Data Governance: Establishes, modifies, and documents data governance projects and recommendations. Implements data governance practices in
partnership with business stakeholders and peers. Interprets company and regulatory policies on data. Educates others on data governance
processes, practices, policies, and guidelines. Provides recommendations on needed updates or inputs into data governance policies, practices, or
guidelines.
Data Source Identification: Understands the priority order of requirements and service level agreements. Defines and identifies the most suitable
sources for required data that is fit for purpose, referring to external sources as required. Performs initial data quality checks on the extracted data.
Reviews the deliverables of junior associates and provides guidance.
Data Transformation and Integration: Builds the infrastructure required for optimal transformation and integration from a wide variety of data sources
using appropriate data integration technologies. Uses modern tools, techniques, and architectures to partially or completely automate the most common,
repeatable and tedious data preparation and integration tasks. Deploys pipelines using scheduling and orchestration frameworks. Evaluates
impacts of data issues and risks at an early stage. Identifies needs and creates methods to fuse and reshape complex, multi-source data and make it
usable for modeling. Updates knowledge of current and emerging big data analytics and data science trends and techniques.
Data Modeling: Assembles large, complex data across all data platforms (for example, relational, dimensional, NoSQL) and data tools . Builds
complex logical and conceptual models and provides guidance to team on physical data models. Identifies and defines the appropriate techniques for
exposing data to other systems. Reviews and provides guidance and inputs on all data modeling activities to team members. Creates and maintains
critical data documentation and metadata that allows data to be understood and leveraged as a shared asset. Assists in defining data modeling
standards and foundational best practices. Provides inputs to the architectural design to make best use of the available resources, given goals, and
expected loads.
Code Development and Testing: Reviews the solution and application design to ensure it meets business, technical, and data requirements. Identifies
language and libraries to use in the development process. Maps test cases to business and functional requirements. Creates proof of concepts.
Reviews and troubleshoots code in line with final designs. Identifies and recommends the appropriate testing methodology. Identifies the
environment(s) for deployment. Identifies and recommends modifications of application based on different environment requirements. Identifies
modifications needed for scalability and drives the change. Monitors applications in production and leads development of patches where required.
Reviews and ensures all code documentation is complete and updated periodically. Review work of Junior associates in the team.
Data Strategy: Understands, articulates, interprets, and applies the principles of the defined strategy to unique, moderately complex business
problems that may span one or main functions or domains.
Drives the execution of multiple business plans and projects by identifying customer and operational needs; developing and communicating business
plans and priorities; removing barriers and obstacles that impact performance; providing resources; identifying performance standards; measuring
progress and adjusting performance accordingly; developing contingency plans; and demonstrating adaptability and supporting continuous learning.
Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to
others in their use and application; ensuring compliance with them; and utilizing and supporting the Open Door Policy.
Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives; consulting with business
partners, managers, co-workers, or other key stakeholders; soliciting, evaluating, and applying suggestions for improving efficiency and cost effectiveness;
and participating in and supporting community outreach events.

Minimum Qualifications...

Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.

Option 1: Bachelor’s degree in Computer Science and 4 years' experience in software engineering or related field. Option 2: 6 years’ experience in
software engineering or related field. Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related
field.
3 years' experience in data engineering, database engineering, business intelligence, or business analytics.

Preferred Qualifications...

Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.

Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 4 years' experience in software engineering",3.3,"Walmart
3.3","Sunnyvale, CA",-1,1001 to 5000 Employees,1962,Company - Public,"Department, Clothing, & Shoe Stores",Retail,$10+ billion (USD),-1
Big Data Engineer,-1,"8+ years of back-end engineering experience, preferably Java Maven or Scala or Node.js
5+ years of experience in building IoT Bigdata and Analytics solution utilizing ETL tools, Kafka, Java or Scala, PL/SQL, Pentaho or Big Query SQL and Shell scripting
5+ years of Web development using Django or Angular JS, JavaScript frameworks, EXT JS, MongoDB or NoSQL or RDBMS
3+ years of API design, development and integration experience
2+ years of hands on experience creating applications and tools using Cloud Platform (example: Google Big Query, Cloud Storage, Cloud Functions, Pub/Sub, App Engine, Cloud SQL)
1+ year of experience in deploying AI models in preferably Google Cloud using Google Cloud Platform tools.
1+ year of experience in designing and scaling cloud-based applications (Google Cloud Platform and Amazon Web Services).",4.7,"JAMY INTERACTIVE, INC
4.7","Santa Clara, CA",-1,1001 to 5000 Employees,-1,Company - Public,-1,-1,Less than $1 million (USD),-1
Senior Data Engineer,$96k-$174k (Glassdoor Est.),"Company Description Robert Bosch is a world-class engineering and manufacturing company. Our products impact hundreds of millions of people every day, many in safety critical systems. Artificial intelligence is impacting Bosch’s products and services in many domains, including: manufacturing, autonomous driving, predictive maintenance, vehicle diagnostics and supply chain & logistics. The Bosch Center for Artificial Intelligence provides services in AI technologies to Bosch’s business units and plants. We are a global team of data scientists, engineers and research scientists focused on applying state-of-the-art AI technologies to improve all aspects of Bosch’s products and operations. Job Description Primary Responsibilities: Extend and improve the existing data platform which serves hundreds of users from assembly plants worldwide. Promote a culture of self-serve data analytics by minimizing technical barriers to data access and understanding. Share knowledge by clearly articulating results and ideas to customers, managers, and key decision-makers. Stay current with the latest research and technology and communicate your knowledge throughout the company. Up to 5% travel may be required (post-COVID-19). Qualifications Basic Qualifications: Bachelor's Degree in Computer Science or a related technical field. 2+ years industry experience in building and operating distributed data systems. 2+ years of programming experience in Scala or Java. Preferred Qualifications: Experience with Big Data technologies, such as Apache Spark. Proficiency in building data pipelines to stream and process datasets at low latencies. Working experience with public clouds is a plus (Azure or similar). Familiarity with our technical stack (Kubernetes, Prometheus, Grafana, Airflow, Jenkins) Experience operating large data warehouses or data lakes. Proficiency in a scripting language - Python, bash or similar. Additional Information BOSCH is a proud supporter of STEM (Science, Technology, Engineering & Mathematics) Initiatives FIRST Robotics (For Inspiration and Recognition of Science and Technology) AWIM (A World In Motion) By choice, we are committed to a diverse workforce – EOE/Protected Veteran/Disabled.",4.1,"Bosch Group
4.1","Sunnyvale, CA",-1,10000+ Employees,1886,Company - Private,Miscellaneous Manufacturing,Manufacturing,$10+ billion (USD),-1
Senior Data Engineer,$158k-$288k (Glassdoor Est.),"Ready to shake things up? Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and strive to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun, and most significantly to each other’s success. We continue to be on a tear while enjoying incredible growth year over year. As a Cloud Data Engineer, you should be an expert with data warehousing technical components (e.g., ETL, ELT, Cloud Databases and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise-level data lake solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions. What you'll do: Yeah, I want to and can do that. As a Data Engineer, you will be responsible for engineering data pipelines for Splunk’s enterprise data platform, democratizing datasets, enabling advanced analytics capabilities, integrating data from various systems, and applications. You will work as part of an evolving Data Technologies Team to rapidly design, secure, build, test and release new data enablement capabilities. The role will collaborate closely with other specialists, InfoSec Consultants, Product Managers & Owners, and internal customers. Set the technical direction for data integration and establish a data processing framework for Enterprise Data Warehouse. Build large-scale batch and real-time data pipelines using the cloud data technologies, such as Snowflake, Matillion, FiveTran, Python, Apache Airflow and Apache Kafka Serve as a resource for data management implementations on other technology teams and collaborate with data owners, business owners, and leaders. Supports the design and development of framework based data integration and interoperability across multiple Splunk Business applications. Advanced knowledge of physical database design and data structures. Expert knowledge of one or more database management systems such as AWS Redshift, Snowflake. Expert level skills in SQL, data integration, data modeling and data architecture. Requirements: I’ve already done that or have that! 10+ years of experience as a Data Warehouse Architect or Data Engineer. 5+ years of experience driving adoption and building automation of data management services and tools. 5+ years of experience with API based ELT automation framework, data management, or interface design, development and maintenance. Large scale design, implementation and operations of Cloud data storage technologies such as AWS Redshift and Snowflake. 5+ years of experience with programming scripting and data science languages such as Python, R, SQL, etc. Strong Experience with data management platforms (e.g. Matillion, FiveTran, Talend) Experience in building data models, including conceptual, logical, and physical for Enterprise Relational, and Dimensional Databases. Working knowledge of Big Data concepts in organizing both structured and unstructured data is a big plus Preferred knowledge and experience: These are a huge plus. Knowledge of Splunk products Education: Got it! Bachelor’s degree preferably in Computer Science, Information Technology, Management Information Systems, or equivalent years of industry experience. What We Offer You: Wow, I want that. A constant stream of new things for you to learn. We're always expanding into new areas, bringing in open source projects and contributing back, and exploring new technologies. A set of extraordinarily hardworking, innovative, open, fun and dedicated peers, all the way from engineering and QA to product management and customer support. Growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe mentors help both sides of the equation. A stable, collaborative and supportive work environment. We don't expect people to work 12 hour days. We want you to have a successful time outside of work too. Want to work from home sometimes? No problem. We trust our colleagues to be responsible with their time and dedication and believe that balance helps cultivate an extraordinary environment This isn’t a job – it’s a life changer – are you ready? Splunk has been named one of San Francisco Bay Area’s “Best Places to Work” by the San Francisco Business Times, ten years in a row. We offer a highly competitive compensation package and a plethora of benefits. Splunk is proud to be an equal opportunity workplace and is an affirmative action employer. We value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which the candidate is applying. For job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records.",4.1,"Splunk
4.1","San Jose, CA",-1,1001 to 5000 Employees,2003,Company - Public,Enterprise Software & Network Solutions,Information Technology,$1 to $2 billion (USD),-1
"Data Engineer, Infrastructure Strategy",$115k-$187k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

How would Facebook scale to the next billion users? The Infrastructure Strategy group is responsible for the strategic analysis to support and enable the continued growth critical to Facebooks infrastructure organization.

We are looking for a Data Engineer to not only build data pipelines but also extend the next generation of our data tools. As a Data Engineer, you will develop a clear sense of connection with our organization and leadership - as Data Engineering is the eyes through which they see the product.

This is a partnership-heavy role. As a member of Infrastructure Strategy Data Engineering, you will belong to a centralized Data Science/Data Engineering team who partners closely with teams in Facebooks Infrastructure organization. Through the consulting-nature of our team, you will contribute to a variety of projects and technologies, depending on partner needs. Projects include analytics, ML modeling, tooling, services, and more.
Partner with leadership, engineers, program managers and data scientists to understand data needs.
Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.
Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.
Educate your partners: Use your data and analytics experience to see whats missing, identifying and addressing gaps in their existing logging and processes.
Broad range of partners equates to a broad range of projects and deliverables: ML Models, datasets, measurements, services, tools and process.
Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.
Build data expertise and own data quality for your areas.
5+ years of Python development experience.
5+ years of SQL experience.
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
3+ years experience with Data Modeling.
Experience analyzing data to discover opportunities and address gaps.
5+ years experience in custom ETL design, implementation and maintenance.
Experience working with cloud or on-prem Big Data/MPP analytics platform(i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
Experience with more than one coding language.
Designing and implementing real-time pipelines.
Experience with data quality and validation.
Experience with SQL performance tuning and e2e process optimization.
Experience with anomaly/outlier detection.
Experience with notebook-based Data Science workflow.
Experience with Airflow.
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Senior Data Engineer - Java,-1,"We are looking for a data-driven problem solver who can automatically discover and extract information from multiple data sources. The senior data engineer will work to recognize patterns and productize your solutions and techniques for integration into our products and our processes. You will have a high degree of authority and autonomy, be capable of managing multiple high-priority tasks in a timely manner, and collaborate across the business. You'll get to build things from scratch! A successful senior data extraction engineer at Plum needs to be innovative, detail-oriented, team-focused, strategic and effective in new and changing environments with a high degree of confidence using Java. One who can dream up and see a project through to conclusion. We are looking for the candidate to start immediately. Data munging with emphasis on ability to deal with imperfections in data. Develop refine and scale data management and analytics procedures, systems, workflows, best practices and other issues. Development of data-driven products. Visualize and communicate data clearly for use both internally and externally. Collaborate with our engineers to produce excellent products for Plum clients as well as streamline internal processes. REQUIRED QUALIFICATIONS Data cleansing, curation, parsing, integration, semantic mapping, and editing. High level language proficiency in Java, prefer also Scala. An entrepreneurial spirit as well as passion for solving difficult challenges through innovation and creativity, with a strong focus on results. Bachelor’s degree in Mathematics, Statistics, Engineering, Computer Science or related discipline. 4+ years-experience in developing large scale distributed software systems. Experience with scraping social media, using social media APIs. Strong knowledge of Linux / UNIX systems. Conscientious and well organized. Eager to produce results and drive forward progress while managing deadlines. Legal authorization to work in the U.S. is required. PREFERRED QUALIFICATIONS Knowledge of machine learning concepts and NLP techniques. Experience in working with big data technologies Spark, MapReduce, NoSQL. Experience in AWS infrastructure. Understanding of statistics. Experience with finance technology/analytics and real estate. BENEFITS Early equity in a startup that is revolutionizing commercial real estate lending. Competitive package of base pay and stock options. Generous health, dental and vision coverage for employees and family members, along with commuter pre-tax program. Unlimited vacation policy. Opportunity to make a meaningful impact on the disruption of an industry and to shape the building of a company and culture. Chance for your direct input to be realized and put into action. Freedom to stretch the boundaries of your past work experience, learn skills outside of your immediate job description and grow your career. Autonomy, flexibility and a flat corporate structure. ABOUT PLUM After decades of experience in institutional financial services, the founders of Plum recognized the need to challenge the status quo which makes the commercial real estate (CRE) lending process opaque and cumbersome. Plum is a more agile and advisory financial technology company that combines data intelligence with best-in-class financial expertise to modernize CRE lending. Our CEO, Bill Fisher, has decades of experience building successful startup businesses, including GetSmart.com, Xing and Trivago. Our team includes senior leaders and talent from AIG, Goldman Sachs, KKR, McKinsey, PWC, Bank of America, Meridian Capital, A10, JP Morgan, Freddie Mac, US Bank, Wells Fargo and PNC Real Estate. In April 2018, Plum announced a Series B equity investment by the $35 billion hedge fund, Elliott Management. Plum’s Series A round in August 2015 was led by Renren Inc, who has backed other fintech companies including SoFi, LendingHome and Motif Investing. This followed an earlier seed investment by QED Investors, a pre-eminent VC firm led by the founders of Capital One, whose portfolio includes Prosper, Orchard and ApplePie Capital. Plum is headquartered in the heart of San Francisco’s Financial District in an airy, industrial loft, close to all forms of public transportation. *Women and minorities are encouraged to apply *No Relocation",4.1,"Plum Lending
4.1","San Francisco, CA",-1,1 to 50 Employees,2014,Company - Private,Lending,Finance,Unknown / Non-Applicable,-1
"Data Engineer, Mission Control",$106k-$175k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Mission Control is the organization that plans and shapes Facebooks evolving workforce. The Analytics team within Mission Control supports the TECH organization by building and scaling solutions that support our workforce planning and programs. We work with stakeholders from across the company to manage headcount demand plans, tell the story of headcount progression and associated components, build resources to manage the flow of talent, and provide data driven insights into the health and development of our global programs. On our team, youll see a direct link between your work, company growth, and our ability to transform over time.
Were looking for a Data Engineer to build and maintain our data pipelines, create custom tables, develop automation, support tooling integrations, and facilitate data migrations to up-level our analyst productivity and efficiency. This role will primarily be supporting our Data Architect, Analysts, and broader Mission Control team.
Build deep HR data expertise and own data quality for allocated areas of ownership
Interface with Data Architect, Analysts and Program Managers to understand data needs
Architect, build, launch and maintain Mission Control specific data pipelines, models and tables to provide intuitive analytics
Triage and investigate data quality and pipeline errors
Use your expert coding skills across a number of languages including Python and Java or PHP
Design, develop and provide technical support for tooling integrations
Build framework for auditing, error logging and master data management for your pipelines
Support on-call shift as needed to support the team
Collaborate with team members to maintain and enhance current data and processing infrastructure
3+ years of Python development experience
3+ years of SQL experience
3+ years of experience in custom ETL design, implementation and maintenance
3+ years of experience in the data warehouse space
3+ years of experience with schema design and dimensional data modeling
Experience analyzing data to identify deliverables, gaps and inconsistencies
Experience leading data driven discussions
Experience with more than one coding language
Experience with designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and E2E process optimization
Experience with anomaly/outlier detection
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc
BS in CE/EE/CSE or computational sciences
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
"Data Engineer- Strong in SQL, Python, ETL- Hadoop",-1,"Please email resume to INFO 'at' ICLOUDNEXUS 'dot' COM*
Data Engineer

San Jose, CA

Long Term

Direct client

Immediate Interviews

We are looking for a Senior Data Engineer with advanced knowledge of SQL and intermediate knowledge of Python.
Nice to have (but not required) beginner or intermediate level java experience.
Your primary focus will be the writing complex SQL queries, optimizing them and development of all server-side
backend data processing logic, ensuring high performance using Python and SQL.

Skills:
 Basics of Computer Science - OOPS, Data Structures and Algorithms.
 Basic understand of regular Linux commands and usage.
 5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries,
function and stored procedures.
 3+ years of experience with hands on experience in writing, debugging Python code on Linux.

Job Types: Full-time, Contract

Work Remotely:
Yes",-1,iCloudNexus,"San Jose, CA",-1,Unknown,-1,Company - Public,-1,-1,Less than $1 million (USD),-1
Sr. Data Engineer,$99k-$176k (Glassdoor Est.),"The Proterra Story Communities are growing and evolving, and with that, our transportation needs are changing. Now more than ever, we need smart solutions that provide safer, more reliable and cleaner transit. Every day, Proterra works to meet those needs, with the world’s best-performing zero-emission buses. Our revolutionary battery-electric buses help fleet operators abandon fossil fuels, improve environmental quality and reduce operating costs. Join the Proterra Revolution. Position Overview Connected Technology Team is building Proterra’s next-gen Telemetry & IOT (Vehicle and Charger data) platform which will enable our customers to optimize asset utilization, access valuable data & analytics via mobile and SaaS solutions, enable Engineering to monitor and analyze in-service data for continued product improvement, and enable Service with predictive and real-time data to support proactive Customer Support. In this role, you will work within the software engineering team to develop, test and maintain databases, data models, APIs, and large-scale processing applications for our connected vehicles platform. This position will also require you to innovate and implement best data governance practices for the platform. About the Role – You will: Have ownership of the data governance, data models, schema design, and databases including tuning, space management, performance management and policy execution Monitor and manage databases across environments, including assisting in the software release process Monitor and manage backend data synchronization applications and external data source retrieval, APIs and applications Expand and optimize our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies. Perform validation procedures to ensure data quality Recommend and implement ways to improve data reliability, efficiency, and quality Conduct systems tests for security, performance, and availability Promote data architecture best practices and standardize on endorsed data storage and message distribution technologies Define security and backup procedures Develop and maintain schema design, data model, API, and troubleshooting documentation Work with the engineering teams to help choose technologies, design system architecture and model data in a scalable and efficient way About Our Group: As strategic partners, the Information Technology team strives to ensure that our values, culture and engagement always allow us to do great things for our company. We strive to provide the highest-level service and support to our clients each and every day. We work collaboratively to build strong relationships with our clients, partners, and own team. We treat each other with respect……and always try to have a little fun every day! You will report to the Manager, Software Engineering and collaborate with other IT team members located at our headquarters in Burlingame, CA and manufacturing facility in Greenville, SC and Los Angeles, CA. About You: Self-starter with the ability to adapt interpersonal styles and techniques to influence at all levels of the organization. Customer-focused attitude, with high level of professionalism and discretion. Ability to maintain strict confidentiality, establish trust and credibility, and act with complete integrity. Detail-oriented, resourceful and diligent. Strong time management and organizational skills. Sound judgment and team problem-solving skills. Excellent communication skills. Your Experience Includes: Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. Thorough understanding of the software development lifecycle and tools used to create data pipelines and perform ETL, with passion to optimize data systems and to build a data architecture from the ground up. Advanced working knowledge in SQL and experience working with relational databases Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores A successful history of manipulating, processing and extracting value from large disconnected datasets Strong experience with object-oriented/object function scripting languages such as Python, Java, etc. (Java is required) Experience with stream-processing systems such as Amazon Kinesis, Spark-Streaming, etc. Ability to use a wide variety of open source technologies and cloud services (experience with AWS, Docker, Kubernetes and Elastic Cloud is required) Experience with AWS cloud services: EC2, ECS/EKS, RDS, SQS, Lambda, Redshift, Glacier, Amazon IoT etc. Significant experience scaling solutions that run on private, public, and hybrid cloud infrastructures Strong background in Linux/Unix Administration Experience with data pipeline workflow management tools (Azkaban, Luigi, Airflow) Bachelor’s degree in Computer Science, Engineering, or relevant field 5+ years’ experience as a Data Engineer or equivalent software-engineering role Able to empathize, sell ideas, and influence others Able to multitask, prioritize, and manage time efficiently Travel: 0 ~ 25% Location: Burlingame, CA Proterra is an Equal Employment Opportunity Employer, providing equal employment opportunities to all Employees and applicants for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity, national origin, disability, age, genetic information, veteran status, pregnancy, childbirth, or related medical conditions, including, but not limited to, lactation or any other characteristic protected by applicable federal, state, or local law or ordinance. Proterra participates in the Electronic Employment Verification Program (E-Verify). Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)",3.5,"Proterra, Inc.
3.5","Burlingame, CA",-1,201 to 500 Employees,2004,Company - Private,Transportation Equipment Manufacturing,Manufacturing,$25 to $50 million (USD),-1
"Data Engineer, Analytics (Family Ecosystems)",$118k-$192k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Our more experienced data engineers are clearly characterized by in-depth technical experience, subject matter expertise and proven progression in leadership responsibility. If you have an interest in owning important and critical problem areas and influencing by building robust company-wide data foundation and tooling, this is the right role for you. You will get to impact the End-to-End (E2E) suite of big-data tools and products that play a critical part in the day-to-day development lifecycle of Data Engineers, Data Scientists, ML Engineers, Research Scientists & Software Engineers.In this role, you will work closely with Data Infrastructure/Product Software Engineering and Product Management teams to foundationally evolve long-term, architecture-driven, E2E analytics development cycle and the Data Products, Platforms, Tools and Infrastructure stacks that underlie such as Logging, Streaming, Batch/Compute engines (Presto, Spark), Language/APIs, Semantic Data and Metadata models, ML workflows/models, Consumption workflows (Visualization/Notebooks), Data Discovery and so on. You will define and find solutions to complex and often ambiguous problems as a Subject Matter Expert. You will be leveraging your deep knowledge and experience to collaboratively define technical vision, strategy and architecture in three key areas Semantic Data and Metadata modeling, Large-scale analytics architecture (covering Logging, ETL and Consumption stacks) and Big Data development lifecycle (coding, testing, deploying, discovery etc.). A few examples of the impact and influence of your work: Consistent E2E Data Model and Definition-driven metrics such as Message Sends across the Family of Apps, Data model and metadata-driven, foundational, company-wide Analyics APIs such as User Retention, Evolving Dataframe APIs, Data Models and company-wide lifecycle development from Logging through Consumption through critical company-wide analytics use cases and Enabling consumption and adhoc exploratory workflows for Data Scientists by helping envision and implement large-scale analytics architecture use cases.
Craft and own the optimal data processing architecture and systems for new data and ETL pipelines/analytics applications
Build and data (dimensional) model core datasets and analytics applications and make them scalable and fault-tolerant
Drive comprehensive Technical Vision on fundamental aspects and evolution of Analytics/Data Infra Foundation/Tooling
Define and disseminate technical or product strategy clearly for effective outcomes
Articulate strategy within teams, effectively communicate with cross-functional
articulate solutions and influence leadership
Collaborate and work with different cross-functional partners - Data Infrastructure, Product Software Engineering, Data Engineering and Product Management teams - on use cases sto foundationally evolve long-term, architecture-driven, E2E analytics development cycle
Technically influence within the function and cross-functional community
Build visualizations to provide insights into the data & metrics
Immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions
Drive internal process improvements and automating manual processes
Provide ongoing proactive communication and collaboration throughout the organization
4+ years experience in the data warehouse space
4+ years experience working with either a MapReduce or an MPP system
7+ years experience in writing complex SQL, Dataframe APIs and ETL processes
4+ years experience with object-oriented programming languages
7+ years experience with schema design and dimensional data modeling
BS/BA in Technical Field, Computer Science or Mathematics
Knowledge in Python or Java or Scala or Pandas
Experience analyzing data to identify deliverables, gaps, and inconsistencies
Experience mentoring team members in their careers
Experience collaborating, defining and communicating complex technical concepts to a broad variety of audiences ariety of audiences
Experience scaling analytics architecture and worked with open source big-data stacks (Spark, Koalas etc.)
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Lead Big Data Engineer,$119k-$219k (Glassdoor Est.),"Job Title:
Lead Big Data Engineer

Location:
US, California, San Jose
Role Overview:


You will lead a team of Software Engineers to create the next generation of McAfee security products to enhance the auto detection and investigation of security breaches within large organizations. You will work with Architects and other Cloud Infrastructure Engineers to understand the designs, validate the features, and to deliver and integrate solutions in an AWS environment. If you are passionate about building security solutions that keep millions of enterprise customers safe with an outstanding user experience, then this position might be a perfect opportunity for you. You will report to the Sr. Manager, Software Engineering and will be based in San Jose, CA.



Company Overview


From device to cloud, McAfee provides market-leading cybersecurity solutions for both business and consumers. We help businesses orchestrate cyber environments that are truly integrated, where protection, detection, and correction of security threats happen simultaneously. For consumers, McAfee secures your devices against viruses, malware, and other threats, both at home and away. We want to continue to shape the future of cybersecurity by working together to build best in class products and solutions.

About the Role:
Lead a group of Software Developers and oversee the whole solution including uptime by working with architects, development, and cloud infrastructure team.
Help develop our next-generation micro-services to enhance auto detection of different breaches and other security concerns.
Use your knowledge of Java, Go or Python to create new features in the cloud.
Understand and influence logging to support our Data flow.
Work and lead the development and infrastructure teams to open bugs, debug issues and achieve resolution.
Pioneer a new way of thinking about Data Pipelines, Orchestration and Configuration at McAfee.
Mentor other Developers.
About You:
Your background includes 8+ years of hands-on experience with Big Data technology(Kafka, Spark, Airflow).
Successful track record in developing and automating large-scale, high-performance data processing systems (batch and streaming).
Lead data engineering projects to ensure pipelines are reliable, efficient, testable and maintainable.
Experience designing Data Models for optimal storage and retrieval to meet critical product requirements.
Experience with both scripting and system programming languages (Python, Go and Scala).
Experience with microservices including defining and testing APIs.
Company Benefits and Perks:


We work hard to embrace diversity and inclusion and encourage everyone at McAfee to bring their authentic selves to work every day. We offer a variety of social programs, flexible work hours and family-friendly benefits to all of our employees.
Pension and Retirement Plans
Medical, Dental and Vision Coverage
Paid Time Off
Paid Parental Leave
Support for Community Involvement
We're serious about our commitment to diversity which is why McAfee prohibits discrimination based on race, color, religion, gender, national origin, age, disability, veteran status, marital status, pregnancy, gender expression or identity, sexual orientation or any other legally protected status.

Job Type:


Experienced Hire

Primary Location:
US, California, San Jose

Additional Locations:",3.5,"McAfee
3.5","San Jose, CA",-1,5001 to 10000 Employees,1987,Company - Private,Computer Hardware & Software,Information Technology,$10+ billion (USD),-1
BIG Data Engineer,$96k-$180k (Glassdoor Est.),"ARK Solutions, Inc is looking for Data Engineer for one of our clients in Sunnyvale, CA.

Position: BIG Data Engineer

Location: Sunnyvale, CA - Initial Remote

Skills: Data Engineer

6 + Professional experience with Big Data systems, pipelines and data processing
Hands on experience Big Data, data ingestion, data processing using Spark, Spark Streaming, Flink, HIVE, Kafka, Hadoop, HDFS, S3
Hands-on experience with design and development with NoSQL technologies Cassandra, HBase or similar scalable Key valueStores and time series data stores like Druid, influx or similar
Apache Parquet and common methods in data transformation
Confirmed understanding of design and development of large scale, high throughput and low latency applications is a plus
Understanding and experience with Micro Services is desired
Excellent problem solving and programming skills
Experience with containerization technologies like Kubernetes, Docker, Mesos, Marathon is desirable
Experience with CI/CD, debugging and monitoring applications and big data jobs is desirable",3.9,"ARK Solutions
3.9","Sunnyvale, CA",-1,201 to 500 Employees,2003,Company - Private,IT Services,Information Technology,$25 to $50 million (USD),-1
Sr. Data Engineer (Remote),$138k-$252k (Glassdoor Est.),"At CrowdStrike we’re on a mission - to stop breaches. Our groundbreaking technology, services delivery, and intelligence gathering together with our innovations in machine learning and behavioral-based detection, allow our customers to not only defend themselves, but do so in a future-proof manner. We’ve earned numerous honors and top rankings for our technology, organization and people – clearly confirming our industry leadership and our special culture driving it. We also offer flexible work arrangements to help our people manage their personal and professional lives in a way that works for them. So if you’re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to stop breaches and protect people globally, let’s talk.

About the Team:
The data engineering team is on a mission to create a hyper scale data lake, which helps finding bad actors and stop breaches. The team builds and operates systems to centralize all of the data the falcon platform collects, making it easy for internal and external customers to transform and access the data for analytics, machine learning, and threat hunting. As an engineer on the team you will contribute to the full spectrum of our systems, from foundational processing and data storage, through scalable pipelines, to frameworks, tools and applications that make that data available to other teams and systems.

Job Responsibilities :

Design, develop, and maintain a data platform that processes petabytes of data

Participate in technical reviews of our products and help us develop new features and enhance stability

Continually help us improve the efficiency of our services so that we can delight our customers

Help us research and implement new ways for both internal stakeholders as well as customers to query their data efficiently and extract results in the format they desire

Qualifications for Data Engineer :

We are looking for a candidate with a BS and 5+ years or MS and 3+ years in Computer Science or related field. They should also have experience with the following software/tools -

A solid understanding of algorithms, distributed systems design and the software development lifecycle

Solid background in Java/Scala and a scripting language like Python

Experience building large scale data pipelines

Strong familiarity with the Apache Hadoop ecosystem including : Spark, Kafka, Hive, Apache Presto, etc.

Experience with relational SQL and NoSQL databases, including Postgres/MySQL, Cassandra, DynamoDB

Good test driven development discipline

Reasonable proficiency with Linux administration tools

Proven ability to work effectively with remote teams

Experience with the following tools is desirable :

Go

Kubernetes

Jenkins

Parquet

Protocol Buffers/GRPC

#LI-JF1
#LI-Remote
#Stack

Benefits of Working at CrowdStrike:
Market leader in compensation and equity awards
Competitive vacation policy
Comprehensive health benefits + 401k plan
Paid parental leave, including adoption
Flexible work environment
Wellness programs
Stocked fridges, coffee, soda, and lots of treats

We are committed to building an inclusive culture of belonging that not only embraces the diversity of our people but also reflects the diversity of the communities in which we work and the customers we serve. We know that the happiest and highest performing teams include people with diverse perspectives and ways of solving problems so we strive to attract and retain talent from all backgrounds and create workplaces where everyone feels empowered to bring their full, authentic selves to work.
CrowdStrike is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.

CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",3.9,"CrowdStrike
3.9","Sunnyvale, CA",-1,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$100 to $500 million (USD),-1
Hiring Data Engineer/ Full-Stack Developers,$68k-$126k (Glassdoor Est.),"Data Engineers/ Full-Stack Developers*
2+ years*
Master’s degree in Computer Science or a related field, *
Job Location - San Jose, USA*
Design and develop computer applications software, which includes assembling large, complex data sets that meet business requirements for artificial intelligence software solutions.
Duties will include: *
designing and customizing software with the aim of optimizing operational efficiency, which includes building analytics tools to optimize key business performance metrics;
analyze user needs, which includes building the infrastructure required for optimal extraction, transformation, and loading of data using SQL and Cloud Computing ‘big data’ technologies;
identifying, designing, and implementing internal process improvements, such as re-designing infrastructure for greater scalability;
creating optimal data pipeline architecture; creating data tools for analytics and data scientist team members that assist them in building and optimizing our product;
keeping our data separated and secure across national boundaries through multiple data centers and Cloud Computing regions; working with stakeholders to strive for greater functionality in our data systems;
and utilizing knowledge of Optical Character Recognition, number corrections, object detection and counting, neural networks, data and statistical analysis, and machine learning methods to perform the duties described above.
REQUIREMENTS: *
Master’s degree in Computer Science or a related field,
two (2) years of relevant experience and should be based in the U.S.
This experience should include two (2) years of experience with designing and developing computer software applications using Object-oriented design and programming;
Python;
algorithm design and development;
neural network frameworks;
Linux Operating System; data engineering; statistical data analysis;
Machine Learning, including Bayesian modeling, multivariate and logistic regression, support vector machines, cluster analysis, decision and regression trees, random forest, neural networks and ensemble methods;
and analyzing large, complex, multi-dimensional data sets and developing analytic solutions.
Job Type: Full-time

Salary: $118,575.00 - $188,047.00 per year

Schedule:
8 hour shift
Day shift
Monday to Friday
This Company Describes Its Culture as:
Innovative -- innovative and risk-taking
Outcome-oriented -- results-focused with strong performance culture
Stable -- traditional, stable, strong processes
People-oriented -- supportive and fairness-focused
Team-oriented -- cooperative and collaborative
Work Remotely:
Temporarily due to COVID-19",3.8,"Scry Analytics
3.8","San Jose, CA",-1,51 to 200 Employees,2014,Company - Private,Computer Hardware & Software,Information Technology,Less than $1 million (USD),-1
VDC Data Engineer,-1,"VDC 4D/5D Data Engineer REDWOOD CITY, CA / OPERATIONS & ENGINEERING / FULL TIME Problem In the $12T construction industry, 98% of projects are delivered, on average, 80% over budget. Doxel is on a mission to stop that from happening. Root cause? Inability to measure site progress in an objective, trustworthy and frequent manner. Construction managers are frequently surprised to discover that their multi-million dollar projects have been running behind for several weeks or months. By that point, the money has already been spent and it s too late to fix the problem. Solution Doxel uses robots to survey sites every day, proprietary deep learning and AI based algorithms to assess progress and turns the terabytes of data it collects into simple insights for project managers that enable them to react to issues in minutes, not months. This enables them to constantly correct site inefficiencies and has demonstrated ROI by eliminating overages and even delivering projects up to 11% below budget. Doxel has taken the market by storm since its launch last year - closing deals with marquee customers such as Kaiser Permanente, Sutter Health, and the Lucas Museum of Narrative Art. Team Doxel s team consists of some of the brightest minds in Silicon Valley that include PhDs, engineers, business leaders and civil engineering professionals on the Forbes 30 Under 30 list, graduates from Stanford University and with experience at organizations such as Google Advanced Technologies & Projects. Doxel is backed by Andreessen Horowitz - famed investor that also backed Facebook, Coinbase, Slack, Airbnb, Github and Lyft. The Role Doxel has an immediate need for a Virtual Design and Construction (VDC) professional with expertise in detailing software (such as Revit and AutoCAD MEP), Navisworks model management platform, and Project Schedule analysis. Your mission for this role is to organize 3D design models with detailed tracking sets and map this organization back to schedule tasks for project schedule tracking efforts. The secondary role is to work with the Doxel Product and Engineering groups to develop utilities for improving model organization efficiencies and tracking tools. This will involve: Analyze Project Schedules Analyze Cost Data in Work Breakdown Structures (WBS) Organize 3D Models with Schedule and Cost Data Work Closely with Product and Engineering Teams for Project Deliverables As a BIM Data Engineer at Doxel, you will have a clear understanding of the Project Schedules and Design Models used for the Doxel analytics engine. This includes familiarization with design detailing software and scheduling platforms used in the construction industry. You will maintain constant communication with the Doxel BIM Manager, Product Manager, and other Doxel Team members. You will support all aspects of the Doxel data pipeline and achieve high levels of accuracy in data organization. Working at Doxel keeps you at the leading edge of the latest 3D Scanning Technology while working with some of the brightest minds in this new industry. Our established team of Data Engineers look forward to adding an enthusiastic mind with a proactive attitude into their world of achieving a never before realized level of construction efficiency. If you believe you are ready to step up to the forefront of AI Technology while keeping Doxel ahead in our industry, then we want you on the Doxel team! Responsibilities 3D Model Data Organization is the primary responsibility for a BIM Data Engineer, which involves organizing models into highly detailed search sets based on schedule and cost data. Secondary Responsibilities Schedule Analysis – Review and organize schedule data from various schedule software platforms such as MS Project, Primavera P6, and others. Cost Analysis - Review and organize cost data from project Work Breakdown Structures. Product Development – Work with the Product and Engineering team to develop new modeling and data analysis utilities to improve operational workflows. Communication – Conduct ongoing communication throughout the workday with project team members. Doxel utilizes various tools for communication with Slack as the primary messaging platform. Jira tickets and tracking are also highly used. Direct Phone Calls and Conference Calls (Zoom as an option) may also be used. Training – Strive to continuously develop your skills in core applications used for model organization and data analysis. Qualifications Clear and precise verbal and written communication skills. Able to focus on completing primary objectives and any secondary objectives while remaining flexible and able to use good judgement to respond to changing field conditions. Able to safely lift and carry up to 40lbs of equipment throughout the workday. Preferred Qualifications BS or MS or foreign equivalent in Construction Digital Design Technology from reputed US Universities preferred (or) 1-2 years of experience in managing BIM detailing work from Pre-construction through Construction phases of commercial construction Proficiency in BIM Design and Analysis tools like AutoCAD MEP, Revit MEP and Navisworks Proficiency with Scheduling Software such as MS Project and Primavera P6 Knowledge of construction industry Proficiency with Microsoft Excel or Google Drive (Sheets) Familiarity with Slack or similar team communication and productivity platforms Familiarity with Jira or similar team issue & project tracking platforms Familiarity with Cloud Computing platforms such as AWS or GCP Excellent collaborative and organizational skills with the ability to communicate effectively Why Doxel? We work on deeply technical, groundbreaking engineering concepts. We like to think we are more competitive and compassionate than most others – Muhammad Ali, Ayrton Senna, and Vince Lombardi are our idols. We are fiercely competitive and unshakably compassionate. Intellectual honesty and radical transparency are core tenets at our company. We find that the trust developed in such culture far outweighs all else. Doxel is an equal opportunity employer and actively seeks diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Employment Specific Information Geographic Location Living in the continental United States, preferably on the West Coast to sync with the Doxel HQ time zone. Doxel HQ resides in the Bay Area of Northern California, but local residence is not required for employment. Equipment All necessary computing equipment and modeling software will be provided by Doxel. Salary Full time Doxel employee at $75k/yr with benefit options. Employment Requirements Must take and pass a background check and drug screening. Must sign an NDA stating employee will not divulge proprietary information and trade secrets to outside entities.",4.5,"Doxel
4.5","Redwood City, CA",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
"Data Engineer, Partnerships Central Systems, Data & Tools",$115k-$187k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Facebook is looking for exceptionally talented and experienced data engineers to join the Partnerships Central Systems, Data and Tools team. The Partnerships team at Facebook works with leading content creators, publishers, and businesses in entertainment, sports, news, and many other verticals. This role is a unique opportunity to work with one of the most important datasets in the world to create analytics tools and systems that enable field organizations, analysts and clients alike. You will work with some of the brightest minds in the industry, and get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match. The ideal candidate will have a passion for problem solving and a belief in the power of incremental change in a fast-paced environment.

Develop methods to unlock access to data for stakeholders across the Partnerships Organization
Design, build and maintain efficient & reliable data pipelines to move and transform data (both large and small amounts)
Architect build and launch new data models that provide intuitive analytics
Design and develop new systems and tools to enable folks to consume and understand data faster
Work cross-functionally to define problem statements, collect data, and make recommendations
Build data expertise and own data quality for allocated areas of ownership
Work with data infrastructure to triage infra issues and drive to resolution
Support on-call shift as needed to support the team

BS in Computer Science, Engineering, Math or related field
4+ years experience in custom ETL/data pipeline design, implementation and maintenance
4+ years experience of SQL (Oracle, Vertica, Hive, etc.)
Expert coding skills with at least one object-oriented programming language (Python, Java, PHP)
Experience with data architecture, data modeling, schema design and software development
Experience with large data sets, Hadoop, and data visualization tools
Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders

Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.

Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Senior SQL Data Engineer,-1,"Our Customer is one of the leading global interconnection platforms and the world’s largest data center provider. They connect the world's leading businesses to their customers, employees and partners inside the world's most connected data centers in 52 markets across five continents. Their mission is to protect, connect and power the digital economy.

We are seeking a Senior SQL Data Engineer on a contract basis with advanced knowledge of SQL and intermediate knowledge of Python.

In this role your primary focus will be writing complex SQL queries, optimizing them and development of all server-side backend data processing logic, ensuring high performance using Python and SQL.

The candidate must be able to pass I-9 verification.

What You’ll Do:

• Develop and maintain scalable ETL pipelines, build new pipelines and facilitate API integrations to support new requirements

• Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging

• Writing SQL functions, procedures as required based on the requirements

• Fine-tune or optimize queries to support the increasing volume of data

• Debug Python code, modify and enhance Python ETL applications based on the requirements on Linux environment

• Writing reusable and efficient code in Java, Python and SQL

• Develop Rest APIs using java libraries

• Write unit, functional, regression tests for enhanced feature, maintain engineering documentation

• Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture

Must Haves:

• 5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries, function and stored procedures.

• 3+ years of experience with hands on experience in writing, debugging Python code on Linux.

• Basics of Computer Science - OOPS, Data Structures and Algorithms

• Basic understanding of regular Linux commands and usage

• Experience writing Python applications that interact with ORM (Object Relational Mapper) libraries

• Knowledge of XML and JSON parsing with unit test and debugging skills

• Willingness and ability to learn new tools/languages as needed

• Process-oriented with excellent verbal and written communication skill with a desire for customer service

• Excellent team player and communicator who can work effectively with cross functional teams and ability to navigate ambiguity

• Beginner or intermediate level java experience a plus!

Education: Bachelor's Degree in Computer Science or related field

Hours & Location:

M-F, 40 hours/week. This role will be remote while Covid restrictions are in place. The expectation is to be onsite at Sunnyvale, CA location once it is deemed safe to do so.

Now for the Perks!

Health Benefits: Medical, Dental, Vision, Life (including spouse & child), 401k, STD/LTD, AD&D, and Commuter Benefits program.",4.7,"The Mom Project
4.7","Sunnyvale, CA",-1,51 to 200 Employees,2016,Company - Private,Staffing & Outsourcing,Business Services,Unknown / Non-Applicable,-1
Lead Data Engineer (remote),$144k-$264k (Glassdoor Est.),"San Francisco, California, USA Are you at your most vibrant when you’ve successfully distilled data into its simplest, most meaningful form? ThoughtWorks is a global software consultancy with an aim to create a positive impact on the world through technology. Our community of technologists thinks disruptively to deliver pragmatic solutions for our clients' most complex challenges. We are curious minds who come together as collaborative and inclusive teams to push boundaries, free to be ourselves and make our mark in tech. Our developers have been contributing code to major organizations and open source projects for over 25 years. They’ve also been writing books, speaking at conferences and helping push software development forward, changing companies and even industries along the way. We passionately believe that software quality is driven by open communication, review and collaboration. That’s why we’re such vehement supporters of open source and have made significant contributions to open source tools for testing, continuous delivery (GoCD), continuous integration (CruiseControl), machine learning and healthcare. As consultants, we work with our clients to ensure we’re evolving their technology and empowering adaptive mindsets to meet their business goals. You could influence the digital strategy of a retail giant, build a bold new mobile application for a bank or redesign platforms using event sourcing and intelligent data pipelines. You will learn to use the latest Lean and Agile thinking, create pragmatic solutions to solve mission-critical problems and challenge yourself every day. Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution. You’ll spend time on the following: You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems You will partner with teammates to create complex data processing pipelines in order to solve our clients’ most ambitious challenges You will collaborate with Data Scientists in order to design scalable implementations of their models You will pair to write clean and iterative code based on TDD Leverage various continuous delivery practices to deploy, support and operate data pipelines Advise and educate clients on how to use different distributed storage and computing technologies from the plethora of options available Develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions Create data models and speak to the tradeoffs of different modeling approaches On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product Seamlessly incorporate data quality into your day-to-day work as well as into the delivery process Here’s what we’re looking for: You are equally happy coding and leading a team to implement a solution You have a track record of innovation and expertise in Data Engineering You’re passionate about craftsmanship and have applied your expertise across a range of industries and organizations You have a deep understanding of data modelling and experience with data engineering tools and platforms such as Kafka, Spark, and Hadoop You have built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, NoSQL databases (Hbase, Cassandra, etc.) and any of the distributed processing platforms like Hadoop, Spark, Hive, Oozie, and Airflow in a production setting Hands on experience in MapR, Cloudera, Hortonworks and/or cloud (AWS EMR, Azure HDInsights, Qubole etc.) based Hadoop distributions You are comfortable taking data-driven approaches and applying data security strategy to solve business problems You’re genuinely excited about data infrastructure and operations with a familiarity working in cloud environments Working with data excites you: you have created Big data architecture, you can build and operate data pipelines, and maintain data storage, all within distributed systems Advocate your data engineering expertise to the broader tech community outside of ThoughtWorks, speaking at conferences and acting as a mentor for more junior-level data engineers Assure effective collaboration between ThoughtWorks’ and the client’s teams, encouraging open communication and advocating for shared outcomes A few important things to know: While we’ve traditionally been a traveling consultancy, travel is not required for this role at the moment. We anticipate the need for travel to our client locations in the future when it’s deemed safe. Candidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD. Not quite ready to apply? Or maybe this isn’t the right role for you? That’s OK, you can stay in touch with AccessThoughtWorks, our learning community (click ""contact me about recruitment opportunities"" to hear about jobs in the future). It is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex/gender, national origin, ethnic origin, veteran or military status, family or marital status, disability, genetic information, age, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.",3.8,"ThoughtWorks
3.8","San Francisco, CA",-1,5001 to 10000 Employees,1993,Company - Private,IT Services,Information Technology,$100 to $500 million (USD),-1
Senior Data Engineer,-1,"As a Data Engineer at Eaze, you'll report to the VP of Engineering and work on our data pipelines and infrastructure to help improve data accessibility, increase data reporting performance, and ensure scalability of our data warehouse as we scale. Maintaining, enhancing, and architecting our data infrastructure Architecting and building internal apps for the data team Architecting and building denormalized tables for our data warehouseBuilding ETL pipelines to move and transform data from internal and external sources Performance tuning our data warehouse Basic Qualifications 3+ years previous experience working on data or engineering teams Demonstrated excellence in Python Demonstrated excellence in SQL Experience with AWS infrastructure (Kinesis, SQS, S3, etc) Experience with Redshift or other large scale data warehousing solutions Experience with Postgres or other transactional databases A passion for analyzing data and working on open ended problems Strong communication and organizational skills Self-motivated with the ability to work independently Must be authorized to work in the United States",-1,Eaze,"San Francisco, CA",-1,-1,-1,-1,-1,-1,-1,-1
"(Senior) Data Engineer, DevOps",-1,"The Opportunity

Data Engineering plays a key role in insitro’s approach to rethinking drug development. The Data Engineering DevOps team ensures the infrastructure which powers our biological data factory’s robots, instruments, and machine learning platform is reliable, scalable, and manageable. You will work closely with a cross-functional team of scientists, bioengineers, and data scientists to identify areas where data engineering can make a difference, by developing data architectures and systems on cutting edge, high throughput platforms that enable our scientists to be maximally productive. You will design, implement, and deploy cloud infrastructure, including managed databases, application servers, data warehouses, and interactive/batch computing environments, and work as part of a team to rigorously design our data platform, identify key architectural performance improvements, and join an on-call rotation to ensure that insitro's platform runs at maximum productivity.

You will be joining as the founding team of a biotech startup that has long-term stability due to significant funding, but yet is very much in formation. A lot can change in this early and exciting phase, providing many opportunities for significant impact. You will work closely with a very talented team, learn a broad range of skills, and help shape insitro’s culture, strategic direction, and outcomes. Join us, and help make a difference to patients!

About You

2-3 years of experience with provisioning AWS cloud services (Experience with GCP and Azure is also relevant).
Experience with cloud configuration and resource management tools such as Terraform
Experience architecting reliable infrastructure platforms including monitoring and alerting, load balancing, scalable services, multi-region
Experience with at least one high-end distributed data processing environment (Hadoop, Spark, etc)
Experience with batch computing systems such as AWS Batch, SLURM
Experience with container build and deployment systems like Docker, Kubernetes, or ECS
Ability to communicate effectively and collaborate with people of diverse backgrounds and job functions
Proficiency in Linux environment (including shell scripting and Python programming), experience with database languages (e.g., SQL, No-SQL) and experience with version control practices and tools (Git, Mercurial, etc.)
Passion for making a difference in the world

Nice to Have

Experience with biological data
Experience with managing medium-sized data sets (100TB+) in object storage systems like S3
Experience with defining infrastructure following compliance (GDPR, HIPAA, etc).
Experience with data processing pipelines
Experience with deploying and monitoring machine learning models in a production environment

Benefits at insitro

Excellent medical, dental, and vision coverage
Open vacation policy
Team lunches (catered daily)
Commuter benefits
Paid parental leave

About insitro

insitro is a data-driven drug discovery and development company using machine learning and high-throughput biology to transform the way that drugs are discovered and delivered to patients. The company is applying state-of-the-art technologies from bioengineering to create massive data sets that enable the power of modern machine learning methods to be brought to bear on key bottlenecks in pharmaceutical R&D. The resulting predictive models are used to accelerate target selection, to design and develop effective therapeutics, and to inform clinical strategy. insitro was launched in 2018 with a Series A of $100M funded by top investors including a16z, Arch Venture Partners, Foresite Capital, GV, and Third Rock Ventures. In 2019 the company announced a collaboration with Gilead Sciences in the area of NASH and, in mid 2020, announced a Series B financing of $143M including current investors and new investors Canada Pension Plan Investment Board (CPP Investments), T. Rowe Price, BlackRock, Casdin Capital and other leading investors. The company is located in South San Francisco, CA. For more information about insitro, please visit the company’s website at www. insitro.com.",-1,insitro,"South San Francisco, CA",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Backend Data Engineer,-1,"About Live Objects
Live Objects delivers continuous business process optimizations to the enterprise through AI-driven automation of discovery, design and process engineering by mining patterns in business objects, cases, and transactions across all process variations. The product integrates deeply with business process management platforms (like SAP, Salesforce, etc.) and delivers continuous process engineering natively as on-demand compositions through the platform’s interfaces. Live Objects’ path-breaking process-calculus engine models predictive, diagnostic and quality-related projections for live cases in business processes. Its AI-driven on-demand process reengineering engine composes process enhancements with rule-based associations with business objects for delivering quantified time, process, margin, and cost efficiency gains. The on-demand compositions can be subject to review by functional subject matter experts in CRM, ERP, MRP, order-to-cash, etc. before deploying them into live business processes. Ongoing client engagements include self-optimizing a wide spectrum of business processes including master data management, order-to-cash, sales distribution, and supply-chain management. The company is based in Palo Alto and venture funded by The Hive. The Hive is a fund and co-creation studio for AI-powered enterprise applications.

About the Role
The Backend Data Engineer will drive the design and development of key components of the platform including data processing pipeline, feature extraction, and process modeling, intelligent integrations with key business process platforms (like SAP, Salesforce, etc.) with live process rules and business object insertions and process risk/conformance modeling.

Responsibilities
As a Backend Data Engineer of a fast-growing startup, the successful candidate will be leading the development of key aspects of Live Objects’ product:

Assemble large, complex data sets that meet functional / non-functional business requirements.
Perform design, development for new product features.
Understand product vision and business needs to define product requirements and product architectural solutions.
Develop architectural and design principles to improve the performance, capacity, and scalability of the product.
Create and maintain optimal data pipeline architecture
Building reusable code and libraries for future use
Implementation of security and data protection

About You
The successful candidate will have experience in working in innovative projects with fast-paced delivery schedules in startups & large enterprises:

Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of SQL and NoSQL databases.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Strong analytic skills related to working with unstructured datasets.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in user authentication and authorization between multiple systems, servers, and environments
Experience in Integration of multiple data sources and databases into one system
Proficient knowledge in Java, JavaScript application development paradigm
Proficient understanding of code versioning tools, such as Git
Understanding of “session management” in a distributed server environment
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Scala, etc.
10+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics,
Informatics, Information Systems or another quantitative field.
Please send your resumes to jobs@liveobjects.ai",-1,LIVE OBJECTS,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Staff BI and Data Engineer,$105k-$124k (Glassdoor Est.),"Job Description: Staff BI and Data Engineer
Location: San Jose, CA (or) New York, NY
Department: Data Engineering
Hours/Shift: Full Time
Reports To: VP, Data Engineering
Job Description:
Affinity Solutions is looking for a hands-on and self-driven Staff BI and Data Engineer, preferably with experience in the bank card loyalty/fin-tech/advertising/marketing space, to enhance and automate its data and analytics infrastructure to support its growing customer base and expanding partner ecosystem. The position is based in San Jose, CA., and reports into the Data Engineering division. The demand for advanced analytical solutions continues to grow exponentially, and this is your opportunity to grow with us in a fast-paced, collaborative environment.
The ideal candidate is a passionate and highly skilled individual, who can utilize programming and analytics tools such as SQL, Python, and Tableau to query and process large data sets to produce high quality customer facing data deliverables and insights. If you have experience designing and building business intelligence/analytics applications, especially around credit/debit card transactions, unified consumer behavioral and profile data, and excited about leveraging your experience to catapult a venture-backed company into hyper-growth, this job is for you.
Responsibilities:
Develop high quality analytical data assets with an eye towards process efficiency and automation through scripting. Experience in building data marts is a plus.
Build automated QA process to validate the quality of the data and report on data quality
Communicate and present data to both internal and external customers by developing reports/dashboards/charts using BI tools such as Tableau
Work closely with a dynamic and growing team of account managers, data engineers and data scientists to perform quantitative analysis of customer data, including gathering data requirements and validate data, applying judgement and statistical analysis to assist with planning and decision making.
Other responsibilities include but not limited to - data validation, troubleshooting issues, and process documentation.
Qualifications:
Bachelor’s or Master’s degree in Computer Science or related field such as Mathematics and Statistics, preferably with focus on Data Analytics.
At least 3 years of hands-on experience in designing and building data pipelines, analytical data applications and BI Reporting.
Proficient in SQL and Tableau, familiar with at least one coding language in Python/Shell scripting.
Experience in using Cloud based managed services and Big Data Environments for data warehousing/analytics is a big plus – e.g. Amazon RedShift, Google BigQuery, Spark, MapR etc.,
Very strong written and verbal communication skills; Ability to tell a story with the data
Analytical thinker, with an ability to evaluate multiple products/technologies to address various aspects of a big data platform.
Experience working on UNIX / Linux development and production environments
Experience working in Agile software development environments
Strong organization skills with attention to detail is a must.
Ability to manage multiple conflicting priorities, take proactive ownership of problems and outcomes, think outside the box
Knowledge of Retail and Financial verticals is useful but not required.",3.2,"Affinity Solutions
3.2","San Jose, CA",-1,51 to 200 Employees,1998,Company - Private,Advertising & Marketing,Business Services,Unknown / Non-Applicable,-1
Senior Data Engineer - Java,-1,"We are looking for a data-driven problem solver who can automatically discover and extract information from multiple data sources. The senior data engineer will work to recognize patterns and productize your solutions and techniques for integration into our products and our processes. You will have a high degree of authority and autonomy, be capable of managing multiple high-priority tasks in a timely manner, and collaborate across the business. You'll get to build things from scratch! A successful senior data extraction engineer at Plum needs to be innovative, detail-oriented, team-focused, strategic and effective in new and changing environments with a high degree of confidence using Java. One who can dream up and see a project through to conclusion. We are looking for the candidate to start immediately. Data munging with emphasis on ability to deal with imperfections in data. Develop refine and scale data management and analytics procedures, systems, workflows, best practices and other issues. Development of data-driven products. Visualize and communicate data clearly for use both internally and externally. Collaborate with our engineers to produce excellent products for Plum clients as well as streamline internal processes. REQUIRED QUALIFICATIONS Data cleansing, curation, parsing, integration, semantic mapping, and editing. High level language proficiency in Java, prefer also Scala. An entrepreneurial spirit as well as passion for solving difficult challenges through innovation and creativity, with a strong focus on results. Bachelor’s degree in Mathematics, Statistics, Engineering, Computer Science or related discipline. 4+ years-experience in developing large scale distributed software systems. Experience with scraping social media, using social media APIs. Strong knowledge of Linux / UNIX systems. Conscientious and well organized. Eager to produce results and drive forward progress while managing deadlines. Legal authorization to work in the U.S. is required. PREFERRED QUALIFICATIONS Knowledge of machine learning concepts and NLP techniques. Experience in working with big data technologies Spark, MapReduce, NoSQL. Experience in AWS infrastructure. Understanding of statistics. Experience with finance technology/analytics and real estate. BENEFITS Early equity in a startup that is revolutionizing commercial real estate lending. Competitive package of base pay and stock options. Generous health, dental and vision coverage for employees and family members, along with commuter pre-tax program. Unlimited vacation policy. Opportunity to make a meaningful impact on the disruption of an industry and to shape the building of a company and culture. Chance for your direct input to be realized and put into action. Freedom to stretch the boundaries of your past work experience, learn skills outside of your immediate job description and grow your career. Autonomy, flexibility and a flat corporate structure. ABOUT PLUM After decades of experience in institutional financial services, the founders of Plum recognized the need to challenge the status quo which makes the commercial real estate (CRE) lending process opaque and cumbersome. Plum is a more agile and advisory financial technology company that combines data intelligence with best-in-class financial expertise to modernize CRE lending. Our CEO, Bill Fisher, has decades of experience building successful startup businesses, including GetSmart.com, Xing and Trivago. Our team includes senior leaders and talent from AIG, Goldman Sachs, KKR, McKinsey, PWC, Bank of America, Meridian Capital, A10, JP Morgan, Freddie Mac, US Bank, Wells Fargo and PNC Real Estate. In April 2018, Plum announced a Series B equity investment by the $35 billion hedge fund, Elliott Management. Plum’s Series A round in August 2015 was led by Renren Inc, who has backed other fintech companies including SoFi, LendingHome and Motif Investing. This followed an earlier seed investment by QED Investors, a pre-eminent VC firm led by the founders of Capital One, whose portfolio includes Prosper, Orchard and ApplePie Capital. Plum is headquartered in the heart of San Francisco’s Financial District in an airy, industrial loft, close to all forms of public transportation. *Women and minorities are encouraged to apply *No Relocation",4.1,"Plum Lending
4.1","San Francisco, CA",-1,1 to 50 Employees,2014,Company - Private,Lending,Finance,Unknown / Non-Applicable,-1
"Data Engineer, Mission Control",$106k-$175k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Mission Control is the organization that plans and shapes Facebooks evolving workforce. The Analytics team within Mission Control supports the TECH organization by building and scaling solutions that support our workforce planning and programs. We work with stakeholders from across the company to manage headcount demand plans, tell the story of headcount progression and associated components, build resources to manage the flow of talent, and provide data driven insights into the health and development of our global programs. On our team, youll see a direct link between your work, company growth, and our ability to transform over time.
Were looking for a Data Engineer to build and maintain our data pipelines, create custom tables, develop automation, support tooling integrations, and facilitate data migrations to up-level our analyst productivity and efficiency. This role will primarily be supporting our Data Architect, Analysts, and broader Mission Control team.
Build deep HR data expertise and own data quality for allocated areas of ownership
Interface with Data Architect, Analysts and Program Managers to understand data needs
Architect, build, launch and maintain Mission Control specific data pipelines, models and tables to provide intuitive analytics
Triage and investigate data quality and pipeline errors
Use your expert coding skills across a number of languages including Python and Java or PHP
Design, develop and provide technical support for tooling integrations
Build framework for auditing, error logging and master data management for your pipelines
Support on-call shift as needed to support the team
Collaborate with team members to maintain and enhance current data and processing infrastructure
3+ years of Python development experience
3+ years of SQL experience
3+ years of experience in custom ETL design, implementation and maintenance
3+ years of experience in the data warehouse space
3+ years of experience with schema design and dimensional data modeling
Experience analyzing data to identify deliverables, gaps and inconsistencies
Experience leading data driven discussions
Experience with more than one coding language
Experience with designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and E2E process optimization
Experience with anomaly/outlier detection
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc
BS in CE/EE/CSE or computational sciences
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Staff Data Engineer,$69k-$126k (Glassdoor Est.),"Get your career started at eHealth eHealthInsurance has many exciting career opportunities in a number of locations, across various functions. Come join us today! About the team: At eHealth, we are passionate about solving our nation's toughest problems to bring more suitable, accessible, and affordable health insurance to Americans. We are seeking a talented data engineer to join our growing Data Engineering team. Data Engineer team at eHealth work closely with other members in the Data Team (BI, Data Scientists, Data Analysts) as well as other teams within eHealth (Finance, Marketing, Platform, Engineering). About the role: As a Data Engineer, you will help us develop cutting-edge data tools and pipelines to drive better and faster decision making within our company and to better serve our customers. You will build data pipelines and data models to deliver insightful analytics while ensuring the highest standard in data integrity. This is a partnership-heavy role. You will work closely with various business functions to understand eHealth’s analytic, data science, and reporting needs and to develop data products that address those needs. How you will make an impact: Design, develop, and operate highly scalable, high-performance and low-cost data pipelines in distributed data processing platforms with AWS/cloud technologies Implement various ETL infrastructures and guidelines on how to most effectively build and maintain them for reporting, analytics and product features Collaborate with Engineers and Data Scientists in the organization to construct complex data sources for algorithms and machine learning models Collaborate with business analysts to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation Contribute to data platform architecture and represent data team in various cross – team technical initiative Minimum Qualifications: Bachelors in Computer Science, Engineering, or a related quantitative field 5+ years of experience in a Data Engineering role, with a focus on data warehouse technologies, data pipelines, data modeling, batch and streaming data architectures development, and ETL/ ELT development. 3+ years of hands on professional experience with AWS based solutions such as EMR, S3, RDS, Lambda, Dynamodb, Redshift, EC2. 3+ years of professional experience working with streaming data applications and utilizing Kafka, Spark, Zookeeper and Spark Streaming for real-time data processing. 3+ years of professional experience working with SQL that demonstrates proven ability to write complex SQL beyond just data access 3+ years of professional experience working Python for data manipulation and automation Preferred Additional Skill Sets: Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations. Familiarity with workflow management tools (Airflow). Working experience with large healthcare related datasets, including EHRs, medical claims data, and health population surveys. Experience in building healthcare data pipelines would be a big plus. Knowledge of healthcare insurance industry, products, systems, business strategies, and products. Experience working with call center operations. Excellent communication skills in written and verbal forms, and an ability to communicate complex issues to a range of audience (management, peers, clients). eHealth is an Equal Employment Opportunity employer. It is our policy to provide equal opportunity to all employees and applicants and to prohibit any discrimination because of race, color, religion, sex, national origin, age, marital status, sexual orientation, genetic information, disability, protected veteran status, or any other consideration made unlawful by applicable federal, state or local laws. The foundation of these policies is our commitment to treat everyone fairly and equally and to have a bias-free work environment. If you are interested in applying for employment with eHealth and need special assistance or an accommodation to apply for a posted position contact us at: accommodations@ehealthinsurance.com .",3.7,"eHealth
3.7","Santa Clara, CA",-1,1001 to 5000 Employees,1997,Company - Public,Insurance Agencies & Brokerages,Insurance,$100 to $500 million (USD),-1
Sr Data Engineer,$157k-$270k (Glassdoor Est.),"Company Overview Fanatics is the global leader in licensed sports merchandise and changing the way fans purchase their favorite team apparel and jerseys. Through an innovative, tech-infused approach to making and selling fan gear in today's on-demand culture, Fanatics operates more than 300 online and offline stores, including the e-commerce business for all major professional sports leagues (NFL, MLB, NBA, NHL, NASCAR, MLS, PGA), major media brands (NBC Sports, CBS Sports, FOX Sports) and more than 200 collegiate and professional team properties, which include several of the biggest global soccer clubs (Manchester United, Real Madrid, Chelsea, Manchester City). Fanatics offers the largest collection of timeless and timely merchandise whether shopping online, on your phone, in stores, in stadiums or on-site at the world's biggest sporting events. About the Team Fanatics is first and foremost a technology company. We are powered by cutting-edge tech created by our small agile teams using the latest tools and technologies under our highly analytical, forward thinking, and open-minded leadership. As the global leader in licensed sports merchandise, we challenge ourselves by improving our new fully responsive NodeJS cloud commerce platform, Elasticsearch engine, and deep data science capabilities while building the best-in-class retail manufacturing and supply chain technologies. Our tech teams work together to revolutionize data science and engineering initiatives, provide highly scalable real-time and streaming platforms, and create secure e-commerce and in-stadium fan experience products. Our own e-commerce platform transacts in over 190 countries, 17 languages, and 14 currencies. Our motto is “#GSD”—get stuff done—and we do just that. If you want to be at the nexus of sports, commerce, and technology, come be a part of our industry-leading team here at Fanatics Tech. Building data pipelines to manage marketing reports and optimize marketing spend. Primary Responsibilities include: •Design and deliver highly scalable multi-tiered distributed software applications. •Coding. Living and Breathing awesome Code. •Be part of an agile team and sharing responsibilities of testing and development as the role demands. •Strong quality focus including automation, design reviews, and unit testing. •Strong analytical, troubleshooting, and problem solving skills with orientation towards innovation and open source tools. •Mentor and guide developers in coding, design methodology and design patterns. •Conceptualizing, coding, deploying, and iterating on next generation prototypes. •Flexible approach to analyzing technical issues and clearly communicating recommendations/solutions. •Cross team development with Product managers, Project managers, engineers, and QA to deploy innovative solutions to meet business unit requirements Required Skills •An experienced Data engineer with Spark, Hive and Hadoop technologies •Strong track record of excellence, and of delivering high quality innovative software! •Outstanding coding skills in Java ,Scala and python •Ability to trace the data transformations and tune them appropriately to reduce the time and cost of execution. •Building distributed back-end systems using Java and related technologies is a plus •Exposure to Data Science and experience integrating with science models •Expertise in SQL technologies and RDBMS (Oracle, MySQL) •Experience in noSQL technologies (MongoDB, CouchDB, Redis) is a plus •Strong understanding & usage of algorithms and data structures in your designs •Self-motivated, passionate for technology, and strong driver for results and continual improvement •Team player - work well independently and in multi-group cross-discipline environments •Flexible, adaptable, and able to autonomously manage multiple tasks in a dynamic, fast-paced environment. •Strong skills for verbal & written communication targeting technical and non-technical audiences Tryouts are open at Fanatics! Our team is passionate, talented, unified, and charged with creating the fan experience of tomorrow. The ball is in your court now. NOTICE TO CALIFORNIA RESIDENTS/APPLICANTS: In connection with your application, we collect information that identifies, reasonably relates to or describes you (“Personal Information”). The categories of Personal Information that we collect include your name, government issued identification number(s), email address, mailing address, other contact information, emergency contact information, employment history, educational history, criminal record, and demographic information. We collect and use those categories of Personal Information about you for human resources and other business management purposes, including identifying and evaluating you as a candidate for potential or future employment or future contract positions, recordkeeping in relation to recruiting and hiring, conducting criminal background checks as permitted by law, conducting analytics, and ensuring compliance with applicable legal requirements and Company policies.",3.4,"Fanatics Inc.
3.4","San Mateo, CA",-1,5001 to 10000 Employees,1996,Company - Private,Sporting Goods Stores,Retail,$1 to $2 billion (USD),-1
Sr. Data Engineer,$99k-$176k (Glassdoor Est.),"The Proterra Story Communities are growing and evolving, and with that, our transportation needs are changing. Now more than ever, we need smart solutions that provide safer, more reliable and cleaner transit. Every day, Proterra works to meet those needs, with the world’s best-performing zero-emission buses. Our revolutionary battery-electric buses help fleet operators abandon fossil fuels, improve environmental quality and reduce operating costs. Join the Proterra Revolution. Position Overview Connected Technology Team is building Proterra’s next-gen Telemetry & IOT (Vehicle and Charger data) platform which will enable our customers to optimize asset utilization, access valuable data & analytics via mobile and SaaS solutions, enable Engineering to monitor and analyze in-service data for continued product improvement, and enable Service with predictive and real-time data to support proactive Customer Support. In this role, you will work within the software engineering team to develop, test and maintain databases, data models, APIs, and large-scale processing applications for our connected vehicles platform. This position will also require you to innovate and implement best data governance practices for the platform. About the Role – You will: Have ownership of the data governance, data models, schema design, and databases including tuning, space management, performance management and policy execution Monitor and manage databases across environments, including assisting in the software release process Monitor and manage backend data synchronization applications and external data source retrieval, APIs and applications Expand and optimize our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies. Perform validation procedures to ensure data quality Recommend and implement ways to improve data reliability, efficiency, and quality Conduct systems tests for security, performance, and availability Promote data architecture best practices and standardize on endorsed data storage and message distribution technologies Define security and backup procedures Develop and maintain schema design, data model, API, and troubleshooting documentation Work with the engineering teams to help choose technologies, design system architecture and model data in a scalable and efficient way About Our Group: As strategic partners, the Information Technology team strives to ensure that our values, culture and engagement always allow us to do great things for our company. We strive to provide the highest-level service and support to our clients each and every day. We work collaboratively to build strong relationships with our clients, partners, and own team. We treat each other with respect……and always try to have a little fun every day! You will report to the Manager, Software Engineering and collaborate with other IT team members located at our headquarters in Burlingame, CA and manufacturing facility in Greenville, SC and Los Angeles, CA. About You: Self-starter with the ability to adapt interpersonal styles and techniques to influence at all levels of the organization. Customer-focused attitude, with high level of professionalism and discretion. Ability to maintain strict confidentiality, establish trust and credibility, and act with complete integrity. Detail-oriented, resourceful and diligent. Strong time management and organizational skills. Sound judgment and team problem-solving skills. Excellent communication skills. Your Experience Includes: Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. Thorough understanding of the software development lifecycle and tools used to create data pipelines and perform ETL, with passion to optimize data systems and to build a data architecture from the ground up. Advanced working knowledge in SQL and experience working with relational databases Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores A successful history of manipulating, processing and extracting value from large disconnected datasets Strong experience with object-oriented/object function scripting languages such as Python, Java, etc. (Java is required) Experience with stream-processing systems such as Amazon Kinesis, Spark-Streaming, etc. Ability to use a wide variety of open source technologies and cloud services (experience with AWS, Docker, Kubernetes and Elastic Cloud is required) Experience with AWS cloud services: EC2, ECS/EKS, RDS, SQS, Lambda, Redshift, Glacier, Amazon IoT etc. Significant experience scaling solutions that run on private, public, and hybrid cloud infrastructures Strong background in Linux/Unix Administration Experience with data pipeline workflow management tools (Azkaban, Luigi, Airflow) Bachelor’s degree in Computer Science, Engineering, or relevant field 5+ years’ experience as a Data Engineer or equivalent software-engineering role Able to empathize, sell ideas, and influence others Able to multitask, prioritize, and manage time efficiently Travel: 0 ~ 25% Location: Burlingame, CA Proterra is an Equal Employment Opportunity Employer, providing equal employment opportunities to all Employees and applicants for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity, national origin, disability, age, genetic information, veteran status, pregnancy, childbirth, or related medical conditions, including, but not limited to, lactation or any other characteristic protected by applicable federal, state, or local law or ordinance. Proterra participates in the Electronic Employment Verification Program (E-Verify). Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)",3.5,"Proterra, Inc.
3.5","Burlingame, CA",-1,201 to 500 Employees,2004,Company - Private,Transportation Equipment Manufacturing,Manufacturing,$25 to $50 million (USD),-1
Sr. Data Engineer,$111k-$171k (Glassdoor Est.),"· Bachelor’s degree in Computer Science or related field or 5+ years relevant experience · Expert level skills writing and optimizing complex SQL · Knowledge of data warehousing concepts. · Experience in data mining, profiling, and analysis · Experience with complex data modelling, ETL design, and using large databases in a business environment · Proficiency with Linux command line and systems administration · Experience with languages like Python, Ruby, Java, or similar language · Experience with Big Data technologies such as Hive/Spark. · Proven ability to develop unconventional solutions; Sees opportunities to innovate and leads the way · Excellent verbal and written communication; Proven interpersonal skills and ability to convey key insights from complex analyses in summarized business terms; Ability to effectively communicate with technical teams · Ability to work with shifting deadlines in a fast paced environment The Alexa Echo Device Team is looking for a talented, highly motivated Senior Data Engineer to join our Business Intelligence team. Alexa is the groundbreaking cloud-based intelligent agent that powers Echo and other devices designed around your voice. We provide actionable business insights that inform future products and services that will power the next generation of Echo and Alexa devices. As a Senior Data Engineer, you will work in one of the world's largest and most complex data warehouse environments. You will work closely with Product Management, Software Development, Data Science, and other Data Engineering teams to develop scalable and innovative analytical solutions, process and store terabytes of low latency structured and unstructured data, and enable the Echo Device team to build successful, data driven strategies. You will be responsible for designing and implementing an analytical environment using third-party and in-house tools and using Python, Scala, or Java to automate the ETL, analytics, and data quality platform from the ground up. You will design and implement complex data models, model metadata, build reports and dashboards, and own data presentation and dashboarding tools for the end users of our data products and systems. You will work with leading edge technologies like Redshift, EMR, Hadoop/Hive/Pig, and more. You will write scalable, highly tuned SQL queries running over billions of rows of data and will develop learning and training programs to drive adoption of data driven decision making across the Echo and Alexa organization. You should have deep expertise in the design, creation, management, and business use of large datasets, across a variety of data platforms. You should have excellent business and interpersonal skills to be able to work with business owners to understand data requirements, and to implement efficient and scalable ETL solutions. You should be an authority at crafting, implementing, and operating stable, scalable, low cost solutions to replicate data from production systems into the BI data store. Key Responsibilities · Design, implement, and improve the analytics platform · Implement and simplify self-service data query and analysis capabilities of the BI platform · Develop and improve the current BI architecture, emphasizing data security, data quality and timeliness, scalability, and extensibility · Deploy and use various big data technologies and run pilots to design low latency data architectures at scale · Collaborate with business analysts, data scientists, product managers, software development engineers, and other BI teams to develop, implement, and validate KPIs, statistical analyses, data profiling, prediction, forecasting, clustering, and machine learning algorithms · Partner with other BI and analytics teams to build and verify hypotheses to improve the AWS customer financial experience · Authoritative in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies. · Experience with building data pipelines and applications to stream and process datasets at low latencies. · Demonstrate efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data. · Sound knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines, knows how to optimize the distribution, partitioning, and MPP of high-level data structures. · Knowledge of Engineering and Operational Excellence using standard methodologies.",3.9,"Amazon.com Services LLC
3.9","Sunnyvale, CA",-1,10000+ Employees,1994,Company - Public,Internet,Information Technology,$10+ billion (USD),-1
"Data Engineer- Strong in SQL, Python, ETL- Hadoop",-1,"Please email resume to INFO 'at' ICLOUDNEXUS 'dot' COM*
Data Engineer

San Jose, CA

Long Term

Direct client

Immediate Interviews

We are looking for a Senior Data Engineer with advanced knowledge of SQL and intermediate knowledge of Python.
Nice to have (but not required) beginner or intermediate level java experience.
Your primary focus will be the writing complex SQL queries, optimizing them and development of all server-side
backend data processing logic, ensuring high performance using Python and SQL.

Skills:
 Basics of Computer Science - OOPS, Data Structures and Algorithms.
 Basic understand of regular Linux commands and usage.
 5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries,
function and stored procedures.
 3+ years of experience with hands on experience in writing, debugging Python code on Linux.

Job Types: Full-time, Contract

Work Remotely:
Yes",-1,iCloudNexus,"San Jose, CA",-1,Unknown,-1,Company - Public,-1,-1,Less than $1 million (USD),-1
Lead Big Data Engineer,$119k-$219k (Glassdoor Est.),"Job Title:
Lead Big Data Engineer

Location:
US, California, San Jose
Role Overview:


You will lead a team of Software Engineers to create the next generation of McAfee security products to enhance the auto detection and investigation of security breaches within large organizations. You will work with Architects and other Cloud Infrastructure Engineers to understand the designs, validate the features, and to deliver and integrate solutions in an AWS environment. If you are passionate about building security solutions that keep millions of enterprise customers safe with an outstanding user experience, then this position might be a perfect opportunity for you. You will report to the Sr. Manager, Software Engineering and will be based in San Jose, CA.



Company Overview


From device to cloud, McAfee provides market-leading cybersecurity solutions for both business and consumers. We help businesses orchestrate cyber environments that are truly integrated, where protection, detection, and correction of security threats happen simultaneously. For consumers, McAfee secures your devices against viruses, malware, and other threats, both at home and away. We want to continue to shape the future of cybersecurity by working together to build best in class products and solutions.

About the Role:
Lead a group of Software Developers and oversee the whole solution including uptime by working with architects, development, and cloud infrastructure team.
Help develop our next-generation micro-services to enhance auto detection of different breaches and other security concerns.
Use your knowledge of Java, Go or Python to create new features in the cloud.
Understand and influence logging to support our Data flow.
Work and lead the development and infrastructure teams to open bugs, debug issues and achieve resolution.
Pioneer a new way of thinking about Data Pipelines, Orchestration and Configuration at McAfee.
Mentor other Developers.
About You:
Your background includes 8+ years of hands-on experience with Big Data technology(Kafka, Spark, Airflow).
Successful track record in developing and automating large-scale, high-performance data processing systems (batch and streaming).
Lead data engineering projects to ensure pipelines are reliable, efficient, testable and maintainable.
Experience designing Data Models for optimal storage and retrieval to meet critical product requirements.
Experience with both scripting and system programming languages (Python, Go and Scala).
Experience with microservices including defining and testing APIs.
Company Benefits and Perks:


We work hard to embrace diversity and inclusion and encourage everyone at McAfee to bring their authentic selves to work every day. We offer a variety of social programs, flexible work hours and family-friendly benefits to all of our employees.
Pension and Retirement Plans
Medical, Dental and Vision Coverage
Paid Time Off
Paid Parental Leave
Support for Community Involvement
We're serious about our commitment to diversity which is why McAfee prohibits discrimination based on race, color, religion, gender, national origin, age, disability, veteran status, marital status, pregnancy, gender expression or identity, sexual orientation or any other legally protected status.

Job Type:


Experienced Hire

Primary Location:
US, California, San Jose

Additional Locations:",3.5,"McAfee
3.5","San Jose, CA",-1,5001 to 10000 Employees,1987,Company - Private,Computer Hardware & Software,Information Technology,$10+ billion (USD),-1
Data Engineer (Lead) – Public Content Monetization,$153k-$249k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

In this role, you will be working as part of the Public Content Monetization team whose mission is to bring the best content for our users by enabling good, high-quality content creators (partners) to monetize. You will be leading the effort to build a strong data foundation for understanding the intersections of partner, user, and advertiser experiences in order to inform decisions on Facebook’s partner monetization product offerings - and to ultimately bring more engaging content for our users around the globe. You will have the opportunity to work on some of the most challenging problems that cut across all of our products, create scalable solutions that benefit multiple teams, and collaborate with leadership to set the data vision and strategy for data engineering.

Drive data vision and strategy for an entire team of Data Engineers
Build data road-maps and setting technical direction for others to follow
Work on big projects across multiple product teams, and with a wide range of stakeholders
Design, build and own the optimal data processing architecture and systems for new data and ETL pipelines, dashboards and experimentation metrics
Build data anomaly detection, data quality checks, and optimize pipelines for ideal compute and storage
Collaborate with cross functional partners including data scientists, product managers and software engineers and the Public Content Monetization leadership to design and develop end to end data assets
Recommend improvements and modifications to existing data and ETL pipelines
Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership
Drive internal process improvements and automating manual processes for data quality and SLA management

5+ years experience in SQL and data processing
7+ years experience in data modeling and how to design scalable data architecture
7+ years experience in building out solutions for huge and complex datasets
5+ years experience working with multiple cross-functional teams
5+ years experience building data road-maps and setting technical direction for others to follow

Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.

Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Senior Data Engineer,$114k-$207k (Glassdoor Est.),"Silicon Valley Bank is seeking an experienced Sr. Data Engineer that will report into the Director of Marketing Technology & Analytics to grow the marketing analytics team and deliver on innovation opportunities. You will be critical asset - assisting the leader to deliver 360 view of client data to enable enhanced segmentation to optimize and personalize cross channel customer journeys. You will work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions. These solutions will generate insights from our connected data, enabling SVB to advance the data-driven decision-making capabilities of our marketing organization, globally. This role requires deep understanding of data architecture, data engineering, data analysis, reporting, and a basic understanding of data science techniques and workflows. In this role you will: Develop and sustain industry leading data solutions to drive data-driven marketing and create client centric experiences. Solve complex data problems to deliver insights that helps our business to achieve their goals Advise, consult, mentor and coach other data and analytic professionals on data standards and practices Foster a culture of sharing, re-use, design for scale, stability, and operational efficiency of data and analytical solutions You have a strong understanding and aptitude for external digital and industry trends and can translate such trends into actionable innovation opportunities. Practice Development Contribute to the development of standard methodologies in analysis, design and test to ensure as an analytics team we are providing outstanding quality work Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Lead evaluation, implementation and deployment of emerging tools & process for analytic data engineering to improve our productivity as a team Contribute to the development and education plans on analytic data engineering capabilities, standards, and processes Design & Development Develop and maintain data pipelines for efficient data ingestion/collection, architect and implement data processing and storage solutions as well as enable data consumption through a variety of applications and tools Collaborate with marketing teams and other SVB teams and understand the systems and data points applicable to each group, and to advise on areas for opportunity and collaboration in ingesting and tracking data Undertake preprocessing of structured and unstructured data Joins, normalizes, aggregates, and transforms data from raw to consumable form Manage existing databases, ensuring data integrity and data usability Collect, analyze and document data wrangling and analytics requirements Contribute to the development of SVB Marketing’s data strategy and roadmap, and develop technical solutions to enrich the client data Define data security and controls requirements Measurement/Optimization Provides input to define IT data requirements to support key business metrics and KPIs Assess the effectiveness and accuracy of new data sources and data gathering techniques Collaborate with other performance marketing experts to define success metrics to measure effectiveness of new analytical models Qualifications Bachelor’s degree in Computer Science, MIS, or Engineering 8+ years of experience working in data engineering or architecture role 3+ years of experience working in marketing organizations 3+ years in B2B marketing / financial services Expertise in SQL and data analysis and experience with at least one programming language Experience developing and maintaining data warehouses in big data solutions Experience working with BI tools such as Tableau Experience working on a collaborative agile product team Strong agile/scrum delivery skillet preferred Leadership in, and understanding of, Agile concepts, processes, practices, procedures and tools Passion for agile software processes, data-driven development, reliability, and experimentation Strong analytical and problem-solving skills; ability to combine attention to detail with big picture perspective Demonstrates excellent oral and written communication skills; ability to communicate with all levels of the organization, including executive management, functional experts, and IT team members",3.6,"Silicon Valley Bank
3.6","Santa Clara, CA",-1,1001 to 5000 Employees,1983,Company - Public,Banks & Credit Unions,Finance,$1 to $2 billion (USD),-1
Sr Data Engineer,$126k-$228k (Glassdoor Est.),"Come work at a place where innovation and teamwork come together to support the most exciting missions in the world!

Position Summary

About Shape Security: We are security and web experts, pioneers, evangelists, and best-in-class researchers. We believe in the power of the Internet to be a positive force; our mission is to protect every website and mobile app from cyber criminals. Shape’s founders fought cybercrime at the Pentagon, Google, and other leading security companies. Shape provides industry-leading web and mobile security services designed to protect organizations against automated attacks that evade traditional security defenses. As we now join F5 Networks, we need highly-motivated and passionate individuals to join us.

We are looking for a Big Data Engineer that will work on the storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be integrating them with the architecture used across the company!

Responsibilities
Selecting and integrating big data frameworks required to provide requested capabilities
Build analysis pipelines as well as visualization dashboards
Monitoring performance and iterate the solution fast
Skills
Experience with Cloud-based service and development environment, such as AWS or GCP
Proficiency with programming languages such as Python and Java
Proficient understanding of distributed computing principles
Good knowledge of Big Data querying databases, such as BigQuery, BigTable and MongoDB
Upbeat, positive teammate.
Qualifications
Bachelors degree in Computer science or similar relevant field with 8+years of experience; Master’s degree with 6+ years; or equivalent experience.

Shape and F5 Networks are equal opportunity employers and we embrace diversity

The Job Description is intended to be a general representation of the responsibilities and requirements of the job. However, the description may not be all-inclusive, and responsibilities and requirements are subject to change.

Please note that F5 only contacts candidates through F5 email address (ending with @f5.com) or auto email notification from Yello/Workday (ending with f5.com or @myworkday.com ) .

Equal Employment Opportunity

It is the policy of F5 to provide equal employment opportunities to all employees and employment applicants without regard to unlawful considerations of race, religion, color, national origin, sex, sexual orientation, gender identity or expression, age, sensory, physical, or mental disability, marital status, veteran or military status, genetic information, or any other classification protected by applicable local, state, or federal laws. This policy applies to all aspects of employment, including, but not limited to, hiring, job assignment, compensation, promotion, benefits, training, discipline, and termination. Reasonable accommodation is available for qualified individuals with disabilities, upon request.",3.9,"f5
3.9","Santa Clara, CA",-1,5001 to 10000 Employees,1996,Company - Public,Computer Hardware & Software,Information Technology,$1 to $2 billion (USD),-1
Senior Data Engineer,$136k-$246k (Glassdoor Est.),"At NVIDIA, our employees are passionate about parallel and GPU computing. We’re united in our quest to transform the way graphics are used for work and play. Are you passionate about creating ground-breaking software for creative and innovative customer experience? Do you thrive in an extremely high performance environment? If so, then we want you to join our Digital Marketing team. The rewards are sweet and include collaborating with some of the smartest people in the industry, an aggressive compensation plan that rewards top performers, and the opportunity to work on products that transform the way people work and play. As a Senior Data Engineer, you will be a member of NVIDIA’s Digital Marketing platform innovation team. You'll be architecting data pipelines, responsible for end-to-end development, and building applications at scale. Using Agile methodologies to design and develop back end using our software stack to improve customer experiences for both our consumer (GeForce.com) and enterprise businesses (NVIDIA.com). If you have a passion for innovation and driving amazing experiences based on integrating / handling large data sets, we are looking for you. You are comfortable working on complex data platforms, developing core marketing and ecommerce tech stack and the core data management platform. Ability to work under a minimal amount of supervision and act as a problem solver and be a standout colleague who knows how to work, think and learn as a team. What you’ll be doing: Architect solutions for complex data platforms, and large scale CI/CD data pipelines utilizing a variety of technologies (REST APIs, Advanced SQL, Amazon S3, Apache Kafka, Data-Lakes, etc.), relational databases (MySQL), and data warehouse solutions (RedShift) Responsible for end-to-end development, starting from requirements gathering with business and engineering partners to deployment to product systems using Agile development methodology Work with analytics, data science and wider engineering teams across NVIDIA to help with automating data analysis and visualization needs, advise on transformation processes to populate data models, and explore ways to design and develop data infrastructure Develop intelligent, insightful self-reporting tools operating on terabytes of structured and non-structured data, and ensure high accuracy in working with the data What we need to see: Bachelors Degree in Computer Science or related field or equivalent work experience 8+ years as data engineer / backend software engineer, or related experience, and experience developing in variety of object oriented (specifically Java) and scripting languages to integrate different data systems Advanced working SQL knowledge and experience working with relational / non-relational databases, schema design, and excellent SQL troubleshooting skills working with large datasets Extensive experience developing for high/elastic scalable, 24x7x365 high availability digital marketing or ecommerce systems Strong background on ETL development, data modeling, metadata management, and data quality, data retention, and data cleansing with exposure to GDPR/CCPA compliance requirements Recommend ways to improve data reliability, efficiency and quality for the whole data platform Experience with implementing CI/CD processes using tools such as Maven, GIT, Jenkins, and with monitoring and alerting for production systems Worked on AWS services - DMS Jobs, Lambda, S3, Redshift, SNS (or Apache Kafka), SQS, Redis, Data-Lakes, and AWS cloud deployment models Strong analytical, problem solving and interpersonal skills, have a hunger to learn, and the ability to operate in a fast-paced rapidly changing environment Ways to stand out from the crowd: Architect level experience as a data engineer developing and deploying using Docker and Kubernetes on cloud technologies Experience with integrating click-stream (e.g. Adobe Analytics, Google Analytics) and batch-mode web-based data-telemetry for collection, persistence, visualization, and campaign based targeting Experience on working with technologies such as Kibana/Elastic, Spark 2.x, Tableau, MuleSoft Exposure to ML “Machine Learning” concepts With competitive salaries and a generous benefits package, we are widely considered to be one of the technology world’s most desirable employers. We have some of the most forward-thinking people in the world working for us and, due to unprecedented growth, our business development teams are rapidly growing. If you're creative and autonomous with a real passion for you work, we want to hear from you. NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression , sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.",4.7,"NVIDIA
4.7","Santa Clara, CA",-1,10000+ Employees,1993,Company - Public,Computer Hardware & Software,Information Technology,$5 to $10 billion (USD),-1
"Data Engineer, Analytics (Family Ecosystems)",$118k-$192k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Our more experienced data engineers are clearly characterized by in-depth technical experience, subject matter expertise and proven progression in leadership responsibility. If you have an interest in owning important and critical problem areas and influencing by building robust company-wide data foundation and tooling, this is the right role for you. You will get to impact the End-to-End (E2E) suite of big-data tools and products that play a critical part in the day-to-day development lifecycle of Data Engineers, Data Scientists, ML Engineers, Research Scientists & Software Engineers.In this role, you will work closely with Data Infrastructure/Product Software Engineering and Product Management teams to foundationally evolve long-term, architecture-driven, E2E analytics development cycle and the Data Products, Platforms, Tools and Infrastructure stacks that underlie such as Logging, Streaming, Batch/Compute engines (Presto, Spark), Language/APIs, Semantic Data and Metadata models, ML workflows/models, Consumption workflows (Visualization/Notebooks), Data Discovery and so on. You will define and find solutions to complex and often ambiguous problems as a Subject Matter Expert. You will be leveraging your deep knowledge and experience to collaboratively define technical vision, strategy and architecture in three key areas Semantic Data and Metadata modeling, Large-scale analytics architecture (covering Logging, ETL and Consumption stacks) and Big Data development lifecycle (coding, testing, deploying, discovery etc.). A few examples of the impact and influence of your work: Consistent E2E Data Model and Definition-driven metrics such as Message Sends across the Family of Apps, Data model and metadata-driven, foundational, company-wide Analyics APIs such as User Retention, Evolving Dataframe APIs, Data Models and company-wide lifecycle development from Logging through Consumption through critical company-wide analytics use cases and Enabling consumption and adhoc exploratory workflows for Data Scientists by helping envision and implement large-scale analytics architecture use cases.
Craft and own the optimal data processing architecture and systems for new data and ETL pipelines/analytics applications
Build and data (dimensional) model core datasets and analytics applications and make them scalable and fault-tolerant
Drive comprehensive Technical Vision on fundamental aspects and evolution of Analytics/Data Infra Foundation/Tooling
Define and disseminate technical or product strategy clearly for effective outcomes
Articulate strategy within teams, effectively communicate with cross-functional
articulate solutions and influence leadership
Collaborate and work with different cross-functional partners - Data Infrastructure, Product Software Engineering, Data Engineering and Product Management teams - on use cases sto foundationally evolve long-term, architecture-driven, E2E analytics development cycle
Technically influence within the function and cross-functional community
Build visualizations to provide insights into the data & metrics
Immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions
Drive internal process improvements and automating manual processes
Provide ongoing proactive communication and collaboration throughout the organization
4+ years experience in the data warehouse space
4+ years experience working with either a MapReduce or an MPP system
7+ years experience in writing complex SQL, Dataframe APIs and ETL processes
4+ years experience with object-oriented programming languages
7+ years experience with schema design and dimensional data modeling
BS/BA in Technical Field, Computer Science or Mathematics
Knowledge in Python or Java or Scala or Pandas
Experience analyzing data to identify deliverables, gaps, and inconsistencies
Experience mentoring team members in their careers
Experience collaborating, defining and communicating complex technical concepts to a broad variety of audiences ariety of audiences
Experience scaling analytics architecture and worked with open source big-data stacks (Spark, Koalas etc.)
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
"Staff Data Engineer, eComm- Visualization (Tableau SME)",$115k-$205k (Glassdoor Est.),"Position Summary... What you'll do... As a Staff Data Engineer, Visualization, you will help the World’s largest omni-channel retailer structure, collect, organize and display robust data sets that will be used to manage and optimize the business. The Staff Data Engineer will work closely with database architects, channel and category BI analysts, the campaign measurement team, and data scientists on data initiatives and will ensure data is consistent and accurate at scale. Responsibilities: •Maintain robust visualizations to support Marketing Vehicle performance, Category performance, Customer cohort activity, and Financial Plan/Forecast reporting. •Define the BI Architecture for dynamic and real time updates of dashboards. •Provide data architecture that is flexible, scalable, and consistent for cross-functional use, and aligned to stakeholder business requirements. •Deliver ongoing and accurate reporting of KPIs for Weekly Business Review reporting. •Define optimized DB schemas that power the snappy dashboards. •Publish KPI definitions and ensure they are consistent with Marketing Analytics, Marketing Finance, and Retail Operations expectations. •Work with BI and data science team members to assist them in accessing and leveraging key data sets. •Provide analytics tools and training to drive actionable insights into key business performance metrics, including campaign and category performance, marketing vehicle performance, customer funnel metrics, and operational efficiency. •Define SLA and acceptable time lags by data source, define QA process, and socialize resolution process to ensure data accuracy and consistency. Minimum Qualifications... Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1: Bachelor’s degree in Computer Science and 4 years' experience in software engineering or related field. Option 2: 6 years’ experience in software engineering or related field. Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field. 3 years' experience in data engineering, database engineering, business intelligence, or business analytics. Preferred Qualifications... Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 4 years' experience in software engineering",3.3,"Walmart
3.3","San Bruno, CA",-1,1001 to 5000 Employees,1962,Company - Public,"Department, Clothing, & Shoe Stores",Retail,$10+ billion (USD),-1
BIG Data Engineer,$96k-$180k (Glassdoor Est.),"ARK Solutions, Inc is looking for Data Engineer for one of our clients in Sunnyvale, CA.

Position: BIG Data Engineer

Location: Sunnyvale, CA - Initial Remote

Skills: Data Engineer

6 + Professional experience with Big Data systems, pipelines and data processing
Hands on experience Big Data, data ingestion, data processing using Spark, Spark Streaming, Flink, HIVE, Kafka, Hadoop, HDFS, S3
Hands-on experience with design and development with NoSQL technologies Cassandra, HBase or similar scalable Key valueStores and time series data stores like Druid, influx or similar
Apache Parquet and common methods in data transformation
Confirmed understanding of design and development of large scale, high throughput and low latency applications is a plus
Understanding and experience with Micro Services is desired
Excellent problem solving and programming skills
Experience with containerization technologies like Kubernetes, Docker, Mesos, Marathon is desirable
Experience with CI/CD, debugging and monitoring applications and big data jobs is desirable",3.9,"ARK Solutions
3.9","Sunnyvale, CA",-1,201 to 500 Employees,2003,Company - Private,IT Services,Information Technology,$25 to $50 million (USD),-1
Sr. Data Engineer (Remote),$138k-$252k (Glassdoor Est.),"At CrowdStrike we’re on a mission - to stop breaches. Our groundbreaking technology, services delivery, and intelligence gathering together with our innovations in machine learning and behavioral-based detection, allow our customers to not only defend themselves, but do so in a future-proof manner. We’ve earned numerous honors and top rankings for our technology, organization and people – clearly confirming our industry leadership and our special culture driving it. We also offer flexible work arrangements to help our people manage their personal and professional lives in a way that works for them. So if you’re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to stop breaches and protect people globally, let’s talk.

About the Team:
The data engineering team is on a mission to create a hyper scale data lake, which helps finding bad actors and stop breaches. The team builds and operates systems to centralize all of the data the falcon platform collects, making it easy for internal and external customers to transform and access the data for analytics, machine learning, and threat hunting. As an engineer on the team you will contribute to the full spectrum of our systems, from foundational processing and data storage, through scalable pipelines, to frameworks, tools and applications that make that data available to other teams and systems.

Job Responsibilities :

Design, develop, and maintain a data platform that processes petabytes of data

Participate in technical reviews of our products and help us develop new features and enhance stability

Continually help us improve the efficiency of our services so that we can delight our customers

Help us research and implement new ways for both internal stakeholders as well as customers to query their data efficiently and extract results in the format they desire

Qualifications for Data Engineer :

We are looking for a candidate with a BS and 5+ years or MS and 3+ years in Computer Science or related field. They should also have experience with the following software/tools -

A solid understanding of algorithms, distributed systems design and the software development lifecycle

Solid background in Java/Scala and a scripting language like Python

Experience building large scale data pipelines

Strong familiarity with the Apache Hadoop ecosystem including : Spark, Kafka, Hive, Apache Presto, etc.

Experience with relational SQL and NoSQL databases, including Postgres/MySQL, Cassandra, DynamoDB

Good test driven development discipline

Reasonable proficiency with Linux administration tools

Proven ability to work effectively with remote teams

Experience with the following tools is desirable :

Go

Kubernetes

Jenkins

Parquet

Protocol Buffers/GRPC

#LI-JF1
#LI-Remote
#Stack

Benefits of Working at CrowdStrike:
Market leader in compensation and equity awards
Competitive vacation policy
Comprehensive health benefits + 401k plan
Paid parental leave, including adoption
Flexible work environment
Wellness programs
Stocked fridges, coffee, soda, and lots of treats

We are committed to building an inclusive culture of belonging that not only embraces the diversity of our people but also reflects the diversity of the communities in which we work and the customers we serve. We know that the happiest and highest performing teams include people with diverse perspectives and ways of solving problems so we strive to attract and retain talent from all backgrounds and create workplaces where everyone feels empowered to bring their full, authentic selves to work.
CrowdStrike is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.

CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",3.9,"CrowdStrike
3.9","Sunnyvale, CA",-1,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$100 to $500 million (USD),-1
VDC Data Engineer,-1,"VDC 4D/5D Data Engineer REDWOOD CITY, CA / OPERATIONS & ENGINEERING / FULL TIME Problem In the $12T construction industry, 98% of projects are delivered, on average, 80% over budget. Doxel is on a mission to stop that from happening. Root cause? Inability to measure site progress in an objective, trustworthy and frequent manner. Construction managers are frequently surprised to discover that their multi-million dollar projects have been running behind for several weeks or months. By that point, the money has already been spent and it s too late to fix the problem. Solution Doxel uses robots to survey sites every day, proprietary deep learning and AI based algorithms to assess progress and turns the terabytes of data it collects into simple insights for project managers that enable them to react to issues in minutes, not months. This enables them to constantly correct site inefficiencies and has demonstrated ROI by eliminating overages and even delivering projects up to 11% below budget. Doxel has taken the market by storm since its launch last year - closing deals with marquee customers such as Kaiser Permanente, Sutter Health, and the Lucas Museum of Narrative Art. Team Doxel s team consists of some of the brightest minds in Silicon Valley that include PhDs, engineers, business leaders and civil engineering professionals on the Forbes 30 Under 30 list, graduates from Stanford University and with experience at organizations such as Google Advanced Technologies & Projects. Doxel is backed by Andreessen Horowitz - famed investor that also backed Facebook, Coinbase, Slack, Airbnb, Github and Lyft. The Role Doxel has an immediate need for a Virtual Design and Construction (VDC) professional with expertise in detailing software (such as Revit and AutoCAD MEP), Navisworks model management platform, and Project Schedule analysis. Your mission for this role is to organize 3D design models with detailed tracking sets and map this organization back to schedule tasks for project schedule tracking efforts. The secondary role is to work with the Doxel Product and Engineering groups to develop utilities for improving model organization efficiencies and tracking tools. This will involve: Analyze Project Schedules Analyze Cost Data in Work Breakdown Structures (WBS) Organize 3D Models with Schedule and Cost Data Work Closely with Product and Engineering Teams for Project Deliverables As a BIM Data Engineer at Doxel, you will have a clear understanding of the Project Schedules and Design Models used for the Doxel analytics engine. This includes familiarization with design detailing software and scheduling platforms used in the construction industry. You will maintain constant communication with the Doxel BIM Manager, Product Manager, and other Doxel Team members. You will support all aspects of the Doxel data pipeline and achieve high levels of accuracy in data organization. Working at Doxel keeps you at the leading edge of the latest 3D Scanning Technology while working with some of the brightest minds in this new industry. Our established team of Data Engineers look forward to adding an enthusiastic mind with a proactive attitude into their world of achieving a never before realized level of construction efficiency. If you believe you are ready to step up to the forefront of AI Technology while keeping Doxel ahead in our industry, then we want you on the Doxel team! Responsibilities 3D Model Data Organization is the primary responsibility for a BIM Data Engineer, which involves organizing models into highly detailed search sets based on schedule and cost data. Secondary Responsibilities Schedule Analysis – Review and organize schedule data from various schedule software platforms such as MS Project, Primavera P6, and others. Cost Analysis - Review and organize cost data from project Work Breakdown Structures. Product Development – Work with the Product and Engineering team to develop new modeling and data analysis utilities to improve operational workflows. Communication – Conduct ongoing communication throughout the workday with project team members. Doxel utilizes various tools for communication with Slack as the primary messaging platform. Jira tickets and tracking are also highly used. Direct Phone Calls and Conference Calls (Zoom as an option) may also be used. Training – Strive to continuously develop your skills in core applications used for model organization and data analysis. Qualifications Clear and precise verbal and written communication skills. Able to focus on completing primary objectives and any secondary objectives while remaining flexible and able to use good judgement to respond to changing field conditions. Able to safely lift and carry up to 40lbs of equipment throughout the workday. Preferred Qualifications BS or MS or foreign equivalent in Construction Digital Design Technology from reputed US Universities preferred (or) 1-2 years of experience in managing BIM detailing work from Pre-construction through Construction phases of commercial construction Proficiency in BIM Design and Analysis tools like AutoCAD MEP, Revit MEP and Navisworks Proficiency with Scheduling Software such as MS Project and Primavera P6 Knowledge of construction industry Proficiency with Microsoft Excel or Google Drive (Sheets) Familiarity with Slack or similar team communication and productivity platforms Familiarity with Jira or similar team issue & project tracking platforms Familiarity with Cloud Computing platforms such as AWS or GCP Excellent collaborative and organizational skills with the ability to communicate effectively Why Doxel? We work on deeply technical, groundbreaking engineering concepts. We like to think we are more competitive and compassionate than most others – Muhammad Ali, Ayrton Senna, and Vince Lombardi are our idols. We are fiercely competitive and unshakably compassionate. Intellectual honesty and radical transparency are core tenets at our company. We find that the trust developed in such culture far outweighs all else. Doxel is an equal opportunity employer and actively seeks diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Employment Specific Information Geographic Location Living in the continental United States, preferably on the West Coast to sync with the Doxel HQ time zone. Doxel HQ resides in the Bay Area of Northern California, but local residence is not required for employment. Equipment All necessary computing equipment and modeling software will be provided by Doxel. Salary Full time Doxel employee at $75k/yr with benefit options. Employment Requirements Must take and pass a background check and drug screening. Must sign an NDA stating employee will not divulge proprietary information and trade secrets to outside entities.",4.5,"Doxel
4.5","Redwood City, CA",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Hiring Data Engineer/ Full-Stack Developers,$68k-$126k (Glassdoor Est.),"Data Engineers/ Full-Stack Developers*
2+ years*
Master’s degree in Computer Science or a related field, *
Job Location - San Jose, USA*
Design and develop computer applications software, which includes assembling large, complex data sets that meet business requirements for artificial intelligence software solutions.
Duties will include: *
designing and customizing software with the aim of optimizing operational efficiency, which includes building analytics tools to optimize key business performance metrics;
analyze user needs, which includes building the infrastructure required for optimal extraction, transformation, and loading of data using SQL and Cloud Computing ‘big data’ technologies;
identifying, designing, and implementing internal process improvements, such as re-designing infrastructure for greater scalability;
creating optimal data pipeline architecture; creating data tools for analytics and data scientist team members that assist them in building and optimizing our product;
keeping our data separated and secure across national boundaries through multiple data centers and Cloud Computing regions; working with stakeholders to strive for greater functionality in our data systems;
and utilizing knowledge of Optical Character Recognition, number corrections, object detection and counting, neural networks, data and statistical analysis, and machine learning methods to perform the duties described above.
REQUIREMENTS: *
Master’s degree in Computer Science or a related field,
two (2) years of relevant experience and should be based in the U.S.
This experience should include two (2) years of experience with designing and developing computer software applications using Object-oriented design and programming;
Python;
algorithm design and development;
neural network frameworks;
Linux Operating System; data engineering; statistical data analysis;
Machine Learning, including Bayesian modeling, multivariate and logistic regression, support vector machines, cluster analysis, decision and regression trees, random forest, neural networks and ensemble methods;
and analyzing large, complex, multi-dimensional data sets and developing analytic solutions.
Job Type: Full-time

Salary: $118,575.00 - $188,047.00 per year

Schedule:
8 hour shift
Day shift
Monday to Friday
This Company Describes Its Culture as:
Innovative -- innovative and risk-taking
Outcome-oriented -- results-focused with strong performance culture
Stable -- traditional, stable, strong processes
People-oriented -- supportive and fairness-focused
Team-oriented -- cooperative and collaborative
Work Remotely:
Temporarily due to COVID-19",3.8,"Scry Analytics
3.8","San Jose, CA",-1,51 to 200 Employees,2014,Company - Private,Computer Hardware & Software,Information Technology,Less than $1 million (USD),-1
"Data Engineer, Partnerships Central Systems, Data and Tools",$115k-$187k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Facebook is looking for exceptionally talented and experienced data engineers to join the Partnerships Central Systems, Data and Tools team. The Partnerships team at Facebook works with leading content creators, publishers, and businesses in entertainment, sports, news, and many other verticals. This role is a unique opportunity to work with one of the most important datasets in the world to create analytics tools and systems that enable field organizations, analysts and clients alike. You will work with some of the brightest minds in the industry, and get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match. The ideal candidate will have a passion for problem solving and a belief in the power of incremental change in a fast-paced environment.
Develop methods to unlock access to data for stakeholders across the Partnerships Organization
Design, build and maintain efficient & reliable data pipelines to move and transform data (both large and small amounts)
Architect build and launch new data models that provide intuitive analytics
Design and develop new systems and tools to enable folks to consume and understand data faster
Work cross-functionally to define problem statements, collect data, and make recommendations
Build data expertise and own data quality for allocated areas of ownership
Work with data infrastructure to triage infra issues and drive to resolution
Support on-call shift as needed to support the team
BS in Computer Science, Engineering, Math or related field
4+ years experience in custom ETL/data pipeline design, implementation and maintenance
4+ years experience of SQL (Oracle, Vertica, Hive, etc.)
Expert coding skills with at least one object-oriented programming language (Python, Java, PHP)
Experience with data architecture, data modeling, schema design and software development
Experience with large data sets, Hadoop, and data visualization tools
Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders
Experience with packages such as R, Tableau, SPSS, SAS, STATA, etc.
Familiar with version control systems (Git, Mercurial, etc.)
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
"Data Engineer, Partnerships Central Systems, Data & Tools",$115k-$187k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Facebook is looking for exceptionally talented and experienced data engineers to join the Partnerships Central Systems, Data and Tools team. The Partnerships team at Facebook works with leading content creators, publishers, and businesses in entertainment, sports, news, and many other verticals. This role is a unique opportunity to work with one of the most important datasets in the world to create analytics tools and systems that enable field organizations, analysts and clients alike. You will work with some of the brightest minds in the industry, and get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match. The ideal candidate will have a passion for problem solving and a belief in the power of incremental change in a fast-paced environment.

Develop methods to unlock access to data for stakeholders across the Partnerships Organization
Design, build and maintain efficient & reliable data pipelines to move and transform data (both large and small amounts)
Architect build and launch new data models that provide intuitive analytics
Design and develop new systems and tools to enable folks to consume and understand data faster
Work cross-functionally to define problem statements, collect data, and make recommendations
Build data expertise and own data quality for allocated areas of ownership
Work with data infrastructure to triage infra issues and drive to resolution
Support on-call shift as needed to support the team

BS in Computer Science, Engineering, Math or related field
4+ years experience in custom ETL/data pipeline design, implementation and maintenance
4+ years experience of SQL (Oracle, Vertica, Hive, etc.)
Expert coding skills with at least one object-oriented programming language (Python, Java, PHP)
Experience with data architecture, data modeling, schema design and software development
Experience with large data sets, Hadoop, and data visualization tools
Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders

Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.

Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Senior SQL Data Engineer,-1,"Our Customer is one of the leading global interconnection platforms and the world’s largest data center provider. They connect the world's leading businesses to their customers, employees and partners inside the world's most connected data centers in 52 markets across five continents. Their mission is to protect, connect and power the digital economy.

We are seeking a Senior SQL Data Engineer on a contract basis with advanced knowledge of SQL and intermediate knowledge of Python.

In this role your primary focus will be writing complex SQL queries, optimizing them and development of all server-side backend data processing logic, ensuring high performance using Python and SQL.

The candidate must be able to pass I-9 verification.

What You’ll Do:

• Develop and maintain scalable ETL pipelines, build new pipelines and facilitate API integrations to support new requirements

• Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging

• Writing SQL functions, procedures as required based on the requirements

• Fine-tune or optimize queries to support the increasing volume of data

• Debug Python code, modify and enhance Python ETL applications based on the requirements on Linux environment

• Writing reusable and efficient code in Java, Python and SQL

• Develop Rest APIs using java libraries

• Write unit, functional, regression tests for enhanced feature, maintain engineering documentation

• Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture

Must Haves:

• 5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries, function and stored procedures.

• 3+ years of experience with hands on experience in writing, debugging Python code on Linux.

• Basics of Computer Science - OOPS, Data Structures and Algorithms

• Basic understanding of regular Linux commands and usage

• Experience writing Python applications that interact with ORM (Object Relational Mapper) libraries

• Knowledge of XML and JSON parsing with unit test and debugging skills

• Willingness and ability to learn new tools/languages as needed

• Process-oriented with excellent verbal and written communication skill with a desire for customer service

• Excellent team player and communicator who can work effectively with cross functional teams and ability to navigate ambiguity

• Beginner or intermediate level java experience a plus!

Education: Bachelor's Degree in Computer Science or related field

Hours & Location:

M-F, 40 hours/week. This role will be remote while Covid restrictions are in place. The expectation is to be onsite at Sunnyvale, CA location once it is deemed safe to do so.

Now for the Perks!

Health Benefits: Medical, Dental, Vision, Life (including spouse & child), 401k, STD/LTD, AD&D, and Commuter Benefits program.",4.7,"The Mom Project
4.7","Sunnyvale, CA",-1,51 to 200 Employees,2016,Company - Private,Staffing & Outsourcing,Business Services,Unknown / Non-Applicable,-1
"Senior Data Engineer, Membership, Data Science and Engineering",$159k-$283k (Glassdoor Est.),"Los Gatos, California Infrastructure and Tooling Netflix is revolutionizing entertainment as well as pushing the limits of what it means to be a subscription business. We are one of the planet’s largest recurring subscription services and our members are rightfully the cornerstone of what we do. We enable AB experiments that help grow our member base as well as support our business and finance teams to meet regulations and create new business opportunities. The data is truly mission-critical. This role requires folks who enjoy the business complexity that emerges as we manage the member lifecycle which includes interfacing with membership and billing systems to help accurately report numbers in our earnings release and present forecasts to company executives. As part of the Membership & Finance Data Engineering team you will play a vital role in creating reliable, distributed, near real-time event-driven data pipelines and building intuitive data products that allow our stakeholders to easily leverage data in a self-service manner. You will be expected to show thought leadership and partner effectively with our science and engineering teams to push the business towards better metrics and more elegant insights. Location of work: For this role, we are considering both, candidates who are willing to relocate to Los Gatos, California, as well as fully-remote candidates (remote in the US with occasional visits to Los Gatos). For fully-remote work: You are a remote-work expert who will help uplevel the team’s collaboration skills! Who are you: You LOVE data of all sorts, big and small! You enjoy helping teams push the boundaries of what business insights can be extracted out of data You have a strong background in distributed data processing, software engineering components, and data modeling concepts You are always on the lookout for opportunities to simplify, automate tasks, and build reusable components that can be leveraged across multiple use cases and teams You have an eye for detail and like to spark joy in internal partners with high-quality data products that are well documented, modeled, and easy to understand You have strong communication skills to effectively partner with data scientists and engineering stakeholders. You love to innovate and push partners to deliver on novel metrics and solutions You are proficient in at least one major language (e.g. Java, Scala, Python) along with SQL. You strive to write beautiful code and you're comfortable with picking up new technologies You relate to and embody many of the aspects of the Netflix Culture. You love to operate independently while collaborating and giving/receiving strong, candid feedback to your fellow team members Learn more about the team, technologies, and the immediate team members you’d get to work with! If you’d like us to make changes to the interview process to improve the odds of your sailing through it with flying colors, please share your thoughts with us - we promise to do whatever is feasible to accommodate. APPLY NOW Share this listing: LINK COPIED",4.1,"Netflix
4.1","Los Gatos, CA",-1,5001 to 10000 Employees,1997,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Senior Data Engineer,-1,"Step is a next generation financial services company building the best banking experience to help teens and young adults achieve financial independence and knowledge at an earlier age. Step's mission is to create a bank that gives you the tools to become smarter with your money and reward you along your financial journey. Step is a well-funded Series A company focused on disrupting banking through a differentiated mobile-first consumer experience. The founding team has extensive background in FinTech and building successful consumer products and brands. If you're looking to join a fast-growing company with a strong mission and vision that puts people first, we want to meet you.

Role

As a Senior Data Engineer, you will implement and manage Data Models and ETLs that will enable fraud analysis and business analytics. You are a self-starter and you are comfortable working cross-functionally with other teams across Step.

Responsibilities
Partner with various stakeholders within the business, understand their data and reporting requirements, and translate them into definitions and technical specifications
Be responsible for defining, developing and optimizing curated datasets and schemas with standardized metrics and definitions across the company
Develop, deploy and maintain ETL jobs and visualizations
Troubleshoot technical issues with platforms, data discrepancies, alerts etc
Perform ad hoc analysis, insight requests, and data extractions to resolve critical business issues
Must haves
5+ years working experience in a successful data engineering or business intelligence team
Expert knowledge in data modeling concepts and implementation
Strong technical accomplishments in SQL, ETLs and data analysis
Hands on experience in processing large data sets
Strong business intuition and ability to understand complex business systems
Core technologies we use
PostgreSQL
BigQuery
AirStream
Apache Beam/Spark
Working at Step
Competitive salary based on experience, with full medical and dental benefits
Stock in an early stage startup that is growing
Flexible WFH and vacation policy
Free daily lunches
Free snacks & drinks
Monthly team events outside the office
Monthly stipend for commuter benefits and card testing
Office right next to Caltrain",2.7,"Step
2.7","Palo Alto, CA",-1,201 to 500 Employees,-1,Nonprofit Organization,Social Assistance,Non-Profit,$10 to $25 million (USD),-1
Senior Data Engineer,-1,"As a Data Engineer at Eaze, you'll report to the VP of Engineering and work on our data pipelines and infrastructure to help improve data accessibility, increase data reporting performance, and ensure scalability of our data warehouse as we scale. Maintaining, enhancing, and architecting our data infrastructure Architecting and building internal apps for the data team Architecting and building denormalized tables for our data warehouseBuilding ETL pipelines to move and transform data from internal and external sources Performance tuning our data warehouse Basic Qualifications 3+ years previous experience working on data or engineering teams Demonstrated excellence in Python Demonstrated excellence in SQL Experience with AWS infrastructure (Kinesis, SQS, S3, etc) Experience with Redshift or other large scale data warehousing solutions Experience with Postgres or other transactional databases A passion for analyzing data and working on open ended problems Strong communication and organizational skills Self-motivated with the ability to work independently Must be authorized to work in the United States",-1,Eaze,"San Francisco, CA",-1,-1,-1,-1,-1,-1,-1,-1
Lead Data Engineer (remote),$144k-$264k (Glassdoor Est.),"San Francisco, California, USA Are you at your most vibrant when you’ve successfully distilled data into its simplest, most meaningful form? ThoughtWorks is a global software consultancy with an aim to create a positive impact on the world through technology. Our community of technologists thinks disruptively to deliver pragmatic solutions for our clients' most complex challenges. We are curious minds who come together as collaborative and inclusive teams to push boundaries, free to be ourselves and make our mark in tech. Our developers have been contributing code to major organizations and open source projects for over 25 years. They’ve also been writing books, speaking at conferences and helping push software development forward, changing companies and even industries along the way. We passionately believe that software quality is driven by open communication, review and collaboration. That’s why we’re such vehement supporters of open source and have made significant contributions to open source tools for testing, continuous delivery (GoCD), continuous integration (CruiseControl), machine learning and healthcare. As consultants, we work with our clients to ensure we’re evolving their technology and empowering adaptive mindsets to meet their business goals. You could influence the digital strategy of a retail giant, build a bold new mobile application for a bank or redesign platforms using event sourcing and intelligent data pipelines. You will learn to use the latest Lean and Agile thinking, create pragmatic solutions to solve mission-critical problems and challenge yourself every day. Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution. You’ll spend time on the following: You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems You will partner with teammates to create complex data processing pipelines in order to solve our clients’ most ambitious challenges You will collaborate with Data Scientists in order to design scalable implementations of their models You will pair to write clean and iterative code based on TDD Leverage various continuous delivery practices to deploy, support and operate data pipelines Advise and educate clients on how to use different distributed storage and computing technologies from the plethora of options available Develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions Create data models and speak to the tradeoffs of different modeling approaches On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product Seamlessly incorporate data quality into your day-to-day work as well as into the delivery process Here’s what we’re looking for: You are equally happy coding and leading a team to implement a solution You have a track record of innovation and expertise in Data Engineering You’re passionate about craftsmanship and have applied your expertise across a range of industries and organizations You have a deep understanding of data modelling and experience with data engineering tools and platforms such as Kafka, Spark, and Hadoop You have built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, NoSQL databases (Hbase, Cassandra, etc.) and any of the distributed processing platforms like Hadoop, Spark, Hive, Oozie, and Airflow in a production setting Hands on experience in MapR, Cloudera, Hortonworks and/or cloud (AWS EMR, Azure HDInsights, Qubole etc.) based Hadoop distributions You are comfortable taking data-driven approaches and applying data security strategy to solve business problems You’re genuinely excited about data infrastructure and operations with a familiarity working in cloud environments Working with data excites you: you have created Big data architecture, you can build and operate data pipelines, and maintain data storage, all within distributed systems Advocate your data engineering expertise to the broader tech community outside of ThoughtWorks, speaking at conferences and acting as a mentor for more junior-level data engineers Assure effective collaboration between ThoughtWorks’ and the client’s teams, encouraging open communication and advocating for shared outcomes A few important things to know: While we’ve traditionally been a traveling consultancy, travel is not required for this role at the moment. We anticipate the need for travel to our client locations in the future when it’s deemed safe. Candidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD. Not quite ready to apply? Or maybe this isn’t the right role for you? That’s OK, you can stay in touch with AccessThoughtWorks, our learning community (click ""contact me about recruitment opportunities"" to hear about jobs in the future). It is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex/gender, national origin, ethnic origin, veteran or military status, family or marital status, disability, genetic information, age, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.",3.8,"ThoughtWorks
3.8","San Francisco, CA",-1,5001 to 10000 Employees,1993,Company - Private,IT Services,Information Technology,$100 to $500 million (USD),-1
Senior Data Engineer,$100k-$179k (Glassdoor Est.),"Leading the future of luxury mobility

Lucid’s mission is to inspire the adoption of sustainable energy by creating the most captivating luxury electric vehicles, centered around the human experience. Working at Lucid Motors means having a shared vision to power the future in revolutionary ways. Be part of a once-in-a-lifetime opportunity to transform the automotive industry.

We are looking for a Senior Data Engineer, who is looking for a challenge, enjoys thinking big and looking to make their mark on an extremely fast growing company. If you have hands-on experience designing and developing streaming and IoT data pipelines this role is for you. Be part of a group who will be building large and building fast, working with a very talented team of engineers and collaborating with the brightest mind in the Automotive industry.
The Role
Hands-on design and develop streaming and IoT data pipelines.
Developing streaming pipeline using MQTT, Kafka, Spark Structure Streaming
Orchestrate and monitor pipelines using Prometheus and Kubernetes
Deploy and maintain streaming jobs in CI/CD and relevant tools.
Python scripting for automation and application development
Design and implement Apache Airflow and other dependency enforcement and scheduling tools.
Hands-on data modeling and data warehousing
Deploy solution using AWS, S3, Redshift and Docker/Kubernetes
Develop storage and retrieval system using Presto and Parquet/ORC
Scripting with Apache Spark and data frame.
Qualifications
Bachelor or Masters in Software Engineering or Computer Science
6 - 8+ years of experience in Data Engineering and Business Intelligence.
Proficient in IoT tools such as MQTT, Kafka, Spark
Proficient with AWS, S3, Redshift
Experience with Presto and Parquet/ORC
Proficient with Apache Spark and data frame.
Experienced in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Python,
Experience with Columnar database such as Redshift, Vertica
Great verbal and written communication skills.
Be part of something amazing

Come work alongside some of the most accomplished minds in the industry. Beyond providing competitive salaries, we’re providing a community for innovators who want to make an immediate and significant impact. If you are driven to create a better, more sustainable future, then this is the right place for you.

At Lucid, we don’t just welcome diversity - we celebrate it! Lucid Motors is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, national or ethnic origin, age, religion, disability, sexual orientation, gender, gender identity and expression, marital status, and any other characteristic protected under applicable State or Federal laws and regulations.

To all recruitment agencies: Lucid Motors does not accept agency resumes. Please do not forward resumes to our careers alias or other Lucid Motors employees. Lucid Motors is not responsible for any fees related to unsolicited resumes.",3.9,"Lucid Motors
3.9","Newark, CA",-1,1001 to 5000 Employees,2007,Company - Private,Transportation Equipment Manufacturing,Manufacturing,Unknown / Non-Applicable,-1
Backend Data Engineer,-1,"About Live Objects
Live Objects delivers continuous business process optimizations to the enterprise through AI-driven automation of discovery, design and process engineering by mining patterns in business objects, cases, and transactions across all process variations. The product integrates deeply with business process management platforms (like SAP, Salesforce, etc.) and delivers continuous process engineering natively as on-demand compositions through the platform’s interfaces. Live Objects’ path-breaking process-calculus engine models predictive, diagnostic and quality-related projections for live cases in business processes. Its AI-driven on-demand process reengineering engine composes process enhancements with rule-based associations with business objects for delivering quantified time, process, margin, and cost efficiency gains. The on-demand compositions can be subject to review by functional subject matter experts in CRM, ERP, MRP, order-to-cash, etc. before deploying them into live business processes. Ongoing client engagements include self-optimizing a wide spectrum of business processes including master data management, order-to-cash, sales distribution, and supply-chain management. The company is based in Palo Alto and venture funded by The Hive. The Hive is a fund and co-creation studio for AI-powered enterprise applications.

About the Role
The Backend Data Engineer will drive the design and development of key components of the platform including data processing pipeline, feature extraction, and process modeling, intelligent integrations with key business process platforms (like SAP, Salesforce, etc.) with live process rules and business object insertions and process risk/conformance modeling.

Responsibilities
As a Backend Data Engineer of a fast-growing startup, the successful candidate will be leading the development of key aspects of Live Objects’ product:

Assemble large, complex data sets that meet functional / non-functional business requirements.
Perform design, development for new product features.
Understand product vision and business needs to define product requirements and product architectural solutions.
Develop architectural and design principles to improve the performance, capacity, and scalability of the product.
Create and maintain optimal data pipeline architecture
Building reusable code and libraries for future use
Implementation of security and data protection

About You
The successful candidate will have experience in working in innovative projects with fast-paced delivery schedules in startups & large enterprises:

Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of SQL and NoSQL databases.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Strong analytic skills related to working with unstructured datasets.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in user authentication and authorization between multiple systems, servers, and environments
Experience in Integration of multiple data sources and databases into one system
Proficient knowledge in Java, JavaScript application development paradigm
Proficient understanding of code versioning tools, such as Git
Understanding of “session management” in a distributed server environment
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Scala, etc.
10+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics,
Informatics, Information Systems or another quantitative field.
Please send your resumes to jobs@liveobjects.ai",-1,LIVE OBJECTS,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Staff BI and Data Engineer,$105k-$124k (Glassdoor Est.),"Job Description: Staff BI and Data Engineer
Location: San Jose, CA (or) New York, NY
Department: Data Engineering
Hours/Shift: Full Time
Reports To: VP, Data Engineering
Job Description:
Affinity Solutions is looking for a hands-on and self-driven Staff BI and Data Engineer, preferably with experience in the bank card loyalty/fin-tech/advertising/marketing space, to enhance and automate its data and analytics infrastructure to support its growing customer base and expanding partner ecosystem. The position is based in San Jose, CA., and reports into the Data Engineering division. The demand for advanced analytical solutions continues to grow exponentially, and this is your opportunity to grow with us in a fast-paced, collaborative environment.
The ideal candidate is a passionate and highly skilled individual, who can utilize programming and analytics tools such as SQL, Python, and Tableau to query and process large data sets to produce high quality customer facing data deliverables and insights. If you have experience designing and building business intelligence/analytics applications, especially around credit/debit card transactions, unified consumer behavioral and profile data, and excited about leveraging your experience to catapult a venture-backed company into hyper-growth, this job is for you.
Responsibilities:
Develop high quality analytical data assets with an eye towards process efficiency and automation through scripting. Experience in building data marts is a plus.
Build automated QA process to validate the quality of the data and report on data quality
Communicate and present data to both internal and external customers by developing reports/dashboards/charts using BI tools such as Tableau
Work closely with a dynamic and growing team of account managers, data engineers and data scientists to perform quantitative analysis of customer data, including gathering data requirements and validate data, applying judgement and statistical analysis to assist with planning and decision making.
Other responsibilities include but not limited to - data validation, troubleshooting issues, and process documentation.
Qualifications:
Bachelor’s or Master’s degree in Computer Science or related field such as Mathematics and Statistics, preferably with focus on Data Analytics.
At least 3 years of hands-on experience in designing and building data pipelines, analytical data applications and BI Reporting.
Proficient in SQL and Tableau, familiar with at least one coding language in Python/Shell scripting.
Experience in using Cloud based managed services and Big Data Environments for data warehousing/analytics is a big plus – e.g. Amazon RedShift, Google BigQuery, Spark, MapR etc.,
Very strong written and verbal communication skills; Ability to tell a story with the data
Analytical thinker, with an ability to evaluate multiple products/technologies to address various aspects of a big data platform.
Experience working on UNIX / Linux development and production environments
Experience working in Agile software development environments
Strong organization skills with attention to detail is a must.
Ability to manage multiple conflicting priorities, take proactive ownership of problems and outcomes, think outside the box
Knowledge of Retail and Financial verticals is useful but not required.",3.2,"Affinity Solutions
3.2","San Jose, CA",-1,51 to 200 Employees,1998,Company - Private,Advertising & Marketing,Business Services,Unknown / Non-Applicable,-1
"(Senior) Data Engineer, DevOps",-1,"The Opportunity

Data Engineering plays a key role in insitro’s approach to rethinking drug development. The Data Engineering DevOps team ensures the infrastructure which powers our biological data factory’s robots, instruments, and machine learning platform is reliable, scalable, and manageable. You will work closely with a cross-functional team of scientists, bioengineers, and data scientists to identify areas where data engineering can make a difference, by developing data architectures and systems on cutting edge, high throughput platforms that enable our scientists to be maximally productive. You will design, implement, and deploy cloud infrastructure, including managed databases, application servers, data warehouses, and interactive/batch computing environments, and work as part of a team to rigorously design our data platform, identify key architectural performance improvements, and join an on-call rotation to ensure that insitro's platform runs at maximum productivity.

You will be joining as the founding team of a biotech startup that has long-term stability due to significant funding, but yet is very much in formation. A lot can change in this early and exciting phase, providing many opportunities for significant impact. You will work closely with a very talented team, learn a broad range of skills, and help shape insitro’s culture, strategic direction, and outcomes. Join us, and help make a difference to patients!

About You

2-3 years of experience with provisioning AWS cloud services (Experience with GCP and Azure is also relevant).
Experience with cloud configuration and resource management tools such as Terraform
Experience architecting reliable infrastructure platforms including monitoring and alerting, load balancing, scalable services, multi-region
Experience with at least one high-end distributed data processing environment (Hadoop, Spark, etc)
Experience with batch computing systems such as AWS Batch, SLURM
Experience with container build and deployment systems like Docker, Kubernetes, or ECS
Ability to communicate effectively and collaborate with people of diverse backgrounds and job functions
Proficiency in Linux environment (including shell scripting and Python programming), experience with database languages (e.g., SQL, No-SQL) and experience with version control practices and tools (Git, Mercurial, etc.)
Passion for making a difference in the world

Nice to Have

Experience with biological data
Experience with managing medium-sized data sets (100TB+) in object storage systems like S3
Experience with defining infrastructure following compliance (GDPR, HIPAA, etc).
Experience with data processing pipelines
Experience with deploying and monitoring machine learning models in a production environment

Benefits at insitro

Excellent medical, dental, and vision coverage
Open vacation policy
Team lunches (catered daily)
Commuter benefits
Paid parental leave

About insitro

insitro is a data-driven drug discovery and development company using machine learning and high-throughput biology to transform the way that drugs are discovered and delivered to patients. The company is applying state-of-the-art technologies from bioengineering to create massive data sets that enable the power of modern machine learning methods to be brought to bear on key bottlenecks in pharmaceutical R&D. The resulting predictive models are used to accelerate target selection, to design and develop effective therapeutics, and to inform clinical strategy. insitro was launched in 2018 with a Series A of $100M funded by top investors including a16z, Arch Venture Partners, Foresite Capital, GV, and Third Rock Ventures. In 2019 the company announced a collaboration with Gilead Sciences in the area of NASH and, in mid 2020, announced a Series B financing of $143M including current investors and new investors Canada Pension Plan Investment Board (CPP Investments), T. Rowe Price, BlackRock, Casdin Capital and other leading investors. The company is located in South San Francisco, CA. For more information about insitro, please visit the company’s website at www. insitro.com.",-1,insitro,"South San Francisco, CA",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Senior Data Engineer,$94k-$168k (Glassdoor Est.),"We are looking for a Senior Data Engineer, who is looking for a challenge, enjoys thinking big and looking to make their mark on an extremely fast growing company. If you have hands-on experience designing and developing streaming and IoT data pipelines this role is for you. Be part of a group who will be building large and building fast, working with a very talented team of engineers and collaborating with the brightest mind in the Automotive industry.

The Role
Hands-on design and develop streaming and IoT data pipelines.
Developing streaming pipeline using MQTT, Kafka, Spark Structure Streaming
Orchestrate and monitor pipelines using Prometheus and Kubernetes
Deploy and maintain streaming jobs in CI/CD and relevant tools.
Python scripting for automation and application development
Design and implement Apache Airflow and other dependency enforcement and scheduling tools.
Hands-on data modeling and data warehousing
Deploy solution using AWS, S3, Redshift and Docker/Kubernetes
Develop storage and retrieval system using Presto and Parquet/ORC
Scripting with Apache Spark and data frame.
Qualifications
Bachelor or Masters in Software Engineering or Computer Science
6 - 8+ years of experience in Data Engineering and Business Intelligence.
Proficient in IoT tools such as MQTT, Kafka, Spark
Proficient with AWS, S3, Redshift
Experience with Presto and Parquet/ORC
Proficient with Apache Spark and data frame.
Experienced in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Python,
Experience with Columnar database such as Redshift, Vertica
Great verbal and written communication skills.
Job Type: Full-time

Pay: $45.00 - $55.00 per hour

Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Education:
Bachelor's (Required)
Location:
Newark, CA (Required)
Work authorization:
United States (Required)
Work Remotely:
Temporarily due to COVID-19",4.3,"Methodica Technologies
4.3","Newark, CA",-1,1 to 50 Employees,2014,Company - Private,Computer Hardware & Software,Information Technology,$1 to $5 million (USD),-1
Hiring for Data Engineer,-1,"*Position: * Data Engineer
*Location: * Sunnyvale, CA
*Hiring: * Long Time Contract
*Minimum Contract Duration: * 6-12 Month
*Telecommute: * Yes, till nCovid Pandemic, then 100% onsite.

* *EX apple is PLUS or from T1 Companies.*
* Candidate should have *STRONG EXP on SQL, PYTHON, UNIX/LINUX, HIVE, Tableau.*

*Job Description: *

* 10 yrs of experience as a Data Analyst.
* Excellent *SQL skills (on SQL and NoSQL databases), Python, UNIX, hive*
* Utilized *SQL, data warehousing, Tableau, *and another dashboard for data intelligence and analysis
* Data Analysis, Pattern & Trend Identification and Generating Actionable Items, Visualization of Data Insights
* Project Manager / Lead experience dealing with different stake-holders across GEOs.
* Liaise with offshore team and different stakeholders for resolving technical dependencies, issues and risks
* Experience in the manufacturing domain a plus
* Excellent communication skills (Oral and Written)

#DataEngineer #DataDeveloper #SQL #UNIX #LINUX #Hive #Tableau #NoSQL #Databases #Python

Job Types: Full-time, Part-time, Temporary, Contract

Pay: $60.00 - $74.00 per hour

Schedule:
* Monday to Friday

Experience:
* Data Engineer: 9 years (Required)
* Python: 3 years (Required)
* Linux/Unix: 3 years (Required)
* Apache Hive: 2 years (Required)
* Tableau: 2 years (Required)
* SQL: 3 years (Required)

Contract Renewal:
* Likely

Company's website:
* https://www.koreminds.com/

Benefit Conditions:
* Waiting period may apply
* Only full-time employees eligible",-1,Koreminds LLC,"Sunnyvale, CA",-1,-1,-1,-1,-1,-1,-1,-1
Data Engineer (Full Time; Multiple Openings),$84k-$156k (Glassdoor Est.),"Data Engineer (Full Time; Multiple Openings) Location: Belmont, CA Job Description: Responsible for optimizing and redesigning RingCentral’s data architecture to support our next generation of products and data initiatives. Job Duties: Creating and maintaining optimal data pipeline architecture Assembling large, complex data sets that meet functional / non-functional business requirements Building the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics Creating data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader Working with data and analytics experts to strive for greater functionality in our data systems Participate in on-call rotation to support data pipelines and tools. Required Skills/Tools: Experience with Hadoop, AWS, Google Cloud, Python, Java, Jira, Tableau, SQL, Linux, and Elasticsearch is required. Education/Experience/Qualifications: U.S. Master’s degree in Computer Science, Information Technology or a related field, or foreign equivalent, is required. U.S. Bachelor’s degree or foreign equivalent in Computer Science, Information Technology or a related field plus five (5) years of related experience, is also acceptable. Within the foregoing parameters, any suitable combination of education, training and experience is acceptable. About RingCentral RingCentral is the worldwide leader in cloud-based communications. Our software communications platform delivers phone, group chat, mobile communications, video calls, videoconference, contact center and AI-driven digital engagement. It’s a powerful, global presence that allows businesses to communicate anywhere, anytime with anyone. RingCentral is headquartered in Belmont, California and has offices around the world. RingCentral is an equal opportunity employer that truly values diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",4.6,"RingCentral
4.6","Belmont, CA",-1,5001 to 10000 Employees,2003,Company - Public,Telecommunications Services,Telecommunications,$1 to $2 billion (USD),-1
"Senior Business Intelligence Data Engineer, Cash App",-1,"Company Description Cash App is the fastest growing financial brand in the world. Initially built to take the pain out of peer-to-peer payments, Cash App has gone from a simple product with a single purpose to a dynamic app with over 24 million monthly active users. We are bringing a better way to send, spend, invest, and save to anyone who has ever sought an alternative to the traditional banking system. Loved by customers and pop culture, we’ve consistently held the top spot for finance in the App Store for many years, seeing more engagement with millions of followers across social media in a day than most brands see in a year. We are building an ecosystem to redefine the world’s relationship with money by making it universally accessible. We want to hire the best talent regardless of location. Our employment model is distributed, offering the opportunity to collaborate with teams across the world in San Francisco, New York, St. Louis, Portland, Toronto, Kitchener-Waterloo, Sydney, and Melbourne. Job Description As a Senior BI Engineer at Cash App, you will work alongside analysts, data scientists, engineers, product managers, and business leads to lay the foundation for the analysis of our large, unique dataset. We are an extremely data-driven team - from understanding our customers, managing and operating our business, and informing product development. You will build, curate, document, and manage key datasets using ETL to increase the leverage of the entire team. You will: Be an expert on the ins and outs of our products, customers, and data Work directly with data scientists, analysts, and product/business owners to create new data models for the most widely used Cash App events, entities, and processes using ETL Standardize and enshrine business and product metric definitions in highly curated and optimized datasets using ETL in our data warehouse Teach and encourage others to self-serve while building tools that make it simpler and faster for them to do so Evangelize data, analytics, and data model design best practices. Develop and manage our current set of BI tools and analytics/ETL infrastructure Qualifications You have: A degree in software engineering, computer science or a similar technical field Experience building complex, scalable ETLs for a variety of different business and product use cases Technical initiative and a desire to perform and grow as an individual Technologies we use and teach: SQL (MySQL, Snowflake, BigQuery, etc.) Airflow, Tableau Python, Java Additional Information Cash App treats all employees and job applicants equally. Every decision is based on merit, qualifications, and talent. We do not discriminate on the basis of race, religion, colour, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with each office’s corresponding local guidelines.",-1,Cash App,"San Francisco, CA",-1,-1,-1,-1,-1,-1,-1,-1
Sr. Data Engineer - Ecommerce (Remote),$138k-$252k (Glassdoor Est.),"At CrowdStrike we’re on a mission - to stop breaches. Our groundbreaking technology, services delivery, and intelligence gathering together with our innovations in machine learning and behavioral-based detection, allow our customers to not only defend themselves, but do so in a future-proof manner. We’ve earned numerous honors and top rankings for our technology, organization and people – clearly confirming our industry leadership and our special culture driving it. We also offer flexible work arrangements to help our people manage their personal and professional lives in a way that works for them. So if you’re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to stop breaches and protect people globally, let’s talk.
About the Role: We're spinning up a brand new team and initiative at CrowdStrike to focus on Ecommerce. We'll be covering areas like the CrowdStrike App Marketplace and monetization (in-app purchasing/micro-transactions/etc), creative new ways to approach Licensing, Billing and Entitlement across the entire product portfolio, and all the backend analytics work to drive a better user experience and optimize engagement, conversion, cross-sell and up-sell. We'll also be spearheading the creation of new CrowdStrike offerings that are built on the existing world-class Falcon platform but target a completely ecommerce driven, low-touch, high engagement product experience. If you're passionate about building intuitive applications that customers will rave about, tell their friends about and can't wait to use daily, we want you on our team.
As a Data Engineer on the CrowdStrike Ecommerce team, you will be involved in building data systems at scale that ingest billions of events, decisions and transactions every day. In this role, you will help make it possible for internal and external customers to easily work with data we produce at massive scale. You will be part of a data platform team that is continuously developing and improving product features to protect our customers from increasingly complex threat actors attempting data breaches.
Job Responsibilities:
Design, develop, and maintain workflows that can process petabytes of data
Use advanced data analysis techniques and architectures that leverage technology to process large volumes of data to perform complex computations in a scalable, reproducible and automated manner.
Help us research and implement new ways for appropriate stakeholders to query their data efficiently and extract results in the format they desire
Participate in technical reviews of our products and help us develop new features and enhance stability
Continually help us improve the efficiency of our services so that we can delight our customers
Qualifications for Data Engineer: We are looking for a candidate with a BS and 5+ years or MS and 3+ years in Computer Science or related field. They should also have experience with the following software/tools -
A solid understanding of algorithms, distributed systems design and the software development lifecycle
Experience with relational SQL and NoSQL databases, including Postgres/MySQL, Cassandra, DynamoDB
Solid background in Web application development (C#, Java/Scala, Go, Node.JS) and scripting languages like Python
Golang experience is desirable
Experience building large scale data pipelines
Good test-driven development discipline
Reasonable proficiency with Linux administration tools
Proven ability to work effectively with remote teams
Experience with the following tools is desirable:
Strong familiarity with the Apache Hadoop ecosystem including: Spark, Kafka, Hive, etc
Recent hands-on experience with modern data platforms such as SQL, RDMS, or Graph DBs, and Data Visualization skills using BI tools such as Tableau, Power BI, Domo, or D3.
Kubernetes
Jenkins
#LI-JF1 #LI-Remote #Stack

Benefits of Working at CrowdStrike:
Market leader in compensation and equity awards
Competitive vacation policy
Comprehensive health benefits + 401k plan
Paid parental leave, including adoption
Flexible work environment
Wellness programs
Stocked fridges, coffee, soda, and lots of treats

We are committed to building an inclusive culture of belonging that not only embraces the diversity of our people but also reflects the diversity of the communities in which we work and the customers we serve. We know that the happiest and highest performing teams include people with diverse perspectives and ways of solving problems so we strive to attract and retain talent from all backgrounds and create workplaces where everyone feels empowered to bring their full, authentic selves to work.
CrowdStrike is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.

CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",3.9,"CrowdStrike
3.9","Sunnyvale, CA",-1,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$100 to $500 million (USD),-1
Remote Data Engineer,$72k-$133k (Glassdoor Est.),"Experience: **Fully Remote** Must be US Citizen and able to obtain a TS/SCI Develop standardized data architecture that includes data structure and transfer protocols to facilitate sensor integration and dynamically share information to improve situational awareness. Develop a standardized architecture that supports a centralized data repository that advances all data analytics, and AIML capabilities enhance command and control decisions. Design and build end-to-end data pipeline solutions (esp. streaming and batch processing, machine learning model training and updating). Develop strategies for data acquisitions, archive recovery, and implementation of a database. Define, design, and build dimensional databases. Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it. Writes unit/integration tests, contributes to engineering wiki, and documents work. Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues. Works closely with a team of frontend and backend engineers, product managers, and analysts. Designs data integrations and data quality framework. Works closely with all engineering teams to develop strategy for long term data platform architecture. Qualifications / Skills: SQL/MySQL/PostgreSQL Microservice deployment, cloud certification (AWS architect) Education, Experience, and Licensing Requirements: BS or MS degree in Computer Science or a related technical field 2-4+ years of experience as a data engineer U.S. Citizenship with ability to obtain government security clearance",3.7,"Edgesource Corporation
3.7","Mountain View, CA",-1,51 to 200 Employees,-1,Company - Public,IT Services,Information Technology,$10 to $25 million (USD),-1
Sr. Data Engineer,-1,"DESCRIPTION
Design and develop framework to automate ingestion of structured data
Build distributed data infrastructure using open source tools.
Design and develop data pipeline components
Scale pipelines to meet performance requirements in a microservice architecture
Provide leadership to other Data Engineers
Responsible for technical direction of the team
Collaborate with colleagues and sharing responsibility throughout the product life-cycle
REQUIREMENTS
5+ years of software development experience
3+ years of experience with building scalable and reliable data pipelines
2+ years of experience with scalable data integration technologies
Demonstrated experience in deploying and managing SQL, NoSQL and Time Series databases
Have Knowledge on big data platform infrastructure like Kafka, Flume, Spark, Hadoop
Extensive experience building Reactive applications with Scala, Play, Lagom, Akka
Proficient in Scala, and Python
Experience with cloud technologies such as VMware, AWS, Azure, or Google Cloud
Demonstrated ability to communicate and collaborate with peers
Demonstrated skills in result-driven problem solving
Experience participating in talent screening and interviewing
BENEFITS
Medical, Dental & Vision Insurance
Life & Disability Coverage
Retirement/401K Plan
Flexible work and vacation schedules
Fully-stocked kitchen with healthy snacks and drinks
Free catered Wednesday lunches
Open and spacious office that is close to public transportation",5.0,"CloudKnox Security
5.0","Sunnyvale, CA",-1,1 to 50 Employees,2016,Company - Public,-1,-1,$1 to $5 million (USD),-1
Marketing Data Engineer,$102k-$187k (Glassdoor Est.),"Silicon Valley Bank is seeking an experienced Data Engineer that will report into the Director of Marketing Technology & Analytics to grow the marketing analytics team and deliver on innovation opportunities. You will be critical asset - assisting the leader to deliver 360 view of client data to enable enhanced segmentation to optimize and personalize cross channel customer journeys. You will work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions. These solutions will generate insights from our connected data, enabling SVB to advance the data-driven decision-making capabilities of our marketing organization, globally. This role requires deep understanding of data architecture, data engineering, data analysis, reporting, and a basic understanding of data science techniques and workflows. In this role you will: Sustain industry leading data solutions to drive data-driven marketing and create client centric experiences. Solve complex data problems to deliver insights that helps our business to achieve their goals Advise, consult, mentor and coach other data and analytic professionals on data standards and practices Foster a culture of sharing, re-use, design for scale, stability, and operational efficiency of data and analytical solutions You have a strong understanding and aptitude for external digital and industry trends and can translate such trends into actionable innovation opportunities. Practice Development Contribute to the development of standard methodologies in analysis, design and test to ensure as an analytics team we are providing outstanding quality work Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Lead evaluation, implementation and deployment of emerging tools & process for analytic data engineering to improve our productivity as a team Contribute to the development and education plans on analytic data engineering capabilities, standards, and processes Design & Development Develop and maintain data pipelines for efficient data ingestion/collection, architect and implement data processing and storage solutions as well as enable data consumption through a variety of applications and tools Collaborate with marketing teams and other SVB teams and understand the systems and data points applicable to each group, and to advise on areas for opportunity and collaboration in ingesting and tracking data Undertake preprocessing of structured and unstructured data Joins, normalizes, aggregates, and transforms data from raw to consumable form Manage existing databases, ensuring data integrity and data usability Collect, analyze and document data wrangling and analytics requirements Contribute to the development of SVB Marketing’s data strategy and roadmap, and develop technical solutions to enrich the client data Define data security and controls requirements Measurement/Optimization Provides input to define IT data requirements to support key business metrics and KPIs Assess the effectiveness and accuracy of new data sources and data gathering techniques Collaborate with other performance marketing experts to define success metrics to measure effectiveness of new analytical models Qualifications Bachelor’s degree in Computer Science, MIS, or Engineering 4+ years of experience working in data engineering or architecture role 2+ years of experience working in marketing organizations Preferred 3+ years in B2B marketing / financial services Expertise in SQL and data analysis and experience with at least one programming language Experience with SAS or other ETL tooling Experience developing and maintaining data warehouses in big data solutions Experience working with BI tools such as Tableau Experience working on a collaborative agile product team Strong agile/scrum delivery skillet preferred Leadership in, and understanding of, Agile concepts, processes, practices, procedures and tools Passion for agile software processes, data-driven development, reliability, and experimentation Strong analytical and problem-solving skills; ability to combine attention to detail with big picture perspective Demonstrates excellent oral and written communication skills; ability to communicate with all levels of the organization, including executive management, functional experts, and IT team members",3.6,"Silicon Valley Bank
3.6","Santa Clara, CA",-1,1001 to 5000 Employees,1983,Company - Public,Banks & Credit Unions,Finance,$1 to $2 billion (USD),-1
Big Data Engineer (Senior),$86k-$161k (Glassdoor Est.),"About NCR NCR Corporation (NYSE: NCR) is a leading software- and services-led enterprise provider in the financial, retail and hospitality industries. NCR is headquartered in Atlanta, Georgia, with 36,000 employees globally. NCR is a trademark of NCR Corporation in the United States and other countries. NCR's Digital Insight™ solutions are a leading Software-as-a-Service (""SaaS"") platform for financial institutions in the United States. We connect over 600 small to mid-size banks and credit unions with over 18 million online banking users and nearly 10 million mobile users. Join us to revolutionize digital banking by building upon our open technology platform developed on a unique service-oriented architecture (SOA), connecting with the wide breadth of NCR offerings across Retail, Hospitality, and Financial Services industries. What we build: The Digital Banking team in Redwood City, California is looking for Data Engineers to develop our next generation Digital Banking Data Platform. You will build and design highly scalable data pipelines using new generation tools and technologies like Google Cloud Platform (Pub Sub, Dataflow, BigQuery, BigTable), Kafka, Flume, to induct data from various systems to provide efficient reporting and analytics capability You will translate complex business requirements into scalable technical solutions, and design dashboards or visualization using BI tools to perform data analysis and to support business Collaborate with multiple cross functional teams such as product management, solution architectures, security, and software engineering Position Summary & Key Areas of Responsibility for the Big Data Engineer: NCR's Digital Banking solutions are a leading Software-as-a-Service (""SaaS"") platform for financial institutions in the United States. The Digital Banking team in Redwood City, California is looking for a Data Engineer to participate in the development of our next generation Digital Banking Data Platform. As a Data Engineer, you will build and design highly scalable data pipelines using new generation tools and technologies like Spark, Kafka, Storm and BigQuery to induct data from various systems to provide efficient reporting and analytics capability. You will translate complex business requirements into scalable technical solutions, and design dashboards or visualization using BI tools to perform data analysis and to support the business. You will collaborate with multiple cross functional teams such as Product Management, Solution Architecture, Security, and Software Engineering. Basic Qualifications for the Big Data Engineer : Bachelor’s degree in a technical discipline or equivalent work experience 6+ years experience in designing and developing ETL data pipelines. 6+ years experience in OOP programming language (Java, Python) Strong understanding of data modeling, data structures and algorithms Any experience with Google Cloud Platform (Pub Sub, Dataflow, BigQuery, BigTable) Experience with all aspects of software development life cycle (source control, continuous integration, deployments, etc.) Preferred Qualifications for the Big Data Engineer : Any experience developing Big Data/Hadoop applications using java, Spark, Hive, Oozie, Kafka, and Map Reduce is a huge plus Experience with or advance courses on data science and machine learning is ideal Any experience with BI tools is a plus Offers of employment are conditional upon passage of screening criteria applicable to the job. EEO Statement Integrated into our shared values is NCR's commitment to diversity and equal employment opportunity. All qualified applicants will receive consideration for employment without regard to sex, age, race, color, creed, religion, national origin, disability, sexual orientation, gender identity, veteran status, military service, genetic information, or any other characteristic or conduct protected by law. NCR is committed to being a globally inclusive company where all people are treated fairly, recognized for their individuality, promoted based on performance and encouraged to strive to reach their full potential. We believe in understanding and respecting differences among all people. Every individual at NCR has an ongoing responsibility to respect and support a globally diverse environment. Statement to Third Party Agencies To ALL recruitment agencies: NCR only accepts resumes from agencies on the NCR preferred supplier list. Please do not forward resumes to our applicant tracking system, NCR employees, or any NCR facility. NCR is not responsible for any fees or charges associated with unsolicited resumes.",3.6,"NCR
3.6","Redwood City, CA",-1,10000+ Employees,1884,Company - Public,Computer Hardware & Software,Information Technology,$5 to $10 billion (USD),-1
Sr. Data Engineer,-1,"At Potrero Medical, our mission is to help clinicians transform patient care by developing a predictive technology platform for supporting the early detection of critical illnesses.

Our medical device, the Accuryn® Monitoring System, received 510(k) clearance from the FDA in 2016 and helps hospitals accurately measure patients’ vital signs such as urine output, body core temperature, and intra-abdominal pressure.

We are a Silicon Valley-based predictive health company developing the next generation of medical devices with smart sensors and artificial intelligence. Founded in the historical Potrero Hill neighborhood of San Francisco, we emerged out of Theranova, a MedTech incubator focused on tackling the biggest challenges in healthcare.

Senior Data Engineer

POSITION DESCRIPTION
This role will implement pipelines to extract, transform and load data from various sources, including data downloaded from medical devices used in clinical trials, as well as the electronic medical record (EMR). The pipelines are then used by Potrero data scientists to develop predictive health algorithms that run on Potrero medical devices. This position will build and maintain data tools to support the analytics and clinical teams in working with the data. Finally, the role will ensure that the data infrastructure is secure and in compliance with recognized cybersecurity standards, including required documentation and testing.

ESSENTIAL FUNCTIONS (Included but not limited to)
Experienced implementing production-grade big data solutions on Amazon Web Services
Experience with relational or analytics databases (Postgres and Redshift or Snowflake).
Experience with an IAC language such as Terraform for provisioning cloud microservices in containerized environments.
Experience moving data (ETL)
Ability to modify or write scripts with pandas, NumPy, or bokeh that move data or provide visualizations.
Familiar with HIPAA regulations
Perform additional responsibilities or other functions as assigned

REQUIREMENTS
5+ years’ experience preferably in the medical device or a similarly regulated industry.
Python
Experience with EHR/EMR integrations via HL7 or FHIR is a plus
Experience implementing and documenting any of the following standards as it pertains to cloud infrastructure and ETL solutions: Soc2, ISO 27001/27002, HITRUST, PCI DSS, NIST 800-5, FEDRAMP.
Familiarity with Agile methodology
Responsible for performing all duties in compliance with FDA’s Quality System Regulation (QSR), ISO13485, ISO 14971, the Canadian Medical Device Regulations, and all other international regulatory requirements with which Potrero Medical complies.

EDUCATION
BS in computer science or similar field.

Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

WORK ENVIRONMENT
This job operates in a professional office environment, it can be performed remotely.
This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets and fax machines.

PHYSICAL DEMANDS
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
While performing the duties of this job, the employee is regularly required to talk or hear and participate in conference calls
While performing the duties of this job, the employee is regularly required to type at a keyboard and produce working computer software.
The employee is occasionally required to sit; balance; and stoop, kneel, or crouch. The employee must frequently lift and/or move up to 10 pounds and occasionally lift and/or move up to 25 pounds. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception and ability to adjust focus.

EQUAL EMPLOYMENT OPPORTUNITY (EEO) STATEMENT
We are an equal opportunity employer and value diversity in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",4.2,"Potrero Medical Inc.
4.2","Hayward, CA",-1,51 to 200 Employees,2015,Company - Private,Health Care Products Manufacturing,Manufacturing,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"Senior Data Engineer

Sunnyvale, US- Full Time

LotusFlare is a product company with engineering offices in Silicon Valley, Belgrade and Kiev. Our solution is adopted by leading telecom providers and lifts their product infrastructure into the digital age. We are replacing traditional business backends with an engagement-centric dynamic product stack. Our cloud-native SaaS platform running on leading public clouds as well as supporting on-premise private and hybrid clouds is based on latest technologies picked from the CNCF stack.

LotusFlare, founded by early product and growth staff from Facebook, is backed by leading investors including Social Capital and Google Ventures. LotusFlare provides a platform for telecom operators to create a 100% digital customer experience where subscribers can choose and port a mobile number, order SIMs, and devices, track shipping, choose and purchase plans, discover and consume content, pay bills, receive loyalty rewards, and access customer service. LotusFlare also provides a Growth platform to drive user acquisition, engagement, and monetization on digital products. LotusFlare's clients include leading companies such as Verizon Wireless, Telenor, Ooredoo, Singtel, Maxis, Globe, LinkedIn, and Skype.

RESPONSIBILITIES

As Senior Data Lead Engineer on our team, you would be responsible for designing, building a shipping and maintaining our critical data platform. In this role, you will lead the development of our Data Warehouse/Data Lake mentoring our cross-functional team of engineers and Data Scientists. Besides architecting and implementing you would also be responsible for collaborating with our Growth/Marketing teams to provide useful insights using various ML driven models.

REQUIREMENTS

BA/BS in Computer Science or other equivalent technical disciplines
You have strong programming skills in Java/Scala or equivalent
5+ Years of experience creating ETL pipelines using Spark/Hadoop/Kafka/ClickHouse
2+ Years experience leading and/or mentoring junior engineers
Experience writing complex SQL and ETL processes
Hands-on Experience with AWS
Knowledge of ClickHouse, Apache Spark, Kafka, and similar technology stacks
Machine learning expertise is a plus

Benefits

Competitive base salary and stock options
Full medical/dental/vision package
Generous vacation time and paid holidays

Perks

High impact work; influence the strategic direction of the company
Access to the latest stacks, modernized tools
Accelerated career growth and opportunity
Sharp, motivated co-workers in a flexible office environment

Powered by JazzHR

LUmFcmxiw1",4.0,"LotusFlare, Inc.
4.0","Castro Valley, CA",-1,51 to 200 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Data Engineer Bachelor's (Intern) - United States,-1,"Who You Are: Data Engineers are focused on enabling a data driven approach to optimization by sourcing, maintaining and ensuring the availability of data used to drive full life cycle marketing insights to optimize CX’s marketing investments and the customer experience. They integrate both batch and streaming approaches to support standard business intelligence, as well as decision automation and machine learning requirements. Data engineers help make data much easier to understand and consume for others and have led to pivot to new technologies rapidly. What you’ll do: Provide leadership and support for development of an integrated Customer Experience data foundation that will enable extensive business intelligence and machine learning for CX Platform and extended user communities. Work with CX business and IT teams to ensure high quality, on-time deliverables that meet usability, scalability, quality and performance standards. Provide hands-on technical support for development, research and quality assurance testing. Apply a variety of technologies to ingest, transform, index, aggregate, correlate, provide API's, visualize and enable a spectrum of organizations across Cisco. Analyze structural requirements for data storage solutions and software Design architectures for technical systems as to ensure robustness, scalability, and completeness We are seeking high energy and qualified candidates who possess the following skills and experience: You're actively pursuing a bachelor’s or master’s degree in computer science, Mathematics, Engineering, Statistics, or related field Experience working with large datasets Able to legally live and work in the country for which you’re applying, without visa support or sponsorship Who you'll work with: You will work cross functionally with the Customer Experience, Sales, Marketing, and IT organizations, playing a leadership role in transforming Cisco by developing and implementing analytic models and intelligent automation to drive us toward a data-driven digital organization. Digital Lifecycle Journey’s (DLJ) digital expertise makes us uniquely qualified to address the evolving expectations of today’s connected customers and partners, along with Cisco sellers. Using real-time connected data, machine learning, and automation; the team enables Cisco sellers and partners to deliver a powerful, personalized experience—throughout the entire customer lifecycle. DLJ is focused on providing customers with an immersive digital experience with Cisco. This in turn drives improvements in recurring revenue, cost savings and sales effectiveness for Cisco and its partners. Why Cisco #WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference powering an inclusive future for all. We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (36 years strong) and only about hardware, but we’re also a software company. And a security company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box! But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.) Day to day, we focus on the give and take. We give our best, give our egos a break, and give of ourselves (because giving back is built into our DNA.) We take accountability, bold steps, and take difference to heart. Because without diversity of thought and a dedication to equality for all, there is no moving forward. So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!",4.2,"Cisco Systems
4.2","San Jose, CA",-1,10000+ Employees,1984,Company - Private,Computer Hardware & Software,Information Technology,$10+ billion (USD),-1
"Data Engineer, Analytics (Product Foundation)",$118k-$192k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Our more experienced data engineers are clearly characterized by in-depth technical experience and proven progression in leadership responsibility. If you have an interest in being responsible for the dynamics of a fast-paced environment, this is the right role for you. You will be working on many projects at a time, but also focused on the details while finding creative ways to pursue big picture challenges.
Able to immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions
Craft optimal data processing architecture and systems for new data and ETL pipelines
Recommend improvements and modifications to existing data and ETL pipelines
Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership
Drive internal process improvements and automating manual processes for data quality and SLA management
Provide ongoing proactive communication and collaboration throughout the organization
4+ years experience in the data warehouse space
4+ years experience working with either a MapReduce or an MPP system
4+ years experience with object-oriented programming languages
7+ years experience in writing SQL and ETL processes
BS/BA in Technical Field, Computer Science or Mathematics
Knowledge in Python or Java
Experience analyzing data to identify deliverables, gaps, and inconsistencies
Ability to effectively collaborate and communicate complex technical concepts to a broad variety of audiences
Experience actively mentoring team members in their careers
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Data Engineer with Azure & Data Warehouse experience,-1,"Skillset reqd*
· 6+ years of overall experience with background in Datawarehousing/BI applications

· Data engg and cloud exposure incl Azure;

· We dont need a Data Scientist; Need 4-5 years experience in areas like Python, Rest API, microservices in Azure / AWS

· Experienced in Deploying ML algorithms in prod either on-prem or on cloud

Job Type: Contract

Pay: $40.00 - $42.00 per hour

Schedule:
Monday to Friday
Experience:
Data warehouse: 2 years (Preferred)
Azure: 1 year (Preferred)
BI tools: 1 year (Preferred)
Location:
Santa Clara, CA (Preferred)
Work authorization:
United States (Preferred)
Contract Length:
5 - 6 months
1 year
Full Time Opportunity:
No
Work Location:
One location
Work Remotely:
Temporarily due to COVID-19",-1,CA-One Tech Cloud Inc,"Santa Clara, CA",-1,51 to 200 Employees,2017,Company - Private,IT Services,Information Technology,$10 to $25 million (USD),-1
Sr. Data Engineer,$114k-$196k (Glassdoor Est.),"It takes powerful technology to connect our brands and partners with an audience of 1 billion. Nearly half of Verizon Media employees are building the code and platforms that help us achieve that. Whether you’re looking to write mobile app code, engineer the servers behind our massive ad tech stacks, or develop algorithms to help us process 4 trillion data points a day, what you do here will have a huge impact on our business—and the world. Want in? As Verizon’s media unit, our brands like Yahoo, TechCrunch and HuffPost help people stay informed and entertained, communicate and transact, while creating new ways for advertisers and partners to connect. With technologies like XR, AI, machine-learning, and 5G, we’re transforming media for tomorrow, too. We're creators and coders, dreamers and doers creating what's next in content, advertising and technology. A Lot About You You get people. You have a unique blend of skills in developing deep consumer insights and competitive intelligence through data that drives product innovation to create the right experiences for people’s daily lives that achieve our goals to acquire, engage, and retain them. You get data. You have a thirst for knowledge and insight. You thrive and strive to present data in ways that product, design, engineering, marketing, and executive teams understand and act upon. Your data is 100% accurate and credible. Your reports are always clear and actionable. You get growth. You are a consumer-focused, data-driven, and growth-enabling analyst who has supported Growth strategies, roadmaps, scrums, and final product rollouts, across the analytics/insights, acquisition/referrals, activation/onboarding, and adoption/retention loop. You get mobile/digital. You have significant industry experience – and a strong understanding of the mobile/digital ecosystem – from apps to advertising and analytics. You have successfully applied the latest mobile/digital tools to help drive reach, retention, and revenue growth. You get it done. You have successfully worked with product, design, engineering, marketing, and executive teams to understand requirements, translate business needs into data requests, develop methodologies/plans, analyze data, and present findings that are embraced/enacted. Responsibilities: Understand the marketplace trends and help answer revenue trends Analyze supply as well as demand patterns and find revenue opportunities, explain model behaviors, suggest improvements etc. Gain insights on what drives performance in terms of reach and revenue growth Create dashboards and reports that provide analysis and commentary, explaining product, sales, and business trends for Executive reporting Work closely with product and inform and update stakeholders on product performance, plans, and progress towards metrics Define data testing plans and create methodologies that help teams to iterate fast and release new features for testing and, if successful, rollout to all users globally Generate and go deep on consumer insights and competitive intelligence to help teams drive product innovation and iteration Build strong partnerships with product, sales, engineering, and marketing teams and enable them to launch new Growth initiatives for testing/iteration Provide feedback to product, sales and engineering teams on impact of product launches: target launch metrics, A|B testing, post-launch metrics Investigate data and monitor data quality – partner closely with and provide requirements to the Data Engineering teams that can be clearly acted upon Frame business problems into questions that can be answered through data analysis, and translate business needs into requirements Requirements: BS/MS in highly-quantitative field (Analytics, Computer Science, Mathematics) is preferred Data analysis, generating insights for consumer-focused products B2B or advertising experience is a must Experience with big data technologies such as Hive, Hadoop, MapReduce, Spark, PIG etc. Experience with scripting programming languages such as Perl/Python is good to have Familiarity with Unix/Linux environment highly recommended Significant experience, proficiency in, and passion for Mobile and/or Web products Track record of proactively establishing and following through on commitments Demonstrated use of analytics, metrics, and benchmarking to drive decisions Excellent interpersonal, organizational, creative, and communications skills Team player in driving growth results combined with a positive attitude Strong work ethic and strong core values (honesty, integrity, creativity) Problem solver who never stops thinking about ways to improve Verizon Media is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category. Verizon Media is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. If you need accessibility assistance and/or a reasonable accommodation due to a disability, please submit a request via the Accommodation Request Form ( https://www.verizonmedia.com/careers/contact-us.html ) or call 408-336-1409. Requests and calls received for non-disability related issues, such as following up on an application, will not receive a response. Currently work for Verizon Media? Please apply on our internal career site.",3.6,"Verizon Media
3.6","Sunnyvale, CA",-1,10000+ Employees,-1,Subsidiary or Business Segment,-1,-1,Less than $1 million (USD),-1
Senior Data Engineer,$106k-$189k (Glassdoor Est.),"About Bill.com Bill.com is a leading provider of cloud-based software that simplifies, digitizes, and automates complex, back-office financial operations for small and midsize businesses. Customers use the Bill.com platform to manage end-to-end financial workflows and to process payments, which totaled over $70 billion for fiscal 2019. The Bill.com AI-enabled, financial software platform creates connections between businesses and their suppliers and clients. It helps manage cash inflows and outflows. The company partners with several of the largest U.S. financial institutions, more than 70 of the top 100 U.S. accounting firms, and popular accounting software providers. Bill.com has offices in Palo Alto, California and Houston, Texas. For more information, visit www.bill.com or follow @billcom. Bill.com moves over $70B per year and we have over 10 years worth of customer data. We are leveraging this data to make data driven decisions, and apply data science and machine learning to solve a variety of tough problems. We are in the middle of a large-scale transformation to the public cloud and are developing data pipelines, data warehouse, and machine learning infrastructure in AWS. Data engineers at Bill.com will be responsible for building data pipelines and the infrastructure to enable data science, data analytics, and machine learning at scale in AWS. Some of the problems we are currently working on include: detecting payment fraud, extracting semantic data from customer documents, and increasing customer acquisition through advanced analytics. Data engineers will own and build the data platform that makes all of this possible. Professional Experience/Background to be successful in this role: 5+ years of experience owning and building data pipelines. Extensive knowledge of data engineering tools, technologies and approaches Ability to absorb business problems and understand how to service required data needs Design and operation of robust distributed systems Proven experience building data platforms from scratch for data consumption across a wide variety of use cases (e.g data science, ML, scalability etc) Demonstrated ability to build complex, scalable systems with high quality Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases. Experience with specific AWS technologies (such as S3, Redshift, EMR, and Kinesis) a plus Experience in SQL and one or more of Python, Java and Scala Experience with containerized workloads, Kubernetes and infrastructure-as-code principles a big plus Expected Outcomes Design and implement data infrastructure and processing workflows required to support data science, machine learning, BI and reporting in AWS Build robust, efficient and reliable data pipelines consisting of diverse data sources Design and develop real time streaming and batch processing pipeline solutions Own the data expertise and data quality for the pipelines Drive the collection of new data and refinement of existing data sources Identify shared data needs across Bill.com, understand their specific requirements, and build efficient and scalable pipelines to meet various needs Build data stores for feature variables required for machine learning Bill.com is committed to a policy of equal employment opportunity. We recruit, employ, train, compensate, and promote without regard to race, color, age, sex, ancestry, marital status, religion, national origin, disability, sexual orientation, veteran status, present or past history of mental disability, genetic information or any other classification protected by state or federal law. Bill.com Culture: Humble – No ego Fun – Celebrate the moments Authentic – We are who we are Passionate – Love what you do Dedicated – To each other and the customer Our Applicant Privacy Notice describes how Bill.com treats the personal information it receives from applicants.",4.2,"Bill.com
4.2","Palo Alto, CA",-1,501 to 1000 Employees,2006,Company - Public,Financial Transaction Processing,Finance,$50 to $100 million (USD),-1
Senior Data Engineer,$109k-$195k (Glassdoor Est.),"Company Description Guardant Health is a leading precision oncology company focused on helping conquer cancer globally through use of its proprietary blood tests, vast data sets and advanced analytics. Its Guardant Health Oncology Platform is designed to leverage its capabilities in technology, clinical development, regulatory and reimbursement to drive commercial adoption, improve patient clinical outcomes and lower healthcare costs. In pursuit of its goal to manage cancer across all stages of the disease, Guardant Health has launched multiple liquid biopsy-based tests, Guardant360® and GuardantOMNI® , for advanced stage cancer patients, which fuel its LUNAR program, which aims to address the needs of early stage cancer patients with neoadjuvant and adjuvant treatment selection, cancer survivors with surveillance, asymptomatic individuals eligible for cancer screening and individuals at a higher risk for developing cancer with early detection. Since its launch in 2014, Guardant360® has been used by more than 7,000 oncologists, over 50 biopharmaceutical companies and all 27 of the National Comprehensive Cancer Network centers. Job Description The Data Platform team provides an enriched and valuable ecosystem of data sources and data services that drive innovation for internal and external systems. This team is dedicated to developing advanced technology (Big Data , Cloud, Machine Learning), systems and services to make data secure, rich, high quality, and fast therefore enabling Guardant the ability to leverage its data assets in an effective and timely manner to maximize technology/business development in the extraordinarily complex oncology diagnostic and therapeutic landscape. We connect patients with clinical trials, help clinicians order our test and receive our clinical reports, and deliver valuable genomic datasets to researchers to help uncover important insights into treatment paradigms and drug discovery. Our technology stack reflects our views of using the best tools for the job, employing Scala, Java, Python along with Kubernetes, Apache Spark, Presto, Kafka, Docker, MySQL, MongoDB and a variety of AWS services to analyze and disseminate vast volumes of genomic data. Data Acquisition: Utilize expert coding skills to build real-time distributed and reliable data pipelines that ingest and process data at scale. Data Architecture: Expertise in designing and building big data systems, data lakes; can translate the needs of the business to productize models and data visualizations into a very functional data architecture; partners with Healthcare Intelligence. Data Validation / Accuracy: Develop quality checks to ensure data accuracy and integrity; recommend process improvements that enhance data integrity; ensure ongoing data integrity and performs skillful data validation. Reporting / Analysis: Work independently with senior leaders to tackle complex problems by developing sophisticated, testable hypotheses; presents findings formally to diverse stakeholders and committees; meaningfully identifies opportunities for improvement that result in change. Display / Visualization: Proficient with data visualization tools; develop visualization concepts; deliver excellent visual storytelling; solve complex technical challenges. Clinical Data Expertise: Strong analytic resource in clinical subject areas with good understanding of the characteristics of data in sources including the EDW and the Data Lake. Qualifications Qualifications: 10+ years of software development experience Minimum 4 years of experience on Big Data Platform Excellent experience with programming languages such as Scala and Java Strong experience coding with streaming/micro-batch compute frameworks, preferably Spark Work collaboratively with business, bioinformatics scientists and translates business requirements into enterprise information architecture Drive the architecture of data integration from various clinical application and stores, research databases and external sources Develop the processes for updating and maintaining terminologies, and vocabularies including mapping from local to international standards when applicable Strong knowledge of statistics, data analysis and databases Strong hands on skills in SOLR querying and Indexing, configuring schema, understanding in advanced schema fields, deciding commit strategies and tuning the relevancy of search results Flair for data, schema, data model, how to bring efficiency in big data related life cycle Expertise in designing and building data warehouses in Big data systems, dimensional data models and strong hands-on SQL knowledge Understanding of automated QA needs related to Big data Understanding of various Visualization platform (Tableau, D3JS, others) Proficiency with agile or lean development practices Strong object-oriented design and analysis skills Strong aesthetic sensibility that supports clear visual communication of quantitative information Experience with application performance monitoring and assessment desired Knowledge of healthcare including Clinical terms and concepts is a plus Experience with managing data in regulated healthcare environment (HIPAA compliant) is a plus Bachelor’s degree in Computer Science or related area Top skill sets / technologies in the ideal candidate: Programming languages - Java (required), Scala, Python, R Databases - Oracle, complex SQL queries, performance tuning concepts, AWS RDS, Apache Presto, RedShift; NoSQL - HBASE, MongoDB, Cassandra Batch processing - Hadoop MapReduce, Apache Spark, AWS EMR Stream processing - Spark streaming, Apache Storm, Flink ETL Tools - Data Stage, Informatica, Nifi Code/Build/Deployment - GIT, SVN, Maven, SBT, Jenkins, Bamboo Additional Qualifications: You have strong knowledge and experience addressing a broad range of accounting matters, ensuring it is processed in compliance with established internal controls. You possess analytical skills needed to correctly grasp and communicate data, and analyze and reconcile accounts; ability to handle confidential and sensitive information with the appropriate discretion; and handle multiple deadlines. You are a self-starter, work well as a team player, but can work independently when appropriate. You possess the ability to analyze problems and actively strategize to resolve them, pay attention to detail, and have excellent organization and communication skills. You are results oriented. You can juggle multiple tasks, work cross-functionally and at all levels of the organization, whether internally or externally. You are flexible and comfortable in a dynamic, fast-paced environment and can prioritize to focus on the important, not just the urgent. Additional Information Guardant Health is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. All your information will be kept confidential according to EEO guidelines. Please visit our career page at: http://www.guardanthealth.com/jobs/ To learn more about the information collected when you apply for a position at Guardant Health, Inc. and how it is used, please review our Privacy Notice for Job Applicants. #LI-LI1",3.8,"Guardant Health
3.8","Redwood City, CA",-1,501 to 1000 Employees,2013,Company - Public,Biotech & Pharmaceuticals,Biotech & Pharmaceuticals,Unknown / Non-Applicable,-1
Data Engineer in Security,$81k-$142k (Glassdoor Est.),"ByteDance AI Lab is looking for a Data Engineer. You will be responsible for designing, developing and building a robust data pipeline infrastructure required for optimal extraction, transformation, loading of data from a wide variety of data sources that allow for the build of our security analytics system. Also, you will have opportunities to learn and apply cutting-edge AI technologies to solve real-world security issues in the live enterprise environment. You will take ownership for large software components, help in the design of the architecture, apply your knowledge to functional design, utilize your programming skills for efficient and robust implementation, and guarantee the quality assurance in the whole software development cycle. Responsibilities 1. Design, construct, test and maintain robust, reliable, and scalable data pipeline infrastructure. 2. Ensure all systems meet the project requirements as well as industry practices. 3. Investigate and integrate up-and-coming big data technologies into existing requirements. 4. Recommend different ways to constantly improve data reliability and quality. 5. Install/update system and component fault-tolerant procedures. 6. Employ an array of technological tools to integrate with 3rd-party data systems. 7. Research new uses for existing data. 8. Develop set processes for data mining, data modeling, and data production. 9. Collaborate with members of your team (e.g., data architects, data scientists, security researchers) on the project’s goals. 1. B.S. in Computer Science or equivalent experience, M.S./Ph. D. a plus 2. 5+ years of experience in software development 3. 3+ years of experience in developing robust and reliable big data infrastructure in scale, especially in supporting inline data ingestion, correlation, and aggregation from multiple data sources 4. Familiar with large data processing/storage tools such as Kafka, Flink, Spark, ElasticSearch, Redis, Hbase, Cassandra, Druid, etc. 5. Able to troubleshoot system-level integration and performance issues 6. Experience with automation/scripting/coding 7. Fast learner and eager to absorb new emerging technologies 8. Enjoy working with other team members with strong collaboration and communication skills The following experiences are a plus: 1. Experience with information security: such as malware analysis, networking intrusion detection, botnet detection, etc. 2. Experience with incident triage and response, e.g, analyzing firewall and device logs, investigating security events, protecting the forensic value of data, and establishing monitoring and incident reporting and response procedures 3. Experience implementing networking packet processing product in the enterprise environment 4. Experience with Cloud platforms such as GCP and AWS 5. Experience with Docker & Kubernetes ecosystems",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Senior Cyber Data Engineer (Remote/Virtual),$153k-$271k (Glassdoor Est.),"What’s your mission?

IronNet’s mission is simple: To deliver the power of collective cybersecurity to defend companies, sectors, and nations. For decades, companies have been defending against cyber attacks on their own while adversaries have been organizing themselves into sophisticated hacker networks … until now, with IronNet Collective Defense. In 2014, General (Ret) Keith Alexander, former Commander U.S. Cyber Command, launched IronNet to strengthen cybersecurity defense against highly sophisticated adversaries, across all borders and sectors.

In response to cyber adversaries who increasingly collaborate for collective offense, leading organizations in our critical infrastructure are using collective defense strategies and solutions to meet these powerful and ever-changing threats. We believe that collective defense is our collective responsibility and we are leading the charge.

The Opportunity

IronNet’s Analytics team is responsible for building behavioral analytics and event correlation code for our NDR and collective defense products. The team focuses on developing cost effective cloud solutions that are distributed and highly scalable, processing large volumes of events.

We are looking for a senior level Cyber Data Engineer with focus on Spark / Scala analytics and development of SaaS solutions to join the team.

To be successful in this role, you must be able to . . .
Use modern programming languages (Python, Scala, Java, Golang, etc.) to develop continuously integrated and deployed production software
Be able to understand, improve, and potentially re-factor ETL and data analytics software written in Spark (Scala)
Interface data transformation, enrichment, and machine learning algorithms to cloud storage and messaging infrastructure
Use AWS to provide microservices architectures that are highly reliable, redundant and scalable
Implement horizontally scalable analytics and data processing pipelines
Turn proof of concept architectures into production environments
Share knowledge and assist others in understanding technical topics
You may be the person we need if your background aligns with the following . . .
Have proven experience as a Data Engineer, Machine Learning Engineer, Software Engineer, Cloud Engineer or similar role.
Demonstrable expertise in a modern programming language (Python, Scala, Java, Golang, etc.)
Experience with working with data scientists to develop machine learning algorithms that run on an ongoing basis, such as within an ETL process, rather than once, or on-demand.
Experience with Kafka, HDFS, AWS EMR and Glue, and other scalable data frameworks
Experience with cybersecurity event processing including high-volume ETL and analytics
Possess strong analytical, technical, and problem-solving skills.
Personal Profile:
Passion for championing projects from concept to delivery to customer.
Competitive spirit; willingness and ability to “sell” your solution during collaborative team discussions.
Desire to be the best and prove it every day.
Eagerness to learn and improve your own skills and to make those around you better.
Highly attentive to detail and a focus on improving the code base and quality of our tests.
Commitment and aptitude to proactively find solutions to ambiguous opportunities.
Bring a unique skill set or elevate the results of the teams you are a part of.
Recognition & Awards

IronNet is recognized as a representative vendor in Gartner’s “Market Guide for Network Detection and Response (NDR)”, and Forrester recently named IronNet a representative vendor in its “Now Tech: Network Analytic and Visibility, Q2, 2020” research.

Recent Awards:

CRN Emerging Vendors

Fortress Cyber Security

Hot 150 Cybersecurity Companies

Fortress Cyber Security

EMA Vendor To Watch

CRN Security100

More About IronNet:

IronNet delivers unmatched collective cyber threat detection for enterprise on-premise, cloud, and hybrid networks. We do this through the application of advanced behavioral analytics, AI, and machine learning techniques. Our team combines the tradecraft knowledge of the best offensive and defensive cyber operators in the world with world-class mathematicians and data scientists to engineer solutions that empower companies to defend against advanced threats.

Our founder and Co-CEO, General (Ret) Keith Alexander, is a recognized cybersecurity innovator and a frequent speaker about current cyberthreats and effective defenses. We have a leadership team with deep government and commercial cyber experience, and the company is advised by a board of esteemed security and venture investment professionals, including Jan Tighe Retired Vice Admiral, Former Deputy Chief of Naval Operations for Information Warfare and Director, Naval Intelligence, US Navy; and Jack Keane Chairman, Institute for the Study of War, Retired Four-Star General, Former Vice Chief of Staff, US Army.

Benefits of Working at IronNet:

IronNet strives to provide and takes pride in being able to offer comprehensive, essential and affordable benefits for our employees and their families. We offer an unlimited PTO plan, 401(k) match as well as Medical, Dental, Vision, and Disability Insurance.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or protected veteran status, or any other legally protected basis, in accordance with applicable law.

Follow us on LinkedIn

#LI-REMOTE

#LI-MG1",4.2,"IronNet Cybersecurity
4.2","San Francisco, CA",-1,201 to 500 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Data Engineer,$129k-$235k (Glassdoor Est.),"Carbon Black is now part of VMware. As a standalone company, Carbon Black established itself as a leader in the endpoint security space. The product portfolio includes the rapidly growing Carbon Black Cloud platform that delivers next-generation endpoint protection capabilities from the cloud. Now with the full resources of VMware, you have the opportunity to make an impact and build upon Carbon Black’s success. VMware Carbon Black engineering group is looking to hire a data engineer who loves working on complex problems and getting things done. The ideal candidate combines excellent engineering skills and passion to work with Big data . As a Senior data engineer, your day-to-day tasks will be the following. Design, implement, and maintain end-to-end data pipelines with an understanding of ML lifecycles • Drive development of data products in collaboration with data scientists and analysts. Build and scale data platform infrastructure that powers analytics both batch and real-time. Working with stakeholders and other team members to deliver and operate large scale, distributed services in the cloud Responsible for overall system architecture, scalability, reliability, and performance of one or more data pipelines and data platform components. Provide Architecture guidance and work closely to uplevel the engineering organization. Actively participate and improve the data engineering craftsmanship and follow the best practices. Review others' work and share knowledge. Skill Set Needed :: 6-8 years of experience in relevant areas in building large scale, distributed systems which can handle 10s of petabytes of data spread across multiple regions Experience in big data technologies. (Spark, Flink, MapReduce, HDFS, Hive, Presto, Avro, Parquet, Airflow, etc) Experience in large scale messaging system (Kinesis, Kafka, etc) Experience in the public cloud (For example S3, RDS, Athena, VPCs, etc) Practical experience in building solid, distributed, internet-scale enterprise-class solutions/services in Java, Python, Golang Experience with big data design, ETL, technology, efficient designs for distributed systems, and big data environments. Experience in both On-prem and in public cloud AWS. Experience in Data warehousing Ability to analyze and debug SQL, NoSQL knowledge is a plus Experience in building data platform infrastructure, tooling, and automation is a plus. Experience in the Security domain is a plus. Category : Engineering and Technology Subcategory: Software Engineering Experience: Manager and Professional Full Time/ Part Time: Full Time Remote: No Posted Date: 2020-10-01 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",3.6,"Carbon Black
3.6","Palo Alto, CA",-1,1001 to 5000 Employees,2002,Company - Public,Computer Hardware & Software,Information Technology,$100 to $500 million (USD),-1
"Data Engineer, Analytics (Platform)",$118k-$192k (Glassdoor Est.),"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.

Our more experienced data engineers are clearly characterized by in-depth technical experience and proven progression in leadership responsibility. If you have an interest in being responsible for the dynamics of a fast-paced environment, this is the right role for you. You will be working on many projects at a time, but also focused on the details while finding creative ways to pursue big picture challenges.
Able to immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions
Craft optimal data processing architecture and systems for new data and ETL pipelines
Recommend improvements and modifications to existing data and ETL pipelines
Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership
Drive internal process improvements and automating manual processes for data quality and SLA management
Provide ongoing proactive communication and collaboration throughout the organization
4+ years experience in the data warehouse space
4+ years experience working with either a MapReduce or an MPP system
4+ years experience with object-oriented programming languages
7+ years experience in writing SQL and ETL processes
BS/BA in Technical Field, Computer Science or Mathematics
Knowledge in Python or Java
Experience analyzing data to identify deliverables, gaps, and inconsistencies
Ability to effectively collaborate and communicate complex technical concepts to a broad variety of audiences
Experience actively mentoring team members in their careers
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",4.5,"Facebook
4.5","Menlo Park, CA",-1,10000+ Employees,2004,Company - Public,Internet,Information Technology,$5 to $10 billion (USD),-1
Sr. Data Engineer - Finance,$130k-$240k (Glassdoor Est.),"Company Description

Our Mission

At Palo Alto Networks®, everything starts and ends with our mission:

Being the cybersecurity partner of choice, protecting our digital way of life.

We have the vision of a world where each day is safer and more secure than the one before. These aren’t easy goals to accomplish — but we’re not here for easy. We’re here for better. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.

Job Description

Your Career

This role will be responsible for expanding and optimizing our data and data pipeline architecture across Finance, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. This role will work on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. In addition, this role will act as an interface between business and technical teams to understand business needs, define requirements and implement new business processes, tools, and systems. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. This role will report to the Director of Financial Systems.

Your Impact

Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Problem solving and analytics: Understand business performance through dashboards and data that you help create or gather. Frame issues in a compelling way and drives resolution to them, providing focused quantitative or qualitative analysis and research support as input into decision making
Partner with internal business teams in developing delivery roadmaps, defining business processes and associated system requirements
Participate in system implementation projects (e.g. requirements gathering, documentation, systems configuration, test documentation/execution, issue identification and resolution)
Architect innovative solutions to automate and scale business systems

Qualifications

Your Experience

Bachelor's degree in Business Administration or Computer Science, or a related technical discipline, or equivalent practical experience
6+ years of related experience including strong functional and technical knowledge
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency, and workload management
A successful history of manipulating, processing, and extracting value from large disconnected datasets
Experience with implementing and supporting business systems for MDM, Data lakes, Reporting and Planning systems
Effective communication skills, with the ability to present technical information in a clear and concise manner to executives and non-technical leaders
Highly self-motivated, able to manage multiple projects with little day to day direction. Able to both lead and support
Exceptional analytical problem-solving ability
High level of professionalism, passion, and sense of urgency
Experience in documenting data definitions (metrics and dimensions) in a business-friendly language and provide logical data models

Additional Information

The Team

You have a passion for numbers, our organization has a passion for cybersecurity. Are you looking for a career with a more fulfilling mission? We have open positions to top talent seeking a financial challenge. Our finance department deals with numbers daily, supporting infrastructure, dealing in automation, and building elegant models to empower our business lines and solutions. The ideal candidate has exceptional skills in financial modeling, data analytics and an innovative mindset to approach finance problems differently. Our innovation doesn’t stay in Research and Development. Within finance, we seek people who are looking to try new things, while solving business critical equations. If you’re seeking a financial challenge but with a world-wide impact – this is it.

Our Commitment

We’re trailblazers who dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: We can’t accomplish our mission without diverse teams innovating, together.

We are committed to providing reasonable accommodations for all qualified individuals with disabilities. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.

Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.

#CT1",3.4,"Palo Alto Networks
3.4","Santa Clara, CA",-1,5001 to 10000 Employees,2005,Company - Public,Enterprise Software & Network Solutions,Information Technology,$2 to $5 billion (USD),-1
Principal Data Engineer,$177k-$324k (Glassdoor Est.),"The Principal Data Engineer will be based in our San Jose headquarters, USA.

What you’ll do

The Principal Data Engineer will participate in the newly formed Cloud Platforms organization position will work closely with all internal stakeholders such as Finance, Product Architecture, Product Management, Program Management, Engineering, and Operations. This individual will need to be a leader, self-motivated, an assertive communicator, one who takes ownership, as well as providing support and direction to level I & II Data Engineers, handling more advance functions within the department. A Data Engineer at the highest tier, is able to influence business initiatives by presenting engaging opportunities while using their solid technical background and experience in Big Data and Cloud technology to drive the opportunity to success.

Your responsibilities

Be up-to-date with emerging trends Big data and cloud technologies and participate in evaluation of new technologies to improve our data processing architecture to improve our data processing architecture to improve our data processing architecture. Presents substantive technical recommendations to senior management
Independently lead data projects and deliver comprehensible solutions to improve business outcomes
Mentor and provide direction in architecture and design to developers
Help Architect and Implement a highly scalable, available, fault-tolerant data processing systems such as Hadoop, BigQuery, Redshift, Kafka, Spark, ELK
Work closely with engineers and architect to deploy models in production both in real time and in batch process and systematically track model performance.
Full understanding of the customer’s infrastructure and how ActiveVideo’s software is to be integrated with both AppCloud SaaS Solutions and CloudTV Product solutions.
Design, develop, and operate efficient and reliable data pipelines to move data and to provide intuitive analytics to our partner teams.
Continual improvements towards better and more efficient data collection and usability.
Own Reporting Data Integrity - Evaluate data accuracy, completeness, timeliness and stability to establish repeatable, sustainable processes to maintain.
Diagnose and solve higher level issues in our existing data pipelines and envision and build their successors.
Work within an Agile work-flow environment, utilizing Jira, Confluence, Git, Office365, Zoom, etc.

What you’ll need

Strong communication and leadership skills
Minimum of 4 years Cloud Data Analytics experience with sizing, creating, and creating systems using AWS, Google Cloud, Microsoft Azure
Minimum of 4 years of experience administering and architecting virtual machine systems with ESXi, KVM, Ovirt or Openstack
Minimum of 10 years of experience in technical industry
Hands-on data/software engineering experience. You are able to build applications and are familiar with best practices such as CI/CD, testing and coding standards
SQL, Python and/or Scala have no secrets for you, especially in the context of data processing and data applications
Knowledge of modern data technologies, tools, platforms and infrastructures, such as Spark, Kafka, Airflow and Kubernetes
Database knowledge and administration, in one more of: MYSQL, Oracle, Data Warehouse, Data mart and/or ETL
Some experience with frontend and backend server development & Android is bonus
Reporting and data formatting experience (ie:, Google DataStudio,, AWS Quicksight)
Ability to analyze and communicate an action plan to outline synthesized data and resolve challenges found through analysis
Expert level Excel, PowerPoint, Word and Visio/Lucid Chart
Background in video content, major cable, or communications network industry is bonus

Who are we?

ActiveVideo is the developer of CloudTV, the only cloud-based software platform that enables service providers, content aggregators and CE manufacturers to virtualize CPE functions in the cloud for the purposes of delivering next-gen user interfaces, online content, and interactive advertising for TV to millions of set-tops and connected devices.

CloudTV enables service providers, content aggregators and CE manufacturers to deliver consistent, branded video entertainment experiences from the cloud to millions of devices. CloudTV overcomes browser incompatibilities, eliminating the need to develop and maintain native, device-specific applications. ActiveVideo is based in San Jose, the heart of Silicon Valley, with offices in Los Angeles and Hilversum, the Netherlands.

What do you get for this?

You’ll become part of a team consisting of strong, delivery minded technologists. The culture is highly informal and our structure flat. This enables our employees to be flexible and to research their ideas. And of course we offer a competitive package.

Who to contact

If you are you interested and enjoy working hard and having fun, please send your resume and motivation to T.Mai@activevideo.com for the attention of Tam Mai, HR Manager (US). Please indicate job title in the subject line.",3.4,"ActiveVideo
3.4","San Jose, CA",-1,51 to 200 Employees,1988,Company - Private,Telecommunications Services,Telecommunications,$25 to $50 million (USD),-1
Senior Big Data Engineer - Remote,$123k-$217k (Glassdoor Est.),"Senior Big Data Platform Engineer Join us as we build a next-generation Enterprise Analytics Platform to reengineer a real estate enterprise. As a Senior Big Data Platform Engineer, you will be joining our Enterprise Analytics Platform Team to build EAP platform from ground up to serve agents, brokers, home buyers and sellers. What we’re looking for: You’re a talented, creative, and motivated Data Platform Engineer who loves to build data platform tools. Be ready to work with a team of individuals who share your passion. With a related degree and relevant experience you’re ready to take your programming and data knowledge to the next level. You enjoy building tools such as Data Ingestion Platform to ingest data from diverse sources to Data Lake, building data lake tools to ingest data, monitor pipelines, and grant access to data. With your commitment to quality, excellent data/cloud skills, and collaborative work ethic, you’ll do great things here at Realogy AI Lab. What you’ll do: You’ll be responsible for designing and building high performance, scalable data solutions that meet the needs of millions of agents, brokers, home buyers, and sellers. You’ll work with other Data Engineers for build out of Next Generation Data Ingestion Platform. You’ll design and develop data ingestion pipelines for batch and real-time streaming of data from in-house OLTP systems and third-party data. You’ll work with team to design and develop Data Lake to store and process 10s of terabyte of data. You’ll work with team to design Data Lake CLI to manage Data Lake Storage and Access. You will design and develop ETL pipelines to process data in data lake for descriptive and prescriptive reporting. You’ll develop ETL data pipelines to build Enterprise Data Models for Property, Agent, Broker, office and other master entities. You will design and develop CI/CD process for continuous delivery in AWS Cloud. You’ll design, develop, and test robust, scalable data platform components. You’ll work with a variety of teams and individuals, including product engineers to understand their data pipeline needs and come up with innovative solutions. EDUCATION AND EXPERIENCE/SPECIAL SKILLS/TECHNOLOGIES/TOOLS REQUIREMENTS: When using experience as a requirement, please quantify the number of years required. Bachelor’s in Computer Science, Engineering, or related technical discipline or equivalent combination of training and experience. 10+ years programming experience: building business logic layers and back-end systems for high-volume pipelines. 2 years' experience with Golang. 2 years' experience with Spark, Spark SQL, and Scala. 2 years' experience using AWS Data Services: any combination of DMS, EMR, Glue, Athena, S3, CloudWatch, Lambda or IAM. 2 years' experience with high-speed messaging frameworks and streaming (Kafka). 1-year exp. with data architecture, ETL and processing of structured and unstructured data. 2 years' experience with DevOps tools (any combination of GitHub, Travis CI or Jira) and methodologies (Lean, Agile, Scrum, Test Driven Development) Excellent written and verbal communication skills in English. #LI-KD1 #LI-Remote",3.0,"Realogy Holdings Corp
3.0","Emeryville, CA",-1,10000+ Employees,2005,Company - Public,Real Estate,Real Estate,$5 to $10 billion (USD),-1
Data Engineer II : 20,-1,"Primary Skills: Scala, Spark-SQL, Machine Learning, Query writing Duration: 12+ Months (Possible chances of extension) Contract Type: W2 Only Note: Remote flexibility during lockdown Skills Required: Bachelors degree in Computer Science or equivalent education/training 4- 5 years of Software development and testing experience. 3+ years of Working experience on tools like Hive, Spark, HBase, Sqoop, Impala, Kafka, Flume, Oozie, MapReduce, etc. 3+ years of programming experience in Scala & Spark. Experience with development and automated testing in a CI/CD environment. Knowledge of GIT/Jenkins and pipeline automation is must. Experience with developing and testing real-time data-processing and Analytics Application System. Strong knowledge in SQL development on Database and/or BI/DW. Strong knowledge in shell scripting. Experience in Web Services - API development and testing. To follow up with any questions, please contact Khushbu @ 408-907-5650 Akraya is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US. We offer comprehensive benefits including Health Insurance (medical, dental, and vision), Cafeteria Plan (HSA, FSA, and dependent care), 401(k) (enrollment subject to eligibility), and Sick Pay (varies based on city and state laws). If this position is not quite what you're looking for, visit akraya.com and submit a copy of your resume. We will get to work finding you a job that is a better fit at one of our many amazing clients. Akraya is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender, race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law. Akraya is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",-1,Akraya Inc.,"Sunnyvale, CA",-1,-1,-1,-1,-1,-1,-1,-1
Lead Data Engineer,$75k-$135k (Glassdoor Est.),"Lead Data Engineer Summary The Lead Data Engineer leads the design and development of tools and process enhancements data pipeline. This pipeline includes high-volume, complex ETL projects on a wide variety of data inputs such as test scores, school characteristics, directory, course enrollment, college readiness, postsecondary outcomes, and others. As the primary architect of the data pipeline, the Lead Data Engineer supports Data Strategy in implementing scalable processes for analytics, and serves as the internal expert on all issues related to GreatSchools’ data infrastructure. They also collaborate closely with Data Science, TechOps, and Web Engineering functions and provide leadership and guide implementation for the processing and loading of data for display on the website and integration into data feeds. This position reports to the VP, Data Strategy. It is a full time, exempt position with headquarters located in Oakland, CA (remote work within US ok). About GreatSchools.org GreatSchools is the leading national nonprofit empowering parents to unlock educational opportunities for their children and support stronger schools in their community. We aim to make school quality information relevant and actionable to support equity for underserved communities who have faced systemic racism and unjust barriers to access a quality education for their children. We are the only national organization that collects education data from all 51 state departments of education and the federal government and then provides analysis, insights and school quality ratings to parents, communities, advocacy organizations, researchers, policymakers and others. GreatSchools.org reaches 45 million users per year, 40% of whom are from low-income households. We also work to uncover bright spots in American education and catalyze discussions about equity. We have worked for two decades to develop parent support resources, including articles, videos, podcasts, parent tips, and learning materials parents can use at home to strengthen the home-school connection and keep their child learning and on track. Responsibilities Demonstrate expertise in GreatSchools data schemas: Designs, owns and manages database warehouse and storage, serving as internal lead on where and how data is stored, and why. This includes understanding the complex interplay between data pipelines and product and software development. Advise on and implement data architecture solutions, working closely with Web Engineering and Data Science in solving data architecture problems. Lead improvements to GreatSchools data pipeline: Identify and assess new tools and processes that increase the efficiency of ETL processes and analytics. Serve as the lead project owner for defining and implementing tool and process enhancements. Support the integration of Data Engineering and Data Science through writing scalable code. Leads and owns the Data Engineering release process. Advise and support Data Engineering team: Develop tools and processes to support ETL developers through increased automation and quality control of data loading Provide guidance for the work of other data team members Advise and assist in size and difficulty assessment for infrastructure projects Lead and advise code review process Track and share best practices related to data-engineering Represent Data Engineering on our Data Governance and Data Strategy Council Requirements: 5+ years of experience as a Data Engineer or in a similar role designing and building scalable and robust data pipelines to enable data-driven decisions. 5+ years of experience using Python for processing of large data sets. Clear and concise communication, able to articulate clearly with a diverse team. Competencies: Core competencies, applies to all GS staff members Database design and management: In-depth knowledge of relational databases (MySQL, PostgreSQL) Proficiency in writing advanced SQL queries, and expertise in performance tuning of SQL queries Experience with database transformation, modeling and normalization Data pipeline development: Experience with data processing and workflow management tools such as Spark, Airflow/Luigi, Azkaban, etc. Experience working with AWS data technologies like Glue, S3, EMR, Lambda, DynamoDB, Redshift etc. Experience integrating custom, open source, and purchased tools into robust systems Programming and software development: Proficiency in programming languages including Python (preferred), Ruby Knowledge of professional software engineering practices & best practices for the full software development life cycle Strong algorithm & data structure knowledge Familiarity with common software development tools and methods e.g. JIRA, git, continuous deployment Experience supporting and working with cross-functional teams in a dynamic environment Experience with Agile Methodology preferred Preferred working knowledge of school data Position Location: Remote or Oakland (CA) Travel: When safe, periodic travel (2-3 times per year) to Oakland, CA may be necessary. To Apply: Please submit a cover letter, including salary requirements, along with your résumé. Your cover letter should include reasons why you want to work at GreatSchools. Résumés without cover letters and salary requirements will not be considered. No phone calls, please.",3.2,"GreatSchools
3.2","Oakland, CA",-1,51 to 200 Employees,1998,Nonprofit Organization,K-12 Education,Education,Unknown / Non-Applicable,-1
Senior Bioinformatics Data Engineer,-1,"GRAIL is a healthcare company whose mission is to detect cancer early, when it can be cured. GRAIL is focused on alleviating the global burden of cancer by developing pioneering technology to detect and identify multiple deadly cancer types early. The company is using the power of next-generation sequencing, population-scale clinical studies, and state-of-the-art computer science and data science to enhance the scientific understanding of cancer biology, and to develop its multi-cancer early detection blood test. GRAIL is headquartered in Menlo Park, CA with locations in Washington, D.C., North Carolina, and the United Kingdom. It is supported by leading global investors and pharmaceutical, technology, and healthcare companies. For more information, please visit www.grail.com. The Bioinformatics Data Engineer will partner with computational, engineering and business functions to co-develop data solutions for the GRAIL product pipeline, as well as help establish a research data model to support the variety of data needs of the Research and Development organization. The BDE will also have opportunities to apply statistical analysis and/or machine learning to help identify patterns and discover insights in data of high complexity and volume. You Will: Define and develop data asset creation workflows for clinical study data, from data pre-processing and automation through data dissemination. Assist in developing and managing interactive data visualization and analytics tools for reporting and trending. Play a key role in understanding user requirements, implementing systems and authoring procedures related to system use. Identify new technologies, concepts, and methodologies to address complex and evolving needs of study teams and other data consumers. Maintain data integrity and quality throughout the data lifecycle, including ensuring clinical study-related blinding where appropriate. Your Background Includes: BS/MS/PhD in a quantitative scientific field (computer science, engineering, mathematics, statistics, bioinformatics, etc.) 3 - 5 years of industry experience. Experience with R or Python programming and at least one system-level programming language like Go, Java or C++. Experience with SQL development and data warehousing concepts (e.g. ETL/ELT). Experience with AWS. Familiarity with AWS Athena, Glue, Data Pipeline a plus. Experience with a workflow engine, like Reflow or NextFlow a plus. Experience with cross-functional collaboration while ensuring data quality and commitment to analysis reproducibility. Experience with real-time data visualization and analytics tools. Experience with next-generation sequencing data is a plus. Excellent interpersonal communication (written and verbal) and organizational skills. Excellent team player with a demonstrated track record of success in a cross-functional team environment. Consistent commitment to delivering on team goals with a sense of shared urgency. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",4.0,"GRAIL, Inc.
4.0","Menlo Park, CA",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
Azure Data Engineer,-1,"Hi

Please find below job details and let me know your interest.
Role: Azure Data Engineer*
Location: San Francisco, CA 94102*
Duration: 12+ Month *
Job Details:

Must Have Skills (Top 3 technical skills only) \* 1. Azure (ADF), 2. Databricks 3. Spark

Detailed Job Description:
Responsible for the build, deployment, onboarding and operations of multi-tenant data platform components/tools.
• Experience building cloud native production grade enterprise data lake(s) leveraging streaming/micro-batch and batch processing engines
• Best practices, design and operational practices experience of data platform components and capabilities, preferably in an Azure ecosystem
o Experience with the design, deployment and operations of elastic spark clusters (preferably Azure Databricks)
o Experience with and working knowledge in the design of/development with Databricks delta lake on cloud data lake (preferably ADLS gen 2)
o Experience with the design, deployment, integration and operational practices of Data Catalog and Governance tooling (such as Azure Data Catalog, Collibra, Alation)
o Experience building and deploying production grade data pipelines with Azure Data Factory.
Automation of deployment, configuration and operations of Cloud PaaS data services (preferably Azure)

Minimum years of experience\*:
10+

Certifications Needed: No

Top 3 responsibilities you would expect the Subcon to shoulder and execute\*:
Participate in building architecture and design
Develop, test and deploy data integration solutions
Provide data management knowledge to deliver the most architecturally efficient
Thanks

Sumit Talekar | Sr. Technical Recruiter | USA & Canada

SILVERLINK TECHNOLOGIES LLC

Contract length: 12 months

Job Types: Full-time, Contract

Pay: $70.00 - $75.00 per hour

Experience:
Spark: 6 years (Preferred)
Databricks : 5 years (Preferred)
Azure ADF: 6 years (Preferred)",3.9,"SILVERLINK TECHNOLOGIES LLC
3.9","San Francisco, CA",-1,201 to 500 Employees,2003,Company - Private,IT Services,Information Technology,$25 to $50 million (USD),-1
Senior Data Engineer,-1,"Our Vision For over one hundred years, cities have managed their streets with concrete, paint, signage, and signals. The last ten years have seen a dramatic degradation of cities' ability to manage the movement of vehicles in the public realm. The tools currently available to them to fight back are not comprehensive enough, largely proprietary and have done a poor job of solving for the real issues. Lacuna works with cities and agencies to establish a new, digital environment that will allow them to effectively manage the public right-of-way for the next 100 years. This will result in a digital infrastructure that will fully support 21st century mobility – for everything from scooters to autonomous vehicles to drones and flying taxis. We aspire to be uniquely city and public friendly - we are building this future on a foundation of open source and open standards, with privacy- and society-first principles from the start. We are housed at Playground Global, an incubator with fantastic facilities, near the Cal Ave train station in Palo Alto, CA. This role supports working remotely. How you'll have impact We are looking for a seasoned data engineer with experience in operating large-scale data systems. As Lacuna grows to a global scale, we are taking on challenges involving scalability, multi-tenancy in a multi-cloud environment, managing customer SLAs, while staying on top of a rapidly evolving platform. In this role you will drive initiatives that enable urban mobility to be managed as code through the use of data engineering tools. You'll be an evangelist on pipeline architecture, data design, system performance and scalability. You'll develop automation for Big-Data processing to provision data services on multiple cloud environments at a global scale. What you'll need: 3+ years of experience as a Data or Platform Engineer managing BigData pipelines (experience with batch, stream and ad-hoc processing for billions of daily events) 2+ years of experience with deploying data services in the cloud (AWS, Azure, or GCP) 2+ years of experience working on data modeling, database management and in-depth knowledge of SQL/NoSQL technologies. Demonstrate in-depth knowledge of ETL/ELT and data processing technologies Demonstrate coding experiences with Python/ML or Java/BigData. Build monitoring and alerting systems to ensure quick response to processing issues, with auto-scaling capability to meet workload on demand Implement best practices and maintain policies on data quality and data security Create and improve reports, dashboard, metrics and analytics to serve business need Troubleshoot and conduct root-cause analysis of data issues, pipeline outages and other operational problems. Bachelor's degree in Computer Science, Engineering or related field Gold stars for experience with: Cloud-native serverless/containerized systems (Lambda, Kubernetes) Familiarity with machine learning toolkits such as scikit-learn,TensorFlow or PyTorch. Master's degree in Computer Science or related field Lacuna is an equal opportunity employer. In building a product to solve real world problems for our cities, we strive to build a company representative of those cities. We encourage diversity in thought, experience, background and perspective. We are committed to creating such an environment for all employees.",5.0,"Lacuna Technologies
5.0","Palo Alto, CA",-1,Unknown,-1,Company - Private,-1,-1,Less than $1 million (USD),-1
SQM Big Data Engineer,$109k-$193k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer , you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. What you'll do: Gather and process raw data at scale Process unstructured data into structured data, manage schema of new data Manage data access to protect data in a safe way Read, extract, transform, stage and load data to selected tools and frameworks as required and requested Perform tasks such as writing scripts, write SQL queries, etc Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis Work closely with the engineering team to monitor product performance and track product quality trends Analyze processed data Monitoring data performance and modifying infrastructure as needed Define data retention policies Skills you have: 4+ years of recent experience in data engineering Bachelor’s Degree or more in Computer Science or a related field Experiences on Cloudera CDH platform, Spark programing, Impala, SQL, analyze data via Hive, etc A solid track record of data management showing your flawless execution and attention to detail Strong knowledge of and experience with statistics Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources Experience in production support and troubleshooting You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer (Match.com LLC,$87k-$157k (Glassdoor Est.),"Support existing data pipelines and systems in production. Design and scale data ingestion systems with high availability and reliability. Design, build and launch new data extraction, transformation and loading processes in production. Design, build and launch new data models in production. Write batch and real-time big data jobs using Hadoop and Spark. Maintain and evolve dimensional data models and schema designs. Create automated reports to be consumed by the rest of the organization. Build machine learning models and derive insights from massive amounts of data. Use strong communications skills (oral and written) to interact with engineers and product managers to support Tinder product decisions. Scale one of the fastest growing data warehouse’s in mobile.

Minimum Requirements: Bachelor’s degree or U.S. equivalent in Computer Science, Computer Engineering, Software Engineering, Computational Data Science, or a related field, plus 3 years of professional experience utilizing big data technologies to support data ingestion and extraction to facilitate data driven decision making. Must also have the following: 3 years of professional experience developing new data models and schema designs using programming languages (including Scala, Python, or Java); 1 year of professional experience designing and implanting ETL (Including Extract, Design, and Load) systems; 1 year of professional experience analyzing and utilizing large data sets to identify trend and troubleshoot issues using analytical tools; 1 year of professional experience working with engineers and product managers to support product decisions.

Please send resume to: Lauren Lozano, Match Group LLC, 8750 North Central Expressway, Suite 1400, Dallas, TX 75231. Please specify ad code KVLL. EOE. MFDV.
Why Match Group?
A lot of places say they change lives, but we actually do it. We’ve helped millions of people find love and happiness! Here are a few perks we have in store for you:

$1,500 annual training budget
100% employer match on 401k contributions
Specific COVID-19 allowance for home office set-up
Matched giving to qualified organizations
100% paid Parental Leave
Happy Hours and Company parties (right now they are all virtual, but still a ton of fun!)

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

Learn more about why our employees love working at Match!",2.8,"Match Group
2.8","Palo Alto, CA",-1,1001 to 5000 Employees,1998,Company - Private,Advertising & Marketing,Business Services,$100 to $500 million (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
"Data Engineer, Video Cloud",$84k-$151k (Glassdoor Est.),"Founded in 2012, ByteDance is a technology company operating a range of content platforms that inform, educate, entertain, and inspire people across languages, cultures, and geographies. With a suite of more than a dozen products, including TikTok, Douyin, Toutiao, Helo, and Resso, ByteDance now has a portfolio of applications available in over 150 markets and 75 languages. ByteDance Video Infrastructure is a world-leading video platform that provides multi-media storage, delivery, transcoding, and streaming services. We are building the next generation video processing platform and the largest live streaming network, which provides excellent experiences for billions of users around the world. Popular video products of ByteDance are all empowered by our cutting-edge cloud technologies. Working in this team, you will have the opportunity to tackle challenges of large-scale networks all over the world while leveraging your expertise in coding, algorithms, complexity analysis, and large-scale system design. Responsibilities: - Craft optimal data processing architecture and systems for new data and ETL pipelines. - Design, build, and maintain efficient and reliable data pipelines to move and transform data (both large and small amounts). - Drive internal process improvements and automate manual processes for data quality and SLA management. - Work with different cross-functional partners including CDN, Video Understanding, Video Transcoding, Live Streaming, and Real-Time Communication. Qualifications: - Bachelor's degree in Computer Science or a related technical background involving software/system engineering, or equivalent working experience. - Good programming experience with at least one of the following languages: C, C++, Java, Python, or Go. - Experience in custom ETL/data pipeline design, implementation, and maintenance. - Experience with data processing software (Hadoop, Spark, Pig, Hive) and algorithms (MapReduce, Flume). ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too. ByteDance is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at USRC@bytedance.com.",3.9,"Bytedance
3.9","Mountain View, CA",-1,10000+ Employees,2012,Company - Private,Internet,Information Technology,Unknown / Non-Applicable,-1
Sr Big Data Engineer,$87k-$155k (Glassdoor Est.),"Position: Sr Big Data Engineer*
Location: Costa Mesa, CA // San Jose, CA , look for locals*
Duration: 12+ Months*
Looking For Seniors. 10 + Years of experience.*
Job Description: *
Requirements *
• BS degree in computer science, computer engineering or equivalent

• Proficient in Java, Spark, Kafka, Python, AWS Cloud technologies

• Must have active current experience with Scala, Java, Python, Oracle, Cassandra, Hbase, Hive

• 3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop, MapReduce, HDFS, Cassandra, HBase, Hive, Flume, Sqoop, Spark, Kafka, Scala Familiarity with AWS scripting and automation

• Flair for data, schema, data model, how to bring efficiency in big data related life cycle

• Must be able to quickly understand technical and business requirements and can translate them into technical implementations

• Experience with Agile Development methodologies

• Experience with data ingestion and transformation

• Solid understanding of secure application development methodologies

• Experienced in developing microservices using spring framework is a plus

• Understanding of automated QA needs related to Big data

• Strong object-oriented design and analysis skills

• Excellent written and verbal communication skills
Responsibilities: *
• Utilize your software engineering skills including Java, Spark, Python, Scala to Analyze disparate, complex systems and collaboratively design new products and services

• Integrate new data sources and tools

• Implement scalable and reliable distributed data replication strategies

• Ability to mentor and provide direction in architecture and design to onsite/offshore developers

• Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases

• Perform analysis of large data sets using components from the Hadoop ecosystem

• Own product features from the development, testing through to production deployment

• Evaluate big data technologies and prototype solutions to improve our data processing architecture

• Automate everything

Contract length: 12 months

Job Types: Part-time, Contract

Salary: $99,777.00 - $216,085.00 per year

Schedule:
Monday to Friday",2.7,"IMG Systems
2.7","San Jose, CA",-1,1 to 50 Employees,-1,Company - Private,Consulting,Business Services,$1 to $5 million (USD),-1
Senior Big Data Engineer,$136k-$237k (Glassdoor Est.),"Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we’ve been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers. Big Data Engineer Job Summary: We’re looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you’ll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design. Job Responsibilities: Gather and process raw data at scale. Process unstructured data into structured data, manage schema of new data. Manage data access to protect data in a safe way. Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, write SQL queries, etc. Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis. Work closely with the engineering team to monitor product performance and track product quality trends. Analyze processed data. Monitoring data performance and modifying infrastructure as needed. Define data retention policies. Job Requirements: 5 + years of recent experience in data engineering. Bachelor’s Degree or more in Computer Science or a related field. Experiences on Cloudear CDH platform,Spark programming，Impala SQL Lauguage, Analyze data via Hive,etc. A solid track record of data management showing your flawless execution and attention to detail. Strong knowledge of and experience with statistics. Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives. Experience in C, Linux Shell, JavaScript or other programming languages is a plus. Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Deep knowledge of data features engineering, data mining, machine learning, or information retrieval. Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. Experience in production support and troubleshooting. You find satisfaction in a job well done and thrive on solving head-scratching problems. Language requirement: English, Mandarin is plus",3.3,"Zoom
3.3","San Jose, CA",-1,501 to 1000 Employees,2006,Company - Private,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
GCP Data Engineer,-1,"*Title : GCP Data Engineers*

*Duration: 6 months + possible extension*

*Location : 100% remote*

*If someone doesn’t have GCP, strong Azure cloud is also fine.*

*Must Haves*

Python (Intermediate to advance Python background)

Strong SQL (Minimum - 3 years)

Worked with CI/CD pipeline

Cloud Exp - 2-3 yrs exp (GCP and open to AWS)

Will Ingest Data Pipelines in GCP !

Rest API consumption

Data Warehousing background

Telemetry Data

Data Profiling

Performance monitoring/Logging into BigData or Cloud Platform

Network Monitoring

Virtualization Logs

*Requirements*

Strong experience in building cloud native data engineering solutions using GCP or AWS platforms

4+ years of development experience with Python at an advanced level including concurrent and metaprogramming

Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs. \*

Deep background in building data integration applications using Spark or mapReduce frameworks

Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns and best practices.

Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts

Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie.

Preferred someone who has worked with GCP products such as BigQuery, Cloud Composer, Data fusion, GCS and GKE or corresponding technologies on AWS platform

High proficiency in working with Git, automated build and CI/CD pipelines

Job Type: Contract

Pay: $50.00 - $60.00 per hour

Schedule:
* Day shift

Work Remotely:
* Yes",3.1,"SSIT
3.1","San Jose, CA",-1,201 to 500 Employees,-1,College / University,Colleges & Universities,Education,Less than $1 million (USD),-1
Senior Big Data Engineer,$122k-$218k (Glassdoor Est.),"DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them. We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system. So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team. Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure. This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be… Seeking Big Data Superhero As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies. Responsibilities The exciting things you will get to do once employed as a Senior Big Data Engineer: Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses Design, build and launch new data extraction, transformation and loading processes in production Create new systems and tools to enable the customer to consume and understand data faster Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process) Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale Study data, identify patterns, make sense out of it and convert it to algorithms Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance Required Qualifications The experience and qualifications we hope you bring to the Senior Big Data Engineer position: Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent 4+ years of hands-on experience in Java and Spring frame work Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment 4+ years of hands-on experience in building data pipeline with Java and Spark 4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation 2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL Experience working in an Agile/Scrum environment Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders Proficient understanding of distributed computing principles Strong written and verbal communications Preferred Qualifications Hands-on experience working with Business Intelligence and Reporting Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions. Experience with integration of data from multiple data sources Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.) Passion for working with open-source technologies as well as commercial platforms Benefits As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with: Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here Flexible work hours, up to 32 days off annually Friendly teams, experienced colleagues, and perfect work equipment Opportunities for career growth and raising professional skills Travel opportunities Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today. WHO WE ARE? DB Best is internationally known for comprehensive data-management services, database development and migration, and the creation of highly successful web and mobile systems. Hundreds of customers from all over the world rely on our developers and architects for cost-effective strategies in migrating and modernizing their on-premises and cloud assets every day. Our expertise covers all major databases, including SQL Server, Oracle, Sybase, IBM DB2, PostgreSQL, MySQL, and Access. Unlike smaller, narrow-focused competitors, our developers can choose among many development platforms, including iOS, Android, Windows Mobile, PHP, ASP.NET, and Java. As a Microsoft Gold Partner for five years running with competencies in Application Development, Data Analytics, Data Platform, and Cloud Platform and an Amazon Web Services Consulting Partner, we have the experience to make our customers successful. DB Best has rapidly become one of America’s fastest growing companies, as recognized by our presence on the prestigious Inc. 500 list in 2012. Originally started in Silicon Valley as a two-person team by Dmitry and Irena Balin, DB Best now has hundreds of employees in five countries, including our headquarters in Redmond, Washington, and locations in San Francisco, the UK, Israel, Poland, and Ukraine.",4.3,"DB Best Technologies
4.3","San Francisco, CA",-1,201 to 500 Employees,2002,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable,-1
Senior Big Data Engineer,$85k-$159k (Glassdoor Est.),"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Description You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements. #LI-DNI #LI-DNP What You’ll Do Activities: data engineering, building data pipelines, migrating legacy code to new platform. Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions Build collaborative partnerships with architects, technical leads and key individuals within other functional groups Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution Actively participate in code review and test solutions to ensure it meets best practice specifications Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation Write project documentation What You Have A degree in an associated field and/or other advanced certification along with significant experience Solid background in Big Data and distributed computing for 5+ years Good experience with Java Hands-on experience with Spark (design, develop, troubleshoot) Good understanding of general concepts of Big Data on the level of infrastructure (Hive, Hadoop) Work experience or knowledge of AWS, S3, EC2, EMR Experience building data ingestion pipelines, Data Warehouse or Database architecture Experience with data modeling; hands-on development experience with modern Big Data components Good understanding of SQL queries, joins, stored procedures, relational schemas Good understanding of CI/CD principles and best practices Experienced in working with modern Agile developing methodologies and tools Analytical approach to problem; excellent interpersonal, mentoring and communication skills Motivated, independent, efficient and able work under pressure with a solid sense for setting priorities Experience in direct customer communications Nice to have Python Athena Terraform Ruby (ability to read code) What We Offer Medical, Dental and Vision Insurance (Subsidized) Health Savings Account Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) Short-Term and Long-Term Disability (Company Provided) Life and AD&D Insurance (Company Provided) Employee Assistance Program Unlimited access to LinkedIn learning solutions Matched 401(k) Retirement Savings Plan Paid Time Off Legal Plan and Identity Theft Protection Accident Insurance Employee Discounts Pet Insurance EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring",4.0,"EPAM Systems
4.0","San Jose, CA",-1,10000+ Employees,1993,Company - Public,IT Services,Information Technology,$1 to $2 billion (USD),-1
Lead Data Engineer,$94k-$165k (Glassdoor Est.),"Job no: 497743 Work type: Staff Location: San José Categories: Unit 9 - CSUEU - Technical Support Services, Information Systems & Technology, Probationary, Full Time Job Summary Reporting to the Interim Associate CIO and Senior Director of Enterprise Solutions, the Lead Data Engineer is responsible for SJSU’s Campus Data Warehousing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions. The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse; answer important questions across a variety of functional areas; and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, and developing and implementing the solution to deliver the desired business outcomes. The Lead Data Engineer will assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments; provide systems and technical support of vendor and locally developed software; and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments may be temporarily or permanently changed to meet the needs and goals of the department and university. Key Responsibilities Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes. Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally. Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy. Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities. Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities. Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc. Oversee the sourcing and creation of partnerships with key analytical vendors who can provide specialized expertise to assist with solution development while leading the in-house effort and managing CO interface. Participate in the selection and managing cloud provider relationships. Oversee project plans, timelines, and vendor relationships to ensure releases are delivered with the highest quality, on-time and within budget. Establish governance, manage technology priorities and develop a 3-5 year analytic roadmap for SJSU. Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture. Partner with University stakeholders and be responsible for data analytics, custodianship, and infrastructure to ensure alignment with departmental data and analytics requirements to avoid conflicting activities, and develop the most efficient data analytics insights across the departments. Identify, interpret, analyze and address critical business issues, questions and to develop use cases. Organize and create an environment that makes data and information accessible with appropriate channels of access controls. Work with internal and external stakeholders to build a relationship of trust and play an advisory role in the use of data to improve performance and University-wide strategy formulation. Develop and document clear understandable plans, roadmaps and communication for business users, technical staff and IT groups. Follow IT policies and standards for maintaining consistency and standards across SJSU campus. Lead and own the interaction with campus technical support staff and customers to provide reliable services. Knowledge, Skills & Abilities Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources. Strong interpersonal, communication, organization and planning skills. Ability to work effectively with faculty, staff, and senior administrators in areas other than technology to develop and implement appropriate uses of technology. Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission. Ability to lead, coordinate and influence the efforts of cross-functional teams. Advanced technical knowledge in IT systems and emerging technology trends and issues. Ability to synthesize data from multiple sources to address complex business questions. Advanced knowledge with analytics tools, “big data” technologies, cloud computing environments, relational database. Ability to partner with key stakeholders to understand what data and analytics support to help drive positive outcomes. Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations. Ability to identify key insights and built large-scale data products that enhance the customer experience and improve operations processes. Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions. Strong ability to deliver on high-impact analytics projects. Skilled in recruiting, developing, inspiring, leading a growing team of data analysts. Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases. Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript. Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak. Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets. Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc. Knowledge building and optimizing ‘big data’ data pipelines, architectures and data sets. Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management. Advanced knowledge of Systems development life cycle methodologies (Agile and Waterfall) in order to develop effective systems. Advanced knowledge of application development, testing and deployment processes and tools. Strong analytic skills related to working with unstructured datasets. Knowledge with big data languages such as R, Python/PySpark and familiarity with cloud architecture. Advanced knowledge in data mining, forecasting, simulation, and/or predictive modeling. Advanced knowledge building BI environments with significant scale and scope. Required Qualifications A Bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience Input required qualifications Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers Preferred Qualifications 3+ years’ experience leading a team 3+ years’ experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting 3+ years advanced analytics experience in support of business strategy 3+ years working experience in implementing public cloud solutions Compensation Classification: Analyst/Programmer - Expert Hiring Range: $6,249/month - $12,100/month San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary. Application Procedure Click to complete the SJSU Online Employment Application and attach the following documents: Resume Letter of Interest All applicants must apply within the specified application period: October 9, 2020 through October 23, 2020. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university. Contact Information University Personnel jobs@sjsu.edu 408-924-2166 #LI-SJSU1 Additional Information Satisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis. The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire. SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS) All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Equal Employment Statement San José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus). Advertised: October 09, 2020 (9:00 AM) Pacific Daylight Time Applications close:",4.2,"California State University
4.2","San Jose, CA",-1,10000+ Employees,1962,College / University,Colleges & Universities,Education,$2 to $5 billion (USD),-1
Lead Data Engineer (Player/Coach),$124k-$218k (Glassdoor Est.),"Req ID: 92593 At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here. NTT DATA Services currently seeks a Lead Data Engineer (Player/Coach) to join our team in Palo Alto, California (US-CA), United States (US). Lead Data Engineer will: Create and maintain the optimal data pipeline architecture based on platform and application requirements Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Recommend, select and integrate the data tools and frameworks required by the SMART Platform and Applications Design and implement data integration strategies and processes using industry standard methods and tools Incoming Lead Data Engineer should possess: 15+ years of overall technology experience, with 7+ years of data management, modeling and architecture design experience 4 years of Python 5 years' experience with integration of data from multiple data sources 2 years' experience using Hadoop, Kafka, HDFS 2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming 2 years' experience using Spark 2 years' experience in a ""Lead"" role. Nice to have: Experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB Experience with various messaging systems, such as MQTT or RabbitMQ Experience working in public cloud environments like AWS, Azure. Experience with assisting Data Scientists with conforming and integrating data, data preparation and data cleansing, and then automating that work over time. Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/MapR/Hortonworks Experience working in large-scale, highly complicated and global environments – Japan experience is a plus A strong team-oriented mindset Experience with SQL and RDBMS systems for analytic querying Experience using various ETL techniques and frameworks, such as Flume and Informatica Experience using Big Data querying tools and approaches including Parquet, Hive, and Impala This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment. About NTT DATA Services NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more. NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.",3.4,"NTT DATA Services
3.4","Palo Alto, CA",-1,10000+ Employees,1967,Company - Public,IT Services,Information Technology,$10+ billion (USD),-1
Lead Big Data Engineer- Opportunity for Working Remotely,-1,"This job requisition is not eligible for employment-based immigration sponsored by VMware Your next adventure at VMware is only a click away! At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community. VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Are you looking for a high-energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley? At VMware, we are not a traditional IT organization, we build differentiator IT solutions in the Data organization. VMware is seeking a Lead Data Engineer for working “hands on” on a platform that powers data-driven insights for enterprise reporting and analytics needs. Responsibilities: Understand the business capability/requirements and transform them into robust design solutions Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms. Perform hands on work using Kafka, Spark, Presto, SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform. Integrate data sets from different sources using Informatica, Python, Kafka, SAP SDI/SLT Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data. Help data consumers to correctly understand and use the data. Building reports based on the business need. Qualifications: 8+ years of experience in as a BI/Data Engineer handling large volumes of data. Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools. Expertise in writing advanced SQL queries. Experience working with Informatica, SAP SDI/SLT Expertise in SAP HANA, Hive/Hadoop/Hawq, Spark, Kafka, Presto Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Familiarity with Amazon Web Services (AWS), Redshift is a plus Strong analytical and troubleshooting skills Excellent verbal and written communication skills Bachelors degree in Computer science, Statistics, Mathematics, Engineering or relevant field. Category : Engineering and Technology Subcategory: Software Engineering Experience: Business Leadership Full Time/ Part Time: Full Time Remote: Yes Posted Date: 2020-10-15 VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",-1,VMware,"Palo Alto, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US),-1,"Senior Data Engineer - ETL/Data-Pipeline (On-site/Remote-US)*
Job Heading: *
Position/Title: Senior Data Engineer - ETL/Data-Pipeline

Position Type: Full-time

Remote: On-site/Remote-US

Company Name: Inffiniti Inc.

Country: USA
Job Information: *
Industry: Healthcare

City: Mountain View

Zip/Postal Code: 94040

State/Province: California

Experience Level: 5-10 years
Job Description: *
Looking for a Senior Data Engineer to design and build data infrastructures to meet the growing data needs in the company while maintaining efficiency, reliability and consistency.
• Design and own data pipelines and integrations for streaming and batch data
• Collect, clean and store large datasets which will be used for analytics, reporting and prediction
• Implement best practices in security and data handling to maintain the privacy of users and partners
• Help develop data products that drive company success by collaborating with the teams across the company
• As part of a small, agile data-engineering team, you will have a tremendous opportunity to make a big impact for the company
• Help accelerate data-engineering, data-science, business intelligence, reporting and analytics by evaluating, integrating and building new tools as needed
• Help achieve a high-level of operational excellence in data-engineering
Job Requirements: *
• A desire to help people to use data effectively in a startup environment

• Expertise in Spark/Storm/Flink/MapReduce/Impala/Hive
• In-depth knowledge of AWS (including EMR, DMS, Athena, RDS, Aurora, Lambda, Redshift)
• Expertise in stream data-processing (DMS/Flink/Spark/Kinesis/Kafka)
• Advanced SQL skills and strong proficiency in the following programming languages: Python,

Scala and Java (at least two)
• Demonstrated expertise in Object-Oriented/Functional programming including a solid grasp of common design patterns, idioms and design
• Fluency in data structures, algorithms, distributed computing, storage systems, and various consistency models
• Familiarity with Pandas, SciPy, Scikit-learn, Seaborn, SparkML packages/modules
• Knowledge of multiple database technologies and their tradeoffs, and also how to make the best use of each type of database
• Goal-oriented, with a passion for developing a world-class engineering culture
• Willingness to learn and mentor in a collaborative team environment
Basic Qualifications: *
• BS/MS in Computer Science/Mathematics/Computer Engineering or related field, or equivalent experience

• 4+ years in data-engineering or equivalent experience
• 7+ years in software engineering or equivalent experience
• Experience in relational, object, columnar, key-value and related database types
• Experience in designing and maintaining at least one type of database
• Experience in data-warehouse modernization, building data-marts, star/snowflake schema designs
• Experience in data infrastructures, ETL/ELT pipelines, and BI/reporting/analytic tools
• Experience in building production-grade data backup/restore and disaster recovery solutions
Preferred Qualifications: *
• SparkML/Scikit-learn/TensorFlow experience
• GraphX/TitanDB/neo4j/range++/graph-engine/orientdb experience
• Delta Lake experience
• Airflow/luigi/oozie/azkaban/pinball/chronos experience
• Snowflake cloud data-warehouse experience
• Hadoop MapReduce/Yarn/HDFS/Pig/Hive experience
• Tableau/Periscope-Sisense/Domo/Looker/Superset experience
Company Info: *
The world's largest AI healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with AI Nurses. Company is on a mission to improve people’s health and happiness through their digital health coach. They are the only AI nurse ever to become fully medically reimbursed to 100% replace a live nurse because they achieved equivalent health outcomes to live healthcare professionals, which allows for infinitely scalable healthcare.

Job Type: Full-time

Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location:
One location
Work Remotely:
Yes",-1,Inffiniti Inc.,"Mountain View, CA",-1,-1,-1,-1,-1,-1,-1,-1
Senior Big Data Engineer,$109k-$192k (Glassdoor Est.),"Senior Big Data Engineer - San Francisco, CA-200012514 At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Senior Big Data Engineer will execute true end-to-end development and testing of software products in an agile lifecycle. Partnering with key technical and business stakeholders, Senior Big Data Engineer will help refine the product vision, estimate and design a solution, create, purchase, or customize the requisite hardware and software to create the solution, ensure the quality of the final product through manual and automated tests, and support the new software as it is moved to production. Primary Responsibilities: • Design and build large scale data processing system (real-time and batch) to address growing AI/ML and Data needs of a Fortune 500 company • Build a product to process large amount data/events for AI/ML and Data consumption • Automate test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests. • Automate CI and deployment processes and best practices for the production data pipelines. • Build AI/ML model based alert mechanism and anomaly detection system for the product. The goal is have a self-annealing product Qualifications Basic Qualifications • Bachelor’s degree or equivalent work experience. • 5+ years of relevant experience. Required Skills/Experience • 10+ years of overall experience in software development with 5-6 years of relevant experience in designing, developing, deploying and operating large data processing data pipelines at scale. • 3-4 years’ experience with Apache Spark Streaming and Apache Kafka • Strong background in programming (Scala Must, good to have knowledge on Java, Python) • Experience on building reusable data frameworks/modules • Experience on Airflow scheduler • Experience with Containers, Kubernetes and scaling elastically • Strong background in algorithms and data structures • Strong analytical and problem solving skills • Strong bent towards engineering solutions which increase productivity of data consumers • Strong bent toward completely automated code deployment/testing (DevOps, CI/CD) • Passion for data engineering and for enabling others by making their data easier to access. • Some experience with working with and operating workflow or orchestration frameworks, including open source tools like Activiti, Spring Boot, Airflow and Luigi or commercial enterprise tools. • Excellent communication (writing, conversation, presentation) skills, consensus builder Required Skills Summary: Apache Spark, Apache Kafka, Scala/Java/Python, NoSQL Databases, Elasticsearch & Kibana, Kubernetes, Docker Containers Preferred Skills / Experience • Knowledge of ReactJS, GraphQL and API Development • Proficiency in working with Kafka • Knowledge front end, back end services, or full stack engineering • Demonstrated ability to tackle tough coding challenges independently and work closely with others on a highly productive coding team • Continuously develop and acquire new technical skills • Apache Flink experience Job Information Technology Primary Location CA-CA-San Francisco Shift 1st - Daytime Average Hours Per Week 40",3.6,"U.S. Bank
3.6","San Francisco, CA",-1,10000+ Employees,1863,Company - Public,Banks & Credit Unions,Finance,$10+ billion (USD),-1
"Sr. Data Engineer, Exempt",$87k-$154k (Glassdoor Est.),"Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission. We're looking for someone to join our team as a Sr. Data Engineer who: Provides senior level expertise in the design, mapping, development and testing of data movement for a data warehouse environment, with emphasis on the Extract-Transform-Load (ETL) process. Develops techniques surrounding data movement to exploit technologies such as DB2, Oracle and Informatica. Participates in data acquisition and transformation delivery and provides oversight and technical checkpoints to enforce standards and quality of Business Intelligence solutions. Essential Functions: Functions as a technical Informatica Administrator to maintain and develop ETL specifications for healthcare business and clinical users. Designs, builds and tests deliverables for complex ETL process and interfaces spanning multiple applications. Moderates discussions between technical and non-technical end users, taking non-technical requirements and converting them into technical requirements. Develops ETL source and target mapping design/specifications based on the business requirements and ETL standards and architecture. Assesses organizations, processes, channels, applications and data to identify practical solutions. Assumes responsibility for problem assessment and resolution in compliance with SLA requirements for ETL support. Participates in the on-call rotation schedule as needed. Performs other job-related duties as assigned. You'll be successful with the following qualifications: Education: Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required Work Experience: Five years ETL development or equivalent work experience using Informatica: Preferred Healthcare experience: Preferred",3.5,"Adventist Health System/West
3.5","Roseville, CA",-1,10000+ Employees,1973,Nonprofit Organization,Health Care Services & Hospitals,Health Care,Unknown / Non-Applicable,-1
Senior Data Engineer,-1,"JOB DESCRIPTION We are looking for Data Engineer to ensure our team always have access to the right data at the right time to make key product decisions. You have: Played a key role in scaling data stack in high growth consumer products. Professional coding experience in C/C++, Java, Python, Javascript, Shell or Go. Experience architecting and developing distributed systems. Led engineering, product, and design team to architect and instrument analytics, feature flagging, A/B testing capabilities in deep and complex UI/UX flows. Created robust data pipeline and interfaces that allow cross-domain analysis (such as combining performance, user events, revenue, etc) reliably and cost-effectively. Engineered automated presentation layers with an internal team or 3rd party tools to help the company understand business goals and their trajectories. You will: Architect and evolve Polarr's data stack to match the need of data and product team. Develop, construct, test and maintain large-scale data processing systems. Drive engineering and product team for data standardization and instrumentation. Communicate cross-functionally with product, design, engineering, and executives. You are: Highly proficient in a modern database, data pipelines, tools, and scripting languages. Passionate about consumer product design and end-user experiences. Curious, skeptical, and ready to drive deep into the weed. Must-Have Experience at Top Tier startups or Top Tier established companies Complex and substantive project experience CS or Computer related STEM degree Modern database and data pipeline experience C/C++ or Java or Python or JavaScript or Shell or Golang Distributed Systems",-1,Hired Recruiters,"San Jose, CA",-1,1 to 50 Employees,2015,Contract,Internet,Information Technology,Unknown / Non-Applicable,-1
